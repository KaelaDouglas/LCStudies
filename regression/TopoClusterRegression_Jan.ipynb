{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TopoCluster Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple, stripped-down notebook for training networks. I've removed most of the models that are present in `TopoClusterRegressionRewrite.ipynb`, as well as most of the plots (I find that the multitude of plots makes things a bit cumbersome and hard to navigate -- I'll see if I can change the way they are displayed later on).\n",
    "\n",
    "Here, we just train the so-called `all` model, which uses images from all $6$ calo layers. We train two versions, for charged and neutral pions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML fitting/loading/saving settings\n",
    "loadModel = False # if false, then run trainings directly. otherwise load the file.\n",
    "saveModel = True # if true, save the current model to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's choose our training data (and associated strategy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data choice\n",
    "# options are jet, pion, pion_reweighted\n",
    "strat = 'pion_reweighted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.22/02\n"
     ]
    }
   ],
   "source": [
    "# Import some basic libraries.\n",
    "import sys, os, uuid, glob\n",
    "import numpy as np\n",
    "import pandas as pd # we will use some uproot/pandas interplay here.\n",
    "import uproot as ur\n",
    "import ROOT as rt # used for plotting\n",
    "\n",
    "# Import our resolution utilities\n",
    "path_prefix = os.getcwd() + '/../'\n",
    "if(path_prefix not in sys.path): sys.path.append(path_prefix)\n",
    "from  util import resolution_util as ru\n",
    "from  util import plot_util as pu\n",
    "from  util import ml_util as mu\n",
    "from  util import qol_util as qu # for progress bar\n",
    "\n",
    "rt.gStyle.SetOptStat(0)\n",
    "# use our custom dark style for plots\n",
    "dark_style = qu.PlotStyle('dark')\n",
    "dark_style.SetStyle() # still need to manually adjust legends, paves\n",
    "\n",
    "plotpath = path_prefix + 'regression/Plots/'\n",
    "modelpath = path_prefix + 'regression/Models/'\n",
    "paths = [plotpath, modelpath]\n",
    "for path in paths:\n",
    "    try: os.makedirs(plotpath)\n",
    "    except: pass\n",
    "\n",
    "# metadata\n",
    "layers = [\"EMB1\", \"EMB2\", \"EMB3\", \"TileBar0\", \"TileBar1\", \"TileBar2\"]\n",
    "cell_size_phi = [0.098, 0.0245, 0.0245, 0.1, 0.1, 0.1]\n",
    "cell_size_eta = [0.0031, 0.025, 0.05, 0.1, 0.1, 0.2]\n",
    "len_phi = [4, 16, 16, 4, 4, 4]\n",
    "len_eta = [128, 16, 8, 4, 4, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Get the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me lay out some definitions, so it's clear as to what the data is.\n",
    "\n",
    "We have a number of different \"strategies\" (the `strat` variable at the top). These correspond to different choices of training, validation and testing datasets.\n",
    "\n",
    "1. `pion`: We train and validate the network using our pion gun data.\n",
    "\n",
    "2. `pion_reweighted`: This is the same as `pion`, except that our training data is reweighted using a jet dataset (via their reco topo-cluster $p_T$ distributions), that corresponds with QCD dijet events.\n",
    "\n",
    "3. `jet`: We train and validate the network using our jet data. This is a facsimile dataset -- we do not know the actual labels of the jet data topo-clusters, so we have assigned labels by matching clusters to truth-level pions in $(\\eta,\\phi)$.\n",
    "\n",
    "The validation performed for these networks is effectively being done on some \"holdout\" dataset from training -- it will by definition have similar kinematics, being drawn from the same set of events. The more interesting test -- how our energy regression performs in tandem with classification on our *unlabeled* jet dataset, will be handled in a separate notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now determine which files we get training and validation data from. Depends on our strategy.\n",
    "if(strat == 'pion' or strat == 'pion_reweighted'):\n",
    "    data_dir = path_prefix + 'data/pion/'\n",
    "    data_filenames = {'pp':data_dir+'piplus.root','pm':data_dir+'piminus.root','p0':data_dir+'pi0.root'}\n",
    "    \n",
    "elif(strat == 'jet'):\n",
    "    data_dir = path_prefix + 'jets/training/'\n",
    "    data_filenames = {'pp':data_dir+'piplus.root','p0':data_dir+'pi0.root'}\n",
    "\n",
    "# adjust our model and plot paths, so that they are unique for each strategy\n",
    "paths = [modelpath, plotpath]\n",
    "for i in range(len(paths)):\n",
    "    path = paths[i]\n",
    "    path = path + strat\n",
    "    try: os.makedirs(path)\n",
    "    except: pass\n",
    "    path = path + '/'\n",
    "    paths[i] = path\n",
    "modelpath, plotpath = paths\n",
    "\n",
    "# we get uproot trees and pandas DataFrames,\n",
    "# for training + validation\n",
    "tree_name = 'ClusterTree'\n",
    "branches = ['truthE', 'clusterE', 'clusterPt', 'clusterEta', 'cluster_ENG_CALIB_TOT']\n",
    "\n",
    "data_trees = {key:ur.open(val)[tree_name] for key,val in data_filenames.items()}\n",
    "data_frames = {key:val.pandas.df(branches,flatten=False) for key,val in data_trees.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing training and validation samples\n",
    "\n",
    "As we're taking logarithms  of `clusterE` and `cluster_ENG_CALIB_TOT`, we will always perform cuts to have `clusterE` > 0 for all datasets, and `cluster_ENG_CALIB_TOT` > 0 for training. \n",
    "\n",
    "On top of those cuts, we're free to apply additional cuts to training, validation and testing data. We can do them below, as we pick event indices for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_frames = {}\n",
    "validation_frames = {}\n",
    "\n",
    "# First, the minimum energy cut that we will always apply to data. Anything that fails to pass this cut will be discarded,\n",
    "# we will never evaluate on events that don't pass this cut.\n",
    "global_energy_cut = 0.\n",
    "\n",
    "# We apply a lower cut on cluster_ENG_CALIB_TOT, as very low-energy clusters can throw off training.\n",
    "energy_cut = [0., -1.]\n",
    "if(strat == 'pion' or strat == 'pion_reweighted'): \n",
    "    energy_cut[0] = 5.0e-1 # GeV\n",
    "    energy_cut[1] = -1.\n",
    "    \n",
    "elif(strat == 'jet'): \n",
    "    energy_cut[0] = 5.0e-2 # GeV\n",
    "    energy_cut[1] = -1.\n",
    "\n",
    "data_indices = {} # indices of all usable data, i.e. non-zero energy\n",
    "training_indices = {} # indices of events actually used for training\n",
    "validation_indices = {} # indices of events not used for training (but usable)\n",
    "\n",
    "# percent of events to hand over from training to testing\n",
    "testing_frac = 0.2\n",
    "rng = np.random.default_rng() # for shuffling indices when splitting training/testing\n",
    "\n",
    "for key in data_frames.keys():\n",
    "    \n",
    "    n = len(data_frames[key])\n",
    "    eng_calib_tot = data_frames[key]['cluster_ENG_CALIB_TOT'].to_numpy()\n",
    "    selected_indices = eng_calib_tot > energy_cut[0]\n",
    "    if(energy_cut[1] > 0.): selected_indices = selected_indices * (eng_calib_tot < energy_cut[1])\n",
    "    \n",
    "    selected_indices = selected_indices.nonzero()[0] # from boolean array to a list of actual indices\n",
    "    rng.shuffle(selected_indices)\n",
    "    n_test = int(0.2 * len(selected_indices))\n",
    "    \n",
    "    # making boolean arrays to select events -- arrays are of same length as dataframe\n",
    "    validation_indices[key] = np.full(n,False)\n",
    "    training_indices[key] = np.full(n,False)\n",
    "    validation_indices[key][np.sort(selected_indices[:n_test])] = True\n",
    "    training_indices[key][np.sort(selected_indices[n_test:])] = True\n",
    "    training_frames[key] = data_frames[key][training_indices[key]].copy()\n",
    "    validation_frames[key] = data_frames[key][validation_indices[key]].copy()\n",
    "\n",
    "# #     validation_indices[key] = (data_frames[key]['cluster_ENG_CALIB_TOT'] > global_energy_cut).to_numpy()\n",
    "# #     validation_indices[key] = validation_indices[key]^(validation_indices[key] * training_indices[key])\n",
    "\n",
    "    data_indices[key] = (data_frames[key]['cluster_ENG_CALIB_TOT'] > global_energy_cut).to_numpy()\n",
    "    data_frames[key] = data_frames[key][data_indices[key]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we treat $\\pi^+$ and $\\pi^-$ as the same, let's combine them so that we have *charged* and *neutral* pions. We will store all the charged pions under the key `pp`, and delete the key `pm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of charged pion training/testing events: 261873/65468\n",
      "Number of neutral pion training/testing events: 196960/49240\n"
     ]
    }
   ],
   "source": [
    "key_conversion = {'pp':'charged pion','p0':'neutral pion'}\n",
    "# combining dataframes\n",
    "if('pm' in data_frames.keys()):\n",
    "    data_frames['pp'] = data_frames['pp'].append(data_frames['pm'])\n",
    "    del data_frames['pm']\n",
    "    \n",
    "    training_frames['pp'] = training_frames['pp'].append(training_frames['pm'])\n",
    "    del training_frames['pm']\n",
    "    \n",
    "    validation_frames['pp'] = validation_frames['pp'].append(validation_frames['pm'])\n",
    "    del validation_frames['pm']\n",
    "\n",
    "for key in data_frames.keys():\n",
    "    print('Number of {type} training/testing events: {val1}/{val2}'.format(type=key_conversion[key], val1 = np.sum(training_indices[key]), val2 = np.sum(validation_indices[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have many more charged pions than neutral pions, so this *may* result in our charged pion regression being better-trained (unless the stats for both are sufficiently high)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some regression vars\n",
    "for key,frame in data_frames.items():\n",
    "    frame['logE'] = np.log(frame['clusterE'])\n",
    "    frame['logECalib'] = np.log(frame['cluster_ENG_CALIB_TOT'])\n",
    "    \n",
    "for key,frame in training_frames.items():\n",
    "    frame['logE'] = np.log(frame['clusterE'])\n",
    "    frame['logECalib'] = np.log(frame['cluster_ENG_CALIB_TOT'])\n",
    "\n",
    "for key,frame in validation_frames.items():\n",
    "    frame['logE'] = np.log(frame['clusterE'])\n",
    "    frame['logECalib'] = np.log(frame['cluster_ENG_CALIB_TOT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create scalers\n",
    "scaler_e = {key:StandardScaler() for key in data_frames.keys()}\n",
    "scaler_cal = {key:StandardScaler() for key in data_frames.keys()}\n",
    "scaler_eta = {key:StandardScaler() for key in data_frames.keys()}\n",
    "\n",
    "# fit our scalers, using the training data\n",
    "for key, frame in training_frames.items():\n",
    "    scaler_e[key].fit(frame['logE'].to_numpy().reshape(-1,1))\n",
    "    scaler_cal[key].fit(frame['logECalib'].to_numpy().reshape(-1,1))\n",
    "    scaler_eta[key].fit(frame['clusterEta'].to_numpy().reshape(-1,1))\n",
    "    \n",
    "# now apply our scalers to our data (training, testing and combo)\n",
    "for key, frame in training_frames.items():\n",
    "    frame['s_logE'] = scaler_e[key].transform(frame['logE'].to_numpy().reshape(-1,1))\n",
    "    frame['s_logECalib'] = scaler_cal[key].transform(frame['logECalib'].to_numpy().reshape(-1,1))\n",
    "    frame['s_eta'] = scaler_eta[key].transform(frame['clusterEta'].to_numpy().reshape(-1,1))\n",
    "    \n",
    "for key, frame in validation_frames.items():\n",
    "    frame['s_logE'] = scaler_e[key].transform(frame['logE'].to_numpy().reshape(-1,1))\n",
    "    frame['s_logECalib'] = scaler_cal[key].transform(frame['logECalib'].to_numpy().reshape(-1,1))\n",
    "    frame['s_eta'] = scaler_eta[key].transform(frame['clusterEta'].to_numpy().reshape(-1,1))\n",
    "\n",
    "for key, frame in data_frames.items():\n",
    "    frame['s_logE'] = scaler_e[key].transform(frame['logE'].to_numpy().reshape(-1,1))\n",
    "    frame['s_logECalib'] = scaler_cal[key].transform(frame['logECalib'].to_numpy().reshape(-1,1))\n",
    "    frame['s_eta'] = scaler_eta[key].transform(frame['clusterEta'].to_numpy().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/local/home/jano/ml4pions/LCStudies/regression/../regression/Models/pion_reweighted/scalers.save']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib as jl\n",
    "# now we save our scalers to a file, so that we can use them when evaluating the model elsewhere\n",
    "scalers = {'e':scaler_e, 'cal':scaler_cal, 'eta':scaler_eta}\n",
    "scaler_file = modelpath + 'scalers.save'\n",
    "jl.dump(scalers,scaler_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish training data preparation, we concatenate the calorimeter images, and then combine them with columns `s_logE` and `s_Eta`.\n",
    "\n",
    "To avoid running out of memory -- which can be an issue depending on where this notebook is run -- we can perform some garbage collection and delete the contents of `calo_images` as they are incorporated into `All_input`, our combined network training input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a handy function for taking a DataFrame (with our scaler-derived columns) and our tree with calo images, and getting the actual network input we need. The `indices` argument is just used for `dtree`, whereas `dframe` should already have the indices applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CombinedInput(dframe, dtree, indices=-1, merge=True):\n",
    "    # Prepare the calo images for input to training.\n",
    "    l = len(layers) * len(dtree.keys())\n",
    "    i = 0\n",
    "    pfx = 'Loading calo images:      '\n",
    "    sfx = 'Complete'\n",
    "    bl = 50\n",
    "    qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=bl)\n",
    "\n",
    "    calo_images = {}\n",
    "    for key in dtree.keys():\n",
    "        calo_images[key] = {}\n",
    "    \n",
    "        for layer in layers:\n",
    "            if(indices != -1): calo_images[key][layer] = mu.setupCells(dtree[key],layer, indices = indices[key])\n",
    "            else: calo_images[key][layer] = mu.setupCells(dtree[key],layer)\n",
    "            i += 1\n",
    "            qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=bl)\n",
    "    \n",
    "    #TODO: A bit hacky, but works for now. Dealing with DataFrames and uproot TTree's together is awful...\n",
    "    # Importantly, the merge here is in the same order as above.\n",
    "    if(merge):\n",
    "        for layer in layers:\n",
    "            calo_images['pp'][layer] = np.row_stack((calo_images['pp'][layer],calo_images['pm'][layer]))\n",
    "        del calo_images['pm']\n",
    "        \n",
    "    # Concatenate images, and prepare our combined input.\n",
    "    All_input = {}\n",
    "    keys = list(calo_images.keys())\n",
    "    l = 3 * len(keys)\n",
    "    i = 0\n",
    "    pfx = 'Preparing combined input: '\n",
    "    qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=bl)\n",
    "\n",
    "    for key in keys:\n",
    "        combined_images = np.concatenate(tuple([calo_images[key][layer] for layer in layers]), axis=1)\n",
    "        del calo_images[key] # delete this element of calo_images, it has been copied and is no longer needed\n",
    "        i = i + 1\n",
    "        qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=bl)\n",
    "\n",
    "        s_combined,scaler_combined = mu.standardCells(combined_images, layers)\n",
    "        i = i + 1\n",
    "        qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=bl)\n",
    "\n",
    "        All_input[key] = np.column_stack((dframe[key]['s_logE'], dframe[key]['s_eta'],s_combined))\n",
    "        i = i + 1\n",
    "        qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=bl)\n",
    "    return All_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading calo images:       |\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m| 100.0% Complete\n",
      "Preparing combined input:  |\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m| 100.0% Complete\n"
     ]
    }
   ],
   "source": [
    "All_input = CombinedInput(training_frames, data_trees,training_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may optionally perform some re-weighting of our training events. If using the `pion_reweighted` strategy, we will re-weight our single-pion training data to match the topo-cluster $p_T$ spectrum of our jet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = {}\n",
    "if(strat == 'pion_reweighted'):\n",
    "    pt_min = 0.\n",
    "    pt_max = 20.\n",
    "    pt_bins = 100\n",
    "    \n",
    "    # Get the jet files for reweighting. We look in jets/ to make sure we only use the relevant files.\n",
    "    jet_files = glob.glob(path_prefix + 'jets/data/*.root')\n",
    "    \n",
    "    for key in training_frames.keys():\n",
    "        h_train    = rt.TH1F(qu.RN(), 'h_train',   pt_bins, pt_min, pt_max)\n",
    "        h_reweight = rt.TH1F(qu.RN(), 'h_reweight',pt_bins, pt_min, pt_max)\n",
    "\n",
    "        # fill training distribution\n",
    "        training_vals = training_frames[key]['clusterPt'].to_numpy()\n",
    "        for entry in training_vals: h_train.Fill(entry)\n",
    "        h_train.Scale(1./h_train.Integral())\n",
    "        \n",
    "        # fill the reweighting distribution\n",
    "        for file in jet_files:\n",
    "            for entry in ur.open(file)['ClusterTree'].array('clusterPt').flatten():\n",
    "                h_reweight.Fill(entry)\n",
    "        h_reweight.Scale(1./h_reweight.Integral())\n",
    "        h_reweight = h_reweight / h_train\n",
    "        \n",
    "        # now get a list of weights for our events\n",
    "        sample_weights[key] = np.array([h_reweight.GetBinContent(h_reweight.FindBin(x)) for x in training_vals])\n",
    "    \n",
    "else:  sample_weights = {key:np.full(len(All_input[key]), 1.) for key in All_input.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \"all\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # disable some of the tensorflow info printouts, only display errors\n",
    "\n",
    "import tensorflow as tf\n",
    "strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\"])\n",
    "ngpu = strategy.num_replicas_in_sync\n",
    "print ('Number of devices: {}'.format(ngpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_nn_All_model():\n",
    "    number_pixels = 512 + 256 + 128 + 16 + 16 + 8\n",
    "    # create model\n",
    "    with strategy.scope():    \n",
    "        model = Sequential()\n",
    "        used_pixels = number_pixels + 2\n",
    "#     if number_pixels < 128:\n",
    "#         used_pixels = 128\n",
    "        model.add(Dense(used_pixels, input_dim=used_pixels, kernel_initializer='normal', activation='relu'))\n",
    "        # model.add(Dropout(0.2))\n",
    "        model.add(Dense(used_pixels, activation='relu'))\n",
    "        # model.add(Dropout(0.2))\n",
    "        model.add(Dense(int(used_pixels/2), activation='relu'))\n",
    "        # model.add(Dropout(0.2))\n",
    "        model.add(Dense(units=1, kernel_initializer='normal', activation='linear'))\n",
    "        opt = Adam(lr=1e-4, decay=1e-6)\n",
    "        model.compile(optimizer=opt, loss='mse',metrics=['mae','mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 938)               880782    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 938)               880782    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 469)               440391    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 470       \n",
      "=================================================================\n",
      "Total params: 2,202,425\n",
      "Trainable params: 2,202,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Print some summary info for this model\n",
    "print(baseline_nn_All_model().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our regressors (actual networks) that we will fit (train).\n",
    "batch_size = 200 * ngpu\n",
    "epochs = 100 # 100\n",
    "verbose = 1\n",
    "regressors = {key: KerasRegressor(build_fn=baseline_nn_All_model, batch_size=batch_size, epochs=epochs, verbose=verbose) for key in All_input.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0490 - mse: 1.5629 - mae: 0.9318\n",
      "Epoch 2/100\n",
      "2612/2612 [==============================] - 13s 5ms/step - loss: 0.0219 - mse: 1.4381 - mae: 0.8840\n",
      "Epoch 3/100\n",
      "2612/2612 [==============================] - 13s 5ms/step - loss: 0.0161 - mse: 1.2631 - mae: 0.8203\n",
      "Epoch 4/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0132 - mse: 0.8020 - mae: 0.6357\n",
      "Epoch 5/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0105 - mse: 0.1351 - mae: 0.2407\n",
      "Epoch 6/100\n",
      "2612/2612 [==============================] - 13s 5ms/step - loss: 0.0090 - mse: 0.0371 - mae: 0.1324\n",
      "Epoch 7/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0083 - mse: 0.0235 - mae: 0.1060\n",
      "Epoch 8/100\n",
      "2612/2612 [==============================] - 13s 5ms/step - loss: 0.0081 - mse: 0.0190 - mae: 0.0968\n",
      "Epoch 9/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0078 - mse: 0.0162 - mae: 0.0888\n",
      "Epoch 10/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0074 - mse: 0.0141 - mae: 0.0830\n",
      "Epoch 11/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0073 - mse: 0.0134 - mae: 0.0809\n",
      "Epoch 12/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0072 - mse: 0.0129 - mae: 0.0792\n",
      "Epoch 13/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0069 - mse: 0.0121 - mae: 0.0764\n",
      "Epoch 14/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0069 - mse: 0.0123 - mae: 0.0769\n",
      "Epoch 15/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0069 - mse: 0.0120 - mae: 0.0759\n",
      "Epoch 16/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0068 - mse: 0.0115 - mae: 0.0741\n",
      "Epoch 17/100\n",
      "2612/2612 [==============================] - 13s 5ms/step - loss: 0.0068 - mse: 0.0117 - mae: 0.0753\n",
      "Epoch 18/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0066 - mse: 0.0112 - mae: 0.0732\n",
      "Epoch 19/100\n",
      "2612/2612 [==============================] - 13s 5ms/step - loss: 0.0066 - mse: 0.0111 - mae: 0.0730\n",
      "Epoch 20/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0066 - mse: 0.0110 - mae: 0.0727\n",
      "Epoch 21/100\n",
      "2612/2612 [==============================] - 13s 5ms/step - loss: 0.0065 - mse: 0.0109 - mae: 0.0727\n",
      "Epoch 22/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0064 - mse: 0.0106 - mae: 0.0715\n",
      "Epoch 23/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0064 - mse: 0.0107 - mae: 0.0717\n",
      "Epoch 24/100\n",
      "2612/2612 [==============================] - 13s 5ms/step - loss: 0.0063 - mse: 0.0108 - mae: 0.0724\n",
      "Epoch 25/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0064 - mse: 0.0105 - mae: 0.0710\n",
      "Epoch 26/100\n",
      "2612/2612 [==============================] - 13s 5ms/step - loss: 0.0063 - mse: 0.0105 - mae: 0.0712\n",
      "Epoch 27/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0063 - mse: 0.0104 - mae: 0.0713\n",
      "Epoch 28/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0062 - mse: 0.0105 - mae: 0.0719\n",
      "Epoch 29/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0062 - mse: 0.0103 - mae: 0.0702\n",
      "Epoch 30/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0061 - mse: 0.0102 - mae: 0.0703\n",
      "Epoch 31/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0062 - mse: 0.0102 - mae: 0.0703\n",
      "Epoch 32/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0061 - mse: 0.0100 - mae: 0.0697\n",
      "Epoch 33/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0060 - mse: 0.0100 - mae: 0.0698\n",
      "Epoch 34/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0060 - mse: 0.0104 - mae: 0.0718\n",
      "Epoch 35/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0060 - mse: 0.0101 - mae: 0.0701\n",
      "Epoch 36/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0060 - mse: 0.0100 - mae: 0.0699\n",
      "Epoch 37/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0059 - mse: 0.0099 - mae: 0.0698\n",
      "Epoch 38/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0059 - mse: 0.0100 - mae: 0.0702\n",
      "Epoch 39/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0059 - mse: 0.0098 - mae: 0.0694\n",
      "Epoch 40/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0059 - mse: 0.0101 - mae: 0.0706\n",
      "Epoch 41/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0058 - mse: 0.0100 - mae: 0.0704\n",
      "Epoch 42/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0059 - mse: 0.0096 - mae: 0.0683\n",
      "Epoch 43/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0058 - mse: 0.0099 - mae: 0.0699\n",
      "Epoch 44/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0058 - mse: 0.0098 - mae: 0.0694\n",
      "Epoch 45/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0058 - mse: 0.0099 - mae: 0.0696\n",
      "Epoch 46/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0058 - mse: 0.0097 - mae: 0.0689\n",
      "Epoch 47/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0057 - mse: 0.0096 - mae: 0.0685\n",
      "Epoch 48/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0057 - mse: 0.0099 - mae: 0.0700\n",
      "Epoch 49/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0056 - mse: 0.0095 - mae: 0.0681\n",
      "Epoch 50/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0057 - mse: 0.0099 - mae: 0.0704\n",
      "Epoch 51/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0056 - mse: 0.0096 - mae: 0.0688\n",
      "Epoch 52/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0056 - mse: 0.0097 - mae: 0.0690\n",
      "Epoch 53/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0056 - mse: 0.0099 - mae: 0.0700\n",
      "Epoch 54/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0055 - mse: 0.0097 - mae: 0.0692\n",
      "Epoch 55/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0055 - mse: 0.0097 - mae: 0.0693\n",
      "Epoch 56/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0055 - mse: 0.0096 - mae: 0.0688\n",
      "Epoch 57/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0055 - mse: 0.0096 - mae: 0.0689\n",
      "Epoch 58/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0055 - mse: 0.0096 - mae: 0.0686\n",
      "Epoch 59/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0054 - mse: 0.0096 - mae: 0.0686\n",
      "Epoch 60/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0054 - mse: 0.0095 - mae: 0.0683\n",
      "Epoch 61/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0054 - mse: 0.0094 - mae: 0.0677\n",
      "Epoch 62/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0054 - mse: 0.0095 - mae: 0.0684\n",
      "Epoch 63/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0054 - mse: 0.0092 - mae: 0.0670\n",
      "Epoch 64/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0053 - mse: 0.0094 - mae: 0.0678\n",
      "Epoch 65/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0053 - mse: 0.0093 - mae: 0.0672\n",
      "Epoch 66/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0053 - mse: 0.0093 - mae: 0.0673\n",
      "Epoch 67/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0052 - mse: 0.0091 - mae: 0.0663\n",
      "Epoch 68/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0052 - mse: 0.0092 - mae: 0.0671\n",
      "Epoch 69/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0052 - mse: 0.0093 - mae: 0.0675\n",
      "Epoch 70/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0052 - mse: 0.0093 - mae: 0.0673\n",
      "Epoch 71/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0052 - mse: 0.0092 - mae: 0.0673\n",
      "Epoch 72/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0052 - mse: 0.0091 - mae: 0.0664\n",
      "Epoch 73/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0051 - mse: 0.0089 - mae: 0.0655\n",
      "Epoch 74/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0051 - mse: 0.0093 - mae: 0.0673\n",
      "Epoch 75/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0051 - mse: 0.0088 - mae: 0.0652\n",
      "Epoch 76/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0051 - mse: 0.0089 - mae: 0.0655\n",
      "Epoch 77/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0051 - mse: 0.0088 - mae: 0.0653\n",
      "Epoch 78/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0051 - mse: 0.0090 - mae: 0.0663\n",
      "Epoch 79/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0051 - mse: 0.0091 - mae: 0.0668\n",
      "Epoch 80/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0050 - mse: 0.0090 - mae: 0.0661\n",
      "Epoch 81/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0050 - mse: 0.0093 - mae: 0.0676\n",
      "Epoch 82/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0050 - mse: 0.0089 - mae: 0.0659\n",
      "Epoch 83/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0049 - mse: 0.0086 - mae: 0.0643\n",
      "Epoch 84/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0050 - mse: 0.0089 - mae: 0.0658\n",
      "Epoch 85/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0049 - mse: 0.0088 - mae: 0.0652\n",
      "Epoch 86/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0049 - mse: 0.0088 - mae: 0.0655\n",
      "Epoch 87/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0049 - mse: 0.0088 - mae: 0.0659\n",
      "Epoch 88/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0049 - mse: 0.0086 - mae: 0.0643\n",
      "Epoch 89/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0049 - mse: 0.0090 - mae: 0.0665\n",
      "Epoch 90/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0049 - mse: 0.0088 - mae: 0.0654\n",
      "Epoch 91/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0048 - mse: 0.0086 - mae: 0.0649\n",
      "Epoch 92/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0048 - mse: 0.0085 - mae: 0.0641\n",
      "Epoch 93/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0048 - mse: 0.0088 - mae: 0.0657\n",
      "Epoch 94/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0048 - mse: 0.0087 - mae: 0.0649\n",
      "Epoch 95/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0047 - mse: 0.0085 - mae: 0.0643\n",
      "Epoch 96/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0047 - mse: 0.0089 - mae: 0.0661\n",
      "Epoch 97/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0047 - mse: 0.0083 - mae: 0.0631\n",
      "Epoch 98/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0046 - mse: 0.0085 - mae: 0.0640\n",
      "Epoch 99/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0046 - mse: 0.0083 - mae: 0.0633\n",
      "Epoch 100/100\n",
      "2612/2612 [==============================] - 12s 5ms/step - loss: 0.0046 - mse: 0.0085 - mae: 0.0644\n",
      "Epoch 1/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 2.0680 - mse: 2.4724 - mae: 1.2371\n",
      "Epoch 2/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.4985 - mse: 2.3413 - mae: 1.2011\n",
      "Epoch 3/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 10.2999 - mse: 2.3814 - mae: 1.2106\n",
      "Epoch 4/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.6036 - mse: 2.2415 - mae: 1.1767\n",
      "Epoch 5/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 3.2172 - mse: 2.2695 - mae: 1.1792\n",
      "Epoch 6/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0298 - mse: 2.1493 - mae: 1.1402\n",
      "Epoch 7/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.3505 - mse: 2.0767 - mae: 1.1201\n",
      "Epoch 8/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 1.7459 - mse: 2.0728 - mae: 1.1158\n",
      "Epoch 9/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0898 - mse: 2.0399 - mae: 1.1043\n",
      "Epoch 10/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.7622 - mse: 2.0111 - mae: 1.0958\n",
      "Epoch 11/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0220 - mse: 1.9305 - mae: 1.0679\n",
      "Epoch 12/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0620 - mse: 1.9459 - mae: 1.0711\n",
      "Epoch 13/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.4339 - mse: 1.8953 - mae: 1.0569\n",
      "Epoch 14/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.9694 - mse: 1.8342 - mae: 1.0379\n",
      "Epoch 15/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.1038 - mse: 1.6859 - mae: 0.9956\n",
      "Epoch 16/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.1446 - mse: 1.6092 - mae: 0.9737\n",
      "Epoch 17/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.4981 - mse: 1.6305 - mae: 0.9757\n",
      "Epoch 18/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.1598 - mse: 1.6746 - mae: 0.9888\n",
      "Epoch 19/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.6817 - mse: 1.5549 - mae: 0.9516\n",
      "Epoch 20/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0172 - mse: 1.4326 - mae: 0.9111\n",
      "Epoch 21/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.1466 - mse: 1.3885 - mae: 0.8969\n",
      "Epoch 22/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.5130 - mse: 1.3357 - mae: 0.8806\n",
      "Epoch 23/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0786 - mse: 1.3547 - mae: 0.8852\n",
      "Epoch 24/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0644 - mse: 1.1984 - mae: 0.8291\n",
      "Epoch 25/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.1207 - mse: 1.0845 - mae: 0.7909\n",
      "Epoch 26/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.1596 - mse: 0.9389 - mae: 0.7341\n",
      "Epoch 27/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0196 - mse: 0.7864 - mae: 0.6739\n",
      "Epoch 28/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.3100 - mse: 0.6733 - mae: 0.6301\n",
      "Epoch 29/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0216 - mse: 0.6606 - mae: 0.6190\n",
      "Epoch 30/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0335 - mse: 0.4251 - mae: 0.4991\n",
      "Epoch 31/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.1424 - mse: 0.2591 - mae: 0.3911\n",
      "Epoch 32/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0566 - mse: 0.2109 - mae: 0.3524\n",
      "Epoch 33/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0860 - mse: 0.1285 - mae: 0.2711\n",
      "Epoch 34/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.1605 - mse: 0.0987 - mae: 0.2286\n",
      "Epoch 35/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0113 - mse: 0.0447 - mae: 0.1485\n",
      "Epoch 36/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0157 - mse: 0.0537 - mae: 0.1631\n",
      "Epoch 37/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.1098 - mse: 0.0593 - mae: 0.1692\n",
      "Epoch 38/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0363 - mse: 0.0316 - mae: 0.1267\n",
      "Epoch 39/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0188 - mse: 0.0256 - mae: 0.1130\n",
      "Epoch 40/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0607 - mse: 0.0375 - mae: 0.1346\n",
      "Epoch 41/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0269 - mse: 0.0227 - mae: 0.1060\n",
      "Epoch 42/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0196 - mse: 0.0227 - mae: 0.1049\n",
      "Epoch 43/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0452 - mse: 0.0227 - mae: 0.1057\n",
      "Epoch 44/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0784 - mse: 0.0373 - mae: 0.1308\n",
      "Epoch 45/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0209 - mse: 0.0235 - mae: 0.1078\n",
      "Epoch 46/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0250 - mse: 0.0163 - mae: 0.0868\n",
      "Epoch 47/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0373 - mse: 0.0249 - mae: 0.1060\n",
      "Epoch 48/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0630 - mse: 0.0182 - mae: 0.0945\n",
      "Epoch 49/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0041 - mse: 0.0128 - mae: 0.0770\n",
      "Epoch 50/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0070 - mse: 0.0184 - mae: 0.0888\n",
      "Epoch 51/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.1039 - mse: 0.0256 - mae: 0.1093\n",
      "Epoch 52/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0246 - mse: 0.0118 - mae: 0.0724\n",
      "Epoch 53/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0159 - mse: 0.0120 - mae: 0.0746\n",
      "Epoch 54/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0214 - mse: 0.0106 - mae: 0.0676\n",
      "Epoch 55/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0183 - mse: 0.0109 - mae: 0.0718\n",
      "Epoch 56/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0336 - mse: 0.0130 - mae: 0.0778\n",
      "Epoch 57/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0207 - mse: 0.0161 - mae: 0.0844\n",
      "Epoch 58/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0084 - mse: 0.0138 - mae: 0.0819\n",
      "Epoch 59/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0063 - mse: 0.0176 - mae: 0.0865\n",
      "Epoch 60/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0424 - mse: 0.0112 - mae: 0.0710\n",
      "Epoch 61/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0034 - mse: 0.0074 - mae: 0.0566\n",
      "Epoch 62/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0411 - mse: 0.0095 - mae: 0.0665\n",
      "Epoch 63/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0182 - mse: 0.0078 - mae: 0.0572\n",
      "Epoch 64/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0024 - mse: 0.0069 - mae: 0.0563\n",
      "Epoch 65/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0030 - mse: 0.0095 - mae: 0.0661\n",
      "Epoch 66/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0294 - mse: 0.0084 - mae: 0.0633\n",
      "Epoch 67/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0067 - mse: 0.0068 - mae: 0.0565\n",
      "Epoch 68/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0256 - mse: 0.0080 - mae: 0.0611\n",
      "Epoch 69/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0099 - mse: 0.0075 - mae: 0.0608\n",
      "Epoch 70/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0142 - mse: 0.0078 - mae: 0.0596\n",
      "Epoch 71/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0212 - mse: 0.0061 - mae: 0.0533\n",
      "Epoch 72/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0094 - mse: 0.0088 - mae: 0.0635\n",
      "Epoch 73/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0097 - mse: 0.0073 - mae: 0.0587\n",
      "Epoch 74/100\n",
      "985/985 [==============================] - 4s 5ms/step - loss: 0.0173 - mse: 0.0058 - mae: 0.0515\n",
      "Epoch 75/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0295 - mse: 0.0087 - mae: 0.0652\n",
      "Epoch 76/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0130 - mse: 0.0057 - mae: 0.0511\n",
      "Epoch 77/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0104 - mse: 0.0065 - mae: 0.0556\n",
      "Epoch 78/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0212 - mse: 0.0066 - mae: 0.0565\n",
      "Epoch 79/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0104 - mse: 0.0056 - mae: 0.0530\n",
      "Epoch 80/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0205 - mse: 0.0070 - mae: 0.0572\n",
      "Epoch 81/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0188 - mse: 0.0048 - mae: 0.0488\n",
      "Epoch 82/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0105 - mse: 0.0076 - mae: 0.0600\n",
      "Epoch 83/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0032 - mse: 0.0042 - mae: 0.0453\n",
      "Epoch 84/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0121 - mse: 0.0058 - mae: 0.0531\n",
      "Epoch 85/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0092 - mse: 0.0064 - mae: 0.0558\n",
      "Epoch 86/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0092 - mse: 0.0100 - mae: 0.0600\n",
      "Epoch 87/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0515 - mse: 0.0083 - mae: 0.0570\n",
      "Epoch 88/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0062 - mse: 0.0044 - mae: 0.0465\n",
      "Epoch 89/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0032 - mse: 0.0041 - mae: 0.0447\n",
      "Epoch 90/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0176 - mse: 0.0098 - mae: 0.0652\n",
      "Epoch 91/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0562 - mse: 0.0047 - mae: 0.0482\n",
      "Epoch 92/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0021 - mse: 0.0056 - mae: 0.0499\n",
      "Epoch 93/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0020 - mse: 0.0075 - mae: 0.0560\n",
      "Epoch 94/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0104 - mse: 0.0055 - mae: 0.0508\n",
      "Epoch 95/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0270 - mse: 0.0048 - mae: 0.0493\n",
      "Epoch 96/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0190 - mse: 0.0051 - mae: 0.0508\n",
      "Epoch 97/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0030 - mse: 0.0047 - mae: 0.0489\n",
      "Epoch 98/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0023 - mse: 0.0034 - mae: 0.0409\n",
      "Epoch 99/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0292 - mse: 0.0056 - mae: 0.0502\n",
      "Epoch 100/100\n",
      "985/985 [==============================] - 5s 5ms/step - loss: 0.0312 - mse: 0.0053 - mae: 0.0524\n"
     ]
    }
   ],
   "source": [
    "# Train the models and save them, or load them from files.\n",
    "model_file_names = {'pp':modelpath + 'all_charged.h5','p0':modelpath + 'all_neutral.h5'}\n",
    "assert (set(model_file_names.keys()) == set(All_input.keys()))\n",
    "histories = {}\n",
    "\n",
    "for key, filename in model_file_names.items():\n",
    "    if not loadModel: histories[key] = regressors[key].fit(All_input[key], training_frames[key]['s_logECalib'], sample_weight = sample_weights[key])\n",
    "    else: regressors[key].model = load_model(model_file_names[key])\n",
    "\n",
    "    if saveModel: regressors[key].model.save(model_file_names[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the networks on *all* our data to see how they've done. This will involve overwriting some training data variables that we won't need for plotting results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions for training data.\n",
      "2612/2612 [==============================] - 6s 2ms/step\n",
      "985/985 [==============================] - 2s 2ms/step\n",
      "Getting predictions for testing data.\n",
      "Loading calo images:       |\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m| 100.0% Complete\n",
      "Preparing combined input:  |\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m| 100.0% Complete\n",
      "653/653 [==============================] - 1s 2ms/step\n",
      "247/247 [==============================] - 0s 2ms/step\n",
      "Loading calo images:       |\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m| 100.0% Complete\n",
      "Preparing combined input:  |\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m| 100.0% Complete\n",
      "4353/4353 [==============================] - 10s 2ms/step\n",
      "1320/1320 [==============================] - 3s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# First, the training data\n",
    "print('Getting predictions for training data.')\n",
    "for key, frame in training_frames.items():\n",
    "    frame['clusterE_pred'] = np.exp(scaler_cal[key].inverse_transform(regressors[key].predict(All_input[key])))\n",
    "\n",
    "# Now regenerate All_input for our testing data, and get predictions for it\n",
    "print('Getting predictions for testing data.')\n",
    "All_input = CombinedInput(validation_frames, data_trees, validation_indices)\n",
    "for key, frame in validation_frames.items():\n",
    "    frame['clusterE_pred'] = np.exp(scaler_cal[key].inverse_transform(regressors[key].predict(All_input[key])))\n",
    "\n",
    "# Now regenerate All_input for training + testing, get predictions.\n",
    "All_input = CombinedInput(data_frames, data_trees, data_indices)\n",
    "for key, frame in data_frames.items():\n",
    "    frame['clusterE_pred'] = np.exp(scaler_cal[key].inverse_transform(regressors[key].predict(All_input[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting results (testing how well our network works)\n",
    "\n",
    "Now, let's plot some kinematics and network results. We'll make two groups of plots -- one for charged pions and one for neutral pions.\n",
    "\n",
    "Within each group of plots, we'll make two plots for each quantity -- one made using just the training data, and then one made using all the data (training + whatever we excluded -- but still excluding events with `cluster_ENG_CALIB_TOT` $< 0$ since these blow up network output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Median(values, counts, default = 1):\n",
    "    nvar = len(values)\n",
    "    assert(len(counts) == nvar)\n",
    "    if(nvar == 0): return default\n",
    "    value_list = [[values[i] for j in range(int(counts[i]))] for i in range(nvar)]\n",
    "    value_array = [item for sublist in value_list for item in sublist]\n",
    "    value_array = np.array(value_array)\n",
    "    if(len(value_array) == 0): return default\n",
    "    med = np.median(value_array)\n",
    "    return med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SetColor(hist, color, alpha = 1., style=0):\n",
    "    hist.SetFillColorAlpha(color, alpha)\n",
    "    hist.SetLineColor(color)\n",
    "    if(style != 0): hist.SetFillStyle(style)\n",
    "    return\n",
    "\n",
    "# a simple version of Max's cool 2D plots\n",
    "def EnergyPlot2D(e1, e2, title='title;x;y', nbins = [100,35], x_range = [0.,2000.], y_range = [0.3, 1.7], offset=False, mode = 'median'):\n",
    "    hist = rt.TH2F(qu.RN(), title, nbins[0], x_range[0], x_range[1], nbins[1], y_range[0], y_range[1])\n",
    "    x_vals = e2\n",
    "    y_vals = e1/e2\n",
    "    if(offset): x_vals = x_vals + 1.\n",
    "    \n",
    "    for i in range(len(e2)):\n",
    "        hist.Fill(x_vals[i],y_vals[i])\n",
    "        \n",
    "    # now we want to make a curve representing the medians/means in y\n",
    "    bin_centers_y = np.array([hist.GetYaxis().GetBinCenter(i+1) for i in range(nbins[1])])\n",
    "    mean_vals = np.zeros(nbins[0])\n",
    "    for i in range(nbins[0]):\n",
    "        weights = np.array([hist.GetBinContent(i+1, j+1) for j in range(nbins[1])])\n",
    "        if(mode == 'median'):\n",
    "            mean_vals[i] = Median(bin_centers_y,np.array(weights,dtype=np.dtype('i8')))\n",
    "        else:\n",
    "            if(np.sum(weights) != 0.):\n",
    "                weights = weights / np.sum(weights)\n",
    "                y_vals = np.array([hist.GetYaxis().GetBinCenter(j+1) for j in range(nbins[1])])\n",
    "                y_vals = np.multiply(y_vals,weights)\n",
    "                mean_vals[i] = np.sum(y_vals)\n",
    "            else: mean_vals[i] = 1.\n",
    "        \n",
    "    x_vals = np.array([hist.GetXaxis().GetBinCenter(i+1) for i in range(nbins[0])])\n",
    "    curve = rt.TGraph(nbins[0],x_vals, mean_vals)\n",
    "    curve.SetLineColor(rt.kRed)\n",
    "    curve.SetLineWidth(2)\n",
    "    return curve, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some ranges for the plots\n",
    "if(strat == 'pion' or strat == 'pion_reweighted'):\n",
    "    max_energy = 2000. # GeV\n",
    "    max_energy_2d = max_energy\n",
    "    bin_energy = 300\n",
    "    ratio_range_2d = [0.3, 1.7]\n",
    "    bins_2d = [200,70]\n",
    "    offset_2d = False\n",
    "    \n",
    "else:\n",
    "    max_energy = 100\n",
    "    max_energy_2d = 10.\n",
    "    bin_energy = 20\n",
    "    ratio_range_2d = [0., 5.]\n",
    "    bins_2d = [50,125]\n",
    "    offset_2d = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_frames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b038904c03b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m dsets = {\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_frames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;34m'valid'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalidation_frames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;34m'all data'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata_frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_frames' is not defined"
     ]
    }
   ],
   "source": [
    "clusterE = {}\n",
    "clusterE_calib = {}\n",
    "clusterE_pred = {}\n",
    "clusterE_true = {}\n",
    "clusterE_ratio1 = {} # ratio of predicted cluster E to calibrated cluster E\n",
    "clusterE_ratio2 = {} # ratio of reco cluster E to calibrated cluster E\n",
    "clusterE_ratio2D = {} # 2D plot, ratio1 versus calibrated cluster E\n",
    "clusterE_ratio2D_zoomed = {} # 2D plot, ratio1 versus calibrated cluster E, zoomed\n",
    "\n",
    "mean_curves = {}\n",
    "mean_curves_zoomed = {}\n",
    "\n",
    "canvs = {}\n",
    "legends = {}\n",
    "stacks = {}\n",
    "\n",
    "key_conversions = {\n",
    "    'pp':'#pi^{#pm}',\n",
    "    'p0':'#pi^{0}',\n",
    "    'check':'#pi^{+} (check)'}\n",
    "\n",
    "dsets = {\n",
    "    'train': training_frames,\n",
    "    'valid': validation_frames,\n",
    "    'all data': data_frames\n",
    "}\n",
    "\n",
    "extensions = ['pdf','png']\n",
    "plot_size = 750\n",
    "\n",
    "for key in ['p0','pp']:\n",
    "    clusterE[key] = {}\n",
    "    clusterE_calib[key] = {}\n",
    "    clusterE_pred[key] = {}\n",
    "    clusterE_true[key] = {}\n",
    "    clusterE_ratio1[key] = {}\n",
    "    clusterE_ratio2[key] = {}\n",
    "    clusterE_ratio2D[key] = {}\n",
    "    clusterE_ratio2D_zoomed[key] = {}\n",
    "    \n",
    "    mean_curves[key] = {}\n",
    "    mean_curves_zoomed[key] = {}\n",
    "\n",
    "    canvs[key] = {}\n",
    "    legends[key] = {}\n",
    "    stacks[key] = {}\n",
    "    \n",
    "    for dkey, frame in dsets.items():\n",
    "        key2 = '(' + key_conversions[key] + ', ' + dkey + ')'\n",
    "        clusterE[key][dkey] = rt.TH1F(qu.RN(), 'E_{reco} ' + key2 +'; E_{reco} [GeV];Count' , bin_energy,0.,max_energy)\n",
    "        clusterE_calib[key][dkey] = rt.TH1F(qu.RN(), 'E_{calib}^{tot} ' + key2 + ';E_{calib}^{tot} [GeV];Count', bin_energy,0.,max_energy)\n",
    "        clusterE_pred[key][dkey] = rt.TH1F(qu.RN(), 'E_{pred} ' + key2 + ';E_{pred} [GeV];Count', bin_energy,0.,max_energy)\n",
    "        clusterE_true[key][dkey] = rt.TH1F(qu.RN(), 'E_{true} ' + key2 + ';E_{true} [GeV];Count', bin_energy,0.,max_energy)\n",
    "        clusterE_ratio1[key][dkey] = rt.TH1F(qu.RN(), 'E_{pred} / E_{calib}^{tot} ' + key2 + ';E_{pred}/E_{calib}^{tot};Count', 250,0.,10.)\n",
    "        clusterE_ratio2[key][dkey] = rt.TH1F(qu.RN(), 'E / E_{calib}^{tot} ' + key2 + ';E_{reco}/E_{calib}^{tot]};Count', 250,0.,10.)\n",
    "\n",
    "        SetColor(clusterE[key][dkey], rt.kViolet, alpha = 0.4)\n",
    "        SetColor(clusterE_calib[key][dkey], rt.kPink + 9, alpha = 0.4)\n",
    "        SetColor(clusterE_pred[key][dkey], rt.kGreen, alpha = 0.4)\n",
    "        SetColor(clusterE_true[key][dkey], rt.kRed, alpha = 0.4)\n",
    "        SetColor(clusterE_ratio1[key][dkey], rt.kViolet, alpha = 0.4)\n",
    "        SetColor(clusterE_ratio2[key][dkey], rt.kGreen, alpha = 0.4)\n",
    "\n",
    "        meas   = frame[key]['clusterE'].to_numpy()\n",
    "        calib  = frame[key]['cluster_ENG_CALIB_TOT'].to_numpy()\n",
    "        pred   = frame[key]['clusterE_pred'].to_numpy()\n",
    "        true   = frame[key]['truthE'].to_numpy()\n",
    "        ratio1 = pred / calib\n",
    "        ratio2 = meas / calib\n",
    "    \n",
    "        for i in range(len(meas)):\n",
    "            clusterE[key][dkey].Fill(meas[i])\n",
    "            clusterE_calib[key][dkey].Fill(calib[i])\n",
    "            clusterE_pred[key][dkey].Fill(pred[i])\n",
    "            clusterE_true[key][dkey].Fill(true[i])\n",
    "            clusterE_ratio1[key][dkey].Fill(ratio1[i])\n",
    "            clusterE_ratio2[key][dkey].Fill(ratio2[i])\n",
    "            \n",
    "        # fill the histogram stacks\n",
    "        stacks[key][dkey] = rt.THStack(qu.RN(), clusterE_ratio1[key][dkey].GetTitle())\n",
    "        stacks[key][dkey].Add(clusterE_ratio1[key][dkey])\n",
    "        stacks[key][dkey].Add(clusterE_ratio2[key][dkey])\n",
    "\n",
    "        # 2D plots\n",
    "        title = 'E_{pred}/E_{calib}^{tot} vs. E_{calib}^{tot} ' + key2 + ';E_{calib}^{tot} [GeV];E_{pred}/E_{calib}^{tot};Count'\n",
    "        x_range = [0.,max_energy_2d]\n",
    "        nbins = bins_2d\n",
    "        mean_curves[key][dkey], clusterE_ratio2D[key][dkey] = EnergyPlot2D(pred, calib, nbins = nbins, x_range = x_range, y_range = ratio_range_2d, title=title, offset=True)\n",
    "\n",
    "        title = 'E_{pred}/E_{calib}^{tot} vs. E_{calib}^{tot} ' + key2 + ';(E_{calib}^{tot} + 1) [GeV];E_{pred}/E_{calib}^{tot};Count'\n",
    "        x_range = [1.,1. + 0.01 * max_energy_2d]\n",
    "        nbins = bins_2d\n",
    "        nbins[0] = nbins[0] - 1\n",
    "        mean_curves_zoomed[key][dkey], clusterE_ratio2D_zoomed[key][dkey] = EnergyPlot2D(pred, calib, nbins = nbins, x_range = x_range, y_range = ratio_range_2d, title=title, offset=False)\n",
    "\n",
    "    plots = [clusterE, clusterE_calib, clusterE_pred, clusterE_true, stacks, clusterE_ratio2D, clusterE_ratio2D_zoomed]\n",
    "    dkeys = list(dsets.keys())\n",
    "    # make legend for the overlapping plots\n",
    "    legends[key] = rt.TLegend(0.7,0.7,0.85,0.85)\n",
    "    legends[key].SetBorderSize(0)\n",
    "    legends[key].AddEntry(clusterE_ratio1[key][dkeys[0]],'x = pred','f')\n",
    "    legends[key].AddEntry(clusterE_ratio2[key][dkeys[0]],'x = reco','f')\n",
    "    \n",
    "    nx = len(dkeys)\n",
    "    ny = len(plots)\n",
    "    canvs[key] = rt.TCanvas(qu.RN(),'c_'+str(key),nx * plot_size,ny * plot_size)\n",
    "    canvs[key].Divide(nx,ny)\n",
    "    \n",
    "    for i, plot in enumerate(plots):\n",
    "        \n",
    "        if(plot == stacks):\n",
    "            x = nx * i + 1\n",
    "            for j, dkey in enumerate(dkeys):\n",
    "                canvs[key].cd(x + j)\n",
    "                plot[key][dkey].Draw('NOSTACK HIST')\n",
    "                rt.gPad.SetLogy()\n",
    "                rt.gPad.SetGrid()\n",
    "                plot[key][dkey].GetHistogram().GetXaxis().SetTitle('E_{x}/E_{calib}^{tot}')\n",
    "                plot[key][dkey].GetHistogram().GetYaxis().SetTitle(clusterE_ratio1[key][dkey].GetYaxis().GetTitle())\n",
    "                \n",
    "                if(strat == 'jet'):\n",
    "                    plot[key][dkey].SetMinimum(5.0e-1)\n",
    "                    plot[key][dkey].SetMaximum(1.0e3)\n",
    "                    \n",
    "                else:\n",
    "                    plot[key][dkey].SetMinimum(5.0e-1)\n",
    "                    plot[key][dkey].SetMaximum(2.0e5)   \n",
    "                    \n",
    "                legends[key].SetTextColor(dark_style.text)\n",
    "                legends[key].Draw()\n",
    "\n",
    "        elif(plot == clusterE_ratio2D or plot == clusterE_ratio2D_zoomed):\n",
    "            x = nx * i + 1\n",
    "            for j, dkey in enumerate(dkeys):\n",
    "                canvs[key].cd(x + j)\n",
    "                plot[key][dkey].Draw('COLZ')\n",
    "                if(plot == clusterE_ratio2D):\n",
    "                    mean_curves[key][dkey].Draw('SAME')\n",
    "\n",
    "                else: \n",
    "                    mean_curves_zoomed[key][dkey].Draw('SAME')\n",
    "                    rt.gPad.SetLogx()\n",
    "                    rt.gPad.SetBottomMargin(0.15)\n",
    "                    plot[key][dkey].GetXaxis().SetTitleOffset(1.5)\n",
    "                    \n",
    "                rt.gPad.SetLogz()\n",
    "                rt.gPad.SetRightMargin(0.2)\n",
    "                plot[key][dkey].GetXaxis().SetMaxDigits(4)\n",
    "                  \n",
    "        else:\n",
    "            x = nx * i + 1\n",
    "            for j, dkey in enumerate(dkeys):\n",
    "                canvs[key].cd(x + j)\n",
    "                plot[key][dkey].Draw('HIST')\n",
    "                rt.gPad.SetLogy()\n",
    "    \n",
    "    # Draw the canvas\n",
    "    canvs[key].Draw()\n",
    "    \n",
    "    # Save the canvas as a PDF & PNG image.\n",
    "    image_name = key + '_plots'\n",
    "    for ext in extensions: canvs[key].SaveAs(plotpath + image_name + '.' + ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `pion`:\n",
    "\n",
    "    - Both of our regressions appear to be working quite well -- we see that our predicted energy (bottom left plot, dark blue) outperforms the reconstructed energy (bottom left plot, cyan) as the peaks aren't as wide while still being centered on $1$ as we would like.\n",
    "    \n",
    "For `jet`:\n",
    "\n",
    "    - Note that our energy range is quite different than with our `pion` training data. It seems that the isolated pions were often far more energetic -- or at least led to far more energetic topo-clusters -- than do our jets. The results don't look terrible when evaluating within the training set, but we see that including events that weren't used for training, which corresponds with very low `cluster_ENG_CALIB_TOT` values, throws things off. The network extrapolates very poorly towards these lower energies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Just double-checking the range of cluster_ENG_CALIB_TOT in the training/plotting data\n",
    "# minimum = np.min(data_frames['pp']['cluster_ENG_CALIB_TOT'].to_numpy())\n",
    "# maximum = np.max(data_frames['pp']['cluster_ENG_CALIB_TOT'].to_numpy())\n",
    "# print('({val1:.1e},{val2:.1e})'.format(val1=minimum, val2=maximum))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
