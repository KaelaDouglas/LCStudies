{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification of ATLAS Calorimeter Topo-Clusters (Jan)\n",
    "\n",
    "## This is a stripped-down version of Max's re-write, so I have removed *some* functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick Navigation:\n",
    "- [Simple feed-forward Neural Network](#Simple-feed-forward-Neural-Network.)\n",
    "- [ROC Curve Scans](#ROC-Curve-Scans)\n",
    "- [Combination Network](#Combination-Network)\n",
    "- [Convolutional Neural Network](#Convolutional-Neural-Network)\n",
    "- [Correlation Plots](#Correlation-Plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make sure that we have `latex` set up correctly. \n",
    "\n",
    "We will need this for the `atlas_mpl_style` package, which is used throughout some of ML4Pion's pre-existing utilities. As of October 22, 2020, the [UChicago ML platform](ml.maniac.uchicago.edu) does *not* have `latex` pre-installed. We can take care of this with our own installation script -- note that installed `latex` with `conda` [does not work well at the moment](https://github.com/conda-forge/texlive-core-feedstock/issues/19) so we fall back on the slower, regular method for installing `texlive` -- but we haven't made the necessary addition to our `$PATH` for `latex` so we must set it now locally for the notebook. (We avoid touching the `.bash_profile` on the ML platform or [making a custom Jupyter kernel](https://stackoverflow.com/a/53595397) for now.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/local/home/jano/miniconda3/envs/ml4p/bin:/local/home/jano/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/local/home/jano/texlive/2020/bin/x86_64-linux\n"
     ]
    }
   ],
   "source": [
    "# Check if latex is set up already.\n",
    "# We use some Jupyter magic -- alternatively one could use python's subprocess here.\n",
    "has_latex = !command -v latex\n",
    "has_latex = (not has_latex == [])\n",
    "\n",
    "# If latex was not a recognized command, our setup script should have installed\n",
    "# at a fixed location, but it is not on the $PATH. Now let's use some Jupyter magic.\n",
    "# See https://ipython.readthedocs.io/en/stable/interactive/shell.html for info.\n",
    "if(not has_latex):\n",
    "    latex_prefix = '/local/home/jano/texlive/2020/bin/x86_64-linux' # '/usr/local/texlive/2020/bin/x86_64-linux'\n",
    "    jupyter_env = %env\n",
    "    path = jupyter_env['PATH']\n",
    "    path = path + ':' + latex_prefix\n",
    "    %env PATH = $path\n",
    "    jupyter_env = %env\n",
    "    path = jupyter_env['PATH']\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.22/02\n"
     ]
    }
   ],
   "source": [
    "#import libraries and some constants\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize, LogNorm\n",
    "import pandas as pd\n",
    "import ROOT as rt # I will use this for some plotting\n",
    "import uproot as ur\n",
    "import atlas_mpl_style as ampl\n",
    "ampl.use_atlas_style()\n",
    "\n",
    "params = {'legend.fontsize': 13,\n",
    "          'axes.labelsize': 18}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "path_prefix = os.getcwd() + '/../'\n",
    "plotpath = path_prefix+'classifier/Plots/'\n",
    "modelpath = path_prefix+'classifier/Models/'\n",
    "# %config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# metadata\n",
    "layers = [\"EMB1\", \"EMB2\", \"EMB3\", \"TileBar0\", \"TileBar1\", \"TileBar2\"]\n",
    "cell_size_phi = [0.098, 0.0245, 0.0245, 0.1, 0.1, 0.1]\n",
    "cell_size_eta = [0.0031, 0.025, 0.05, 0.1, 0.1, 0.2]\n",
    "len_phi = [4, 16, 16, 4, 4, 4]\n",
    "len_eta = [128, 16, 8, 4, 4, 2]\n",
    "cell_shapes = {layers[i]:(len_eta[i],len_phi[i]) for i in range(len(layers))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fancy display names for each pion type\n",
    "pi_latex = {\n",
    "    'pi0': '\\(\\pi^{0}\\)',\n",
    "    'piplus': '\\(\\pi^{+}\\)',\n",
    "    'piminus': '\\(\\pi^{-}\\)',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import our resolution utilities. These take care of some plotting, using `matplotlib` and the `atlas_mpl_style` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prefix = os.getcwd() + '/../'\n",
    "if(path_prefix not in sys.path): sys.path.append(path_prefix)\n",
    "from util import resolution_util as ru\n",
    "from util import plot_util as pu\n",
    "from util import ml_util as mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will import our data from the `ROOT` files into a `pandas` DataFrame. The first cell takes care of scalars, and the second takes care of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pi0     events:     263891\t(23.3%)\n",
      "Number of piplus  events:     435967\t(38.4%)\n",
      "Number of piminus events:     434627\t(38.3%)\n",
      "Total: 1134485\n"
     ]
    }
   ],
   "source": [
    "# import pi+- vs. pi0 images\n",
    "source = 'pion' # also try 'jet'\n",
    "\n",
    "if(source == 'pion'):\n",
    "    inputpath = path_prefix+'data/pion/'\n",
    "    rootfiles = [\"pi0\", \"piplus\", \"piminus\"]\n",
    "    branches = ['runNumber', 'eventNumber', 'truthE', 'truthPt', 'truthEta', 'truthPhi', 'clusterIndex', 'nCluster', 'clusterE', 'clusterECalib', 'clusterPt', 'clusterEta', 'clusterPhi', 'cluster_nCells', 'cluster_sumCellE', 'cluster_ENG_CALIB_TOT', 'cluster_ENG_CALIB_OUT_T', 'cluster_ENG_CALIB_DEAD_TOT', 'cluster_EM_PROBABILITY', 'cluster_HAD_WEIGHT', 'cluster_OOC_WEIGHT', 'cluster_DM_WEIGHT', 'cluster_CENTER_MAG', 'cluster_FIRST_ENG_DENS', 'cluster_cell_dR_min', 'cluster_cell_dR_max', 'cluster_cell_dEta_min', 'cluster_cell_dEta_max', 'cluster_cell_dPhi_min', 'cluster_cell_dPhi_max', 'cluster_cell_centerCellEta', 'cluster_cell_centerCellPhi', 'cluster_cell_centerCellLayer', 'cluster_cellE_norm']\n",
    "elif(source == 'jet'):\n",
    "    inputpath = path_prefix+'jets/training/'\n",
    "    rootfiles = [\"pi0\", \"piplus\"]\n",
    "    branches = ['runNumber', 'eventNumber', 'truthE', 'truthPt', 'truthEta', 'truthPhi', 'clusterIndex', 'nCluster', 'clusterE', 'clusterECalib', 'clusterPt', 'clusterEta', 'clusterPhi', 'cluster_nCells', 'cluster_ENG_CALIB_TOT']\n",
    "else:\n",
    "    assert(False)\n",
    "\n",
    "trees = {\n",
    "    rfile : ur.open(inputpath+rfile+\".root\")['ClusterTree']\n",
    "    for rfile in rootfiles\n",
    "}\n",
    "pdata = {\n",
    "    ifile : itree.pandas.df(branches, flatten=False)\n",
    "    for ifile, itree in trees.items()\n",
    "}\n",
    "\n",
    "total = 0\n",
    "for key in rootfiles:\n",
    "    total += len(pdata[key])\n",
    "\n",
    "for key in rootfiles:\n",
    "    n = len(pdata[key])\n",
    "    print(\"Number of {a:<7} events: {b:>10}\\t({c:.1f}%)\".format(a=key, b = n, c = 100. * n / total))\n",
    "print(\"Total: {}\".format(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of events for each category may be quite different -- ultimately we want to train our classifier on a \"balanced\" dataset, where we have equal numbers of entries from each category.\n",
    "\n",
    "We're training our network to classify between $\\pi^\\pm$ and $\\pi^0$ events. Thus, we should ultimately merge our $\\pi^+$ and $\\pi^-$ data.\n",
    "\n",
    "Thus, we will first generate selected indices for all categories, such that the total number of events from each category is equal, and *then* we will merge things.\n",
    "\n",
    "Note that as we're dealing with DataFrames (`pdata`) and uproot trees (`trees`, whose contents get loaded into `pcells`), we have to be careful that when we merge data, we do it the same way for both sets of objects. Otherwise we might scramble our $\\pi^\\pm$ data -- which will matter *if* we ever want to use inputs beyond just the images (from `trees`) as network input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_indices = {}\n",
    "n_max = int(np.min(np.array([len(pdata[key]) for key in trees.keys()])))\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "# If we have a piminus key, assume the dataset are piplus, piminus, pi0\n",
    "if('piminus' in trees.keys()):\n",
    "    n_indices['piplus']  = int(np.ceil((n_max / 2)))\n",
    "    n_indices['piminus'] = int(np.floor((n_max / 2)))\n",
    "    n_indices['pi0']     = n_max\n",
    "    \n",
    "# Otherwise, assume we already have piplus (or piplus + piminus) and pi0, no merging needed\n",
    "else: n_indices = {key:n_max for key in trees.keys}\n",
    "indices = {key:rng.choice(len(pdata[key]), n_indices[key], replace=False) for key in trees.keys()}\n",
    "\n",
    "# Make a boolean array version of our indices, since pandas is weird and doesn't handle non-bool indices?\n",
    "bool_indices = {}\n",
    "for key in pdata.keys():\n",
    "    bool_indices[key] = np.full(len(pdata[key]), False)\n",
    "    bool_indices[key][indices[key]] = True\n",
    "\n",
    "# Apply the (bool) indices to pdata\n",
    "for key in trees.keys():\n",
    "    pdata[key] = pdata[key][bool_indices[key]]\n",
    "\n",
    "\n",
    "# prepare pcells -- immediately apply our selected indices\n",
    "pcells = {\n",
    "    ifile : {\n",
    "        layer : mu.setupCells(itree, layer, indices = indices[ifile])\n",
    "        for layer in layers\n",
    "    }\n",
    "    for ifile, itree in trees.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging piplus and piminus.\n"
     ]
    }
   ],
   "source": [
    "# Now with the data extracted from the trees into pcells, we merge pdata and pcells as needed.\n",
    "# Note the order in which we concatenate things: piplus -> piplus + piminus.\n",
    "if('piminus' in trees.keys()):\n",
    "    print('Merging piplus and piminus.')\n",
    "    \n",
    "    # merge pdata\n",
    "    pdata['piplus'] = pdata['piplus'].append(pdata['piminus'])\n",
    "    del pdata['piminus']\n",
    "    \n",
    "    # merge contents of pcells\n",
    "    for layer in layers:\n",
    "        pcells['piplus'][layer] = np.row_stack((pcells['piplus'][layer],pcells['piminus'][layer]))\n",
    "    del pcells['piminus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train the network on $\\pi^+$ and $\\pi^0$ events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "training_dataset = ['pi0','piplus']\n",
    "\n",
    "# create train/validation/test subsets containing 70%/10%/20%\n",
    "# of events from each type of pion event\n",
    "for p_index, plabel in enumerate(training_dataset):\n",
    "    mu.splitFrameTVT(pdata[plabel],trainfrac=0.7)\n",
    "    pdata[plabel]['label'] = p_index\n",
    "\n",
    "# merge pi0 and pi+ events\n",
    "pdata_merged = pd.concat([pdata[ptype] for ptype in training_dataset])\n",
    "pcells_merged = {\n",
    "    layer : np.concatenate([pcells[ptype][layer]\n",
    "                            for ptype in training_dataset])\n",
    "    for layer in layers\n",
    "}\n",
    "plabels = np_utils.to_categorical(pdata_merged['label'],len(training_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a few example images.\n",
    "\n",
    "These are the images that we will use to train our network (together with a few other variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots for E = 0.5-2000 GeV pi0/pi+/pi- samples\n",
    "\n",
    "# specify which cluster to plot\n",
    "cluster = 100\n",
    "\n",
    "# make the plot\n",
    "plt.cla(); plt.clf()\n",
    "fig = plt.figure(figsize=(60,20))\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "i = 0\n",
    "for ptype, pcell in pcells.items():\n",
    "    for layer in layers:\n",
    "        i = i+1\n",
    "        plt.subplot(3,6,i)\n",
    "        plt.imshow(pcell[layer][cluster].reshape(cell_shapes[layer]), extent=[-0.2, 0.2, -0.2, 0.2],\n",
    "            cmap=plt.get_cmap('Blues'), origin='lower', interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "        plt.title(ptype+ 'in '+str(layer))\n",
    "        ampl.set_xlabel(\"$\\Delta\\phi$\")\n",
    "        ampl.set_ylabel(\"$\\Delta\\eta$\")\n",
    "\n",
    "# show the plots\n",
    "plt.savefig(plotpath+'plots_pi0_plus_minus.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a few histograms.\n",
    "\n",
    "These are a bit uglier than the `matplotlib` ones Max made, but it's perhaps even easier to see any differences between $\\pi^\\pm$ and $\\pi^0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt.gStyle.SetOptStat(0)\n",
    "\n",
    "# For storing histograms and legends, to prevent overwriting. (TODO: Probably better ways to do this in PyROOT)\n",
    "histos = []\n",
    "legends = []\n",
    "\n",
    "qtys = ['cluster_nCells', 'clusterE', 'clusterEta', 'clusterPhi', 'cluster_EM_PROBABILITY', 'cluster_sumCellE']\n",
    "qty_labels = ['Cells/Cluster', 'Cluster Energy [GeV]', 'Cluster #eta', 'Cluster #phi', 'Cluster EMProb', 'Cluster SumCellE']\n",
    "qty_ranges = [(0,500), (50,200), (-0.8,0.8), (-4.,4.), (0.,1.), (0.,2500.)]\n",
    "\n",
    "if(source == 'jet'):\n",
    "    qtys = ['cluster_nCells', 'clusterE', 'clusterEta', 'clusterPhi']\n",
    "    qty_labels = ['Cells/Cluster', 'Cluster Energy [GeV]', 'Cluster #eta', 'Cluster #phi']\n",
    "    qty_ranges = [(0,300), (0,100), (-0.8,0.8), (-4.,4.)]\n",
    "\n",
    "# Set up a canvas.\n",
    "plot_size = 500\n",
    "nx = int(np.ceil(len(qtys) / 2))\n",
    "ny = 2\n",
    "n_pad = nx * ny\n",
    "canvas = rt.TCanvas('cluster_hists','c1',plot_size * nx,plot_size * ny)\n",
    "canvas.Divide(nx,ny)\n",
    "\n",
    "colors = {'piplus':rt.kRed,'piminus':rt.kBlue,'pi0':rt.kOrange}\n",
    "styles = {'piplus':3440, 'piminus':3404, 'pi0':1001}\n",
    "\n",
    "n_bins=20\n",
    "for i, (qty, label, rng) in enumerate(zip(qtys, qty_labels, qty_ranges)):\n",
    "    canvas.cd(i+1)\n",
    "    leg = rt.TLegend(0.7,0.8,0.9,0.9)\n",
    "    for ptype, p in pdata.items():\n",
    "        hist = rt.TH1F('h_'+str(ptype)+'_'+str(qty),'',n_bins,rng[0],rng[1])\n",
    "        for entry in p[qty]: hist.Fill(entry)\n",
    "        integral = hist.Integral()\n",
    "        if(integral != 0): hist.Scale(1./hist.Integral())\n",
    "        hist.SetLineColor(colors[ptype])\n",
    "        hist.SetLineWidth(2)\n",
    "        hist.SetFillColorAlpha(colors[ptype],0.5)\n",
    "        hist.SetFillStyle(styles[ptype])\n",
    "        hist.Draw('HIST SAME')\n",
    "        hist.GetXaxis().SetTitle(label)\n",
    "        hist.GetYaxis().SetTitle('Normalised events')\n",
    "        hist.SetMaximum(1.5 * hist.GetMaximum())\n",
    "        leg.AddEntry(hist,pi_latex[ptype],'f')\n",
    "        leg.Draw()\n",
    "        histos.append(hist)\n",
    "        legends.append(leg)\n",
    "    if(qty in ['cluster_nCells','clusterE', 'cluster_EM_PROBABILITY', 'cluster_sumCellE']): rt.gPad.SetLogy()\n",
    "canvas.Draw()\n",
    "canvas.SaveAs(plotpath+'hist_pi0_plus_minus.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple feed-forward Neural Network.\n",
    "<div style=\"text-align: right\"> <a href=\"#Image-Classification-of-ATLAS-Calorimeter-Topo-Clusters-Rewrite\">Top</a> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we're going to train a simple, feed-foward neural network. This will be our \"baseline network\".\n",
    "\n",
    "Let's import `TensorFlow`, and get our GPU ready. We assume that there's only one available, otherwise you can modify the list below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "ngpu = 1\n",
    "gpu_list = [\"/gpu:\"+str(i) for i in range(ngpu)]\n",
    "#gpu_list = [\"/gpu:0\"] #[\"/gpu:0\",\"/gpu:1\",\"/gpu:2\",\"/gpu:3\"]\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # disable some of the tensorflow info printouts, only display errors\n",
    "import tensorflow as tf\n",
    "strategy = tf.distribute.MirroredStrategy(devices=gpu_list)\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "ngpu = strategy.num_replicas_in_sync\n",
    "print ('Number of devices: {}'.format(ngpu))\n",
    "#sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import baseline_nn_model\n",
    "\n",
    "lr = 5e-5\n",
    "dropout = 0.2 # < 0 -> no dropout\n",
    "model = baseline_nn_model(strategy, lr=lr, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "models = {}\n",
    "for layer in layers:\n",
    "    npix = cell_shapes[layer][0]*cell_shapes[layer][1]\n",
    "    models[layer] = model(npix)\n",
    "    models[layer].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 100\n",
    "batch_size = 200 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "\n",
    "model_history = {}\n",
    "model_performance = {}\n",
    "model_scores = {}\n",
    "for layer in layers:\n",
    "    print('On layer ' + layer + '.')\n",
    "    \n",
    "    # train+validate model\n",
    "    model_history[layer] = models[layer].fit(\n",
    "        pcells_merged[layer][pdata_merged.train], plabels[pdata_merged.train],\n",
    "        validation_data = (\n",
    "            pcells_merged[layer][pdata_merged.val], plabels[pdata_merged.val]\n",
    "        ),\n",
    "        epochs = nepochs, batch_size = batch_size, verbose = verbose,\n",
    "    )\n",
    "    \n",
    "    model_history[layer] = model_history[layer].history\n",
    "    \n",
    "    # get overall performance metric\n",
    "    model_performance[layer] = models[layer].evaluate(\n",
    "        pcells_merged[layer][pdata_merged.test], plabels[pdata_merged.test],\n",
    "        verbose = 0,\n",
    "    )\n",
    "    \n",
    "    # get network scores for the dataset\n",
    "    model_scores[layer] = models[layer].predict(\n",
    "        pcells_merged[layer]\n",
    "    )\n",
    "    print('Finished layer ' + layer + '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save this model to a set of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "flat_dir = modelpath + 'flat' # directory for our \"flat\", single calo-layer networks\n",
    "try: os.makedirs(flat_dir)\n",
    "except: pass\n",
    "\n",
    "for layer in layers:\n",
    "    print('Saving ' + layer)\n",
    "    models[layer].save(flat_dir + '/' +'model_' + layer + '_flat_do20.h5')\n",
    "    \n",
    "    with open(flat_dir + '/' + 'model_' + layer + '_flat_do20.history','wb') as model_history_file:\n",
    "        pickle.dump(model_history[layer], model_history_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively we can load a saved model from a set of files (use this to skip training above, if it's been done before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "flat_dir = modelpath + 'flat' # directory for our \"flat\", single calo-layer networks\n",
    "models = {}\n",
    "model_history = {}\n",
    "model_scores = {}\n",
    "for layer in layers:\n",
    "    print('Loading ' + layer)\n",
    "    models[layer] = tf.keras.models.load_model(flat_dir + '/'+'model_' + layer + '_flat_do20.h5')\n",
    "    \n",
    "    # load history object\n",
    "    with open(flat_dir + '/' + 'model_' + layer + '_flat_do20.history','rb') as model_history_file:\n",
    "        model_history[layer] = pickle.load(model_history_file)\n",
    "    \n",
    "    # recalculate network scores for the dataset\n",
    "    model_scores[layer] = models[layer].predict(\n",
    "        pcells_merged[layer]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot model accuracy and loss as a function of training epoch, for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Log scale doesn't seem to actually affect the curves. Why? (weird mpl behaviour)\n",
    "use_log = False\n",
    "x_lim = [0.,100.]\n",
    "y_lim = {'acc':[0.5,1.],'loss':[0.2,0.7]}\n",
    "for layer in layers:\n",
    "#     print(history_flat[layer_i].history.keys())\n",
    "    plt.cla(); plt.clf()\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,6))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    if(use_log): \n",
    "        ax1.set_yscale('log')\n",
    "        ax2.set_yscale('log')\n",
    "        \n",
    "    ax1.plot(model_history[layer]['acc'])\n",
    "    ax1.plot(model_history[layer]['val_acc'])\n",
    "    ax1.set_title('model accuracy for ' + layer)\n",
    "    ax1.set(xlabel = 'epoch',ylabel='accuracy')\n",
    "    ax1.set_xlim(x_lim)\n",
    "    ax1.set_ylim(y_lim['acc'])\n",
    "    ax1.legend(['train', 'test'], loc='upper left')\n",
    "    ax1.grid(True)\n",
    "    extent = ax1.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "    plt.savefig('Plots/accuracy_' + layer + '.pdf',bbox_inches=extent)\n",
    "\n",
    "    # summarize history for loss\n",
    "    ax2.plot(model_history[layer]['loss'])\n",
    "    ax2.plot(model_history[layer]['val_loss'])\n",
    "    ax2.set_title('model loss for ' + layer)\n",
    "    ax2.set(xlabel = 'epoch',ylabel='loss')\n",
    "    ax2.set_xlim(x_lim)\n",
    "    ax2.set_ylim(y_lim['loss'])\n",
    "    ax2.legend(['train', 'test'], loc='upper right')\n",
    "    ax2.grid(True)\n",
    "    extent = ax2.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "    plt.savefig(plotpath + 'loss_' + layer + '.pdf',bbox_inches=extent)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at ROC curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "roc_fpr = {}\n",
    "roc_tpr = {}\n",
    "roc_thresh = {}\n",
    "roc_auc = {}\n",
    "\n",
    "for layer in layers:\n",
    "    roc_fpr[layer], roc_tpr[layer], roc_thresh[layer] = roc_curve(\n",
    "        plabels[pdata_merged.test][:,1],\n",
    "        model_scores[layer][pdata_merged.test,1],\n",
    "        drop_intermediate=False,\n",
    "    )\n",
    "    roc_auc[layer] = auc(roc_fpr[layer], roc_tpr[layer])\n",
    "    print('Area under curve for ' + layer + ': ' + str(roc_auc[layer]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the area under the curve for the old method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_fpr, lc_tpr, lc_thresh = roc_curve(\n",
    "    plabels[pdata_merged.test][:,1],\n",
    "    1-pdata_merged[\"cluster_EM_PROBABILITY\"][pdata_merged.test],\n",
    ")\n",
    "lc_auc = auc(lc_fpr, lc_tpr)\n",
    "print(\"Area under curve for cluster_EM_PROB: \" + str(lc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare one of our new, simple networks, against the old method `EMProb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_method = 'EMB1'\n",
    "pu.roc_plot([lc_fpr,roc_fpr[comp_method]],[lc_tpr,roc_tpr[comp_method]],\n",
    "            figfile=plotpath + 'roc_lc_only.pdf',\n",
    "            labels=['LC EMProb (area = {:.3f})'.format(lc_auc),\n",
    "                    comp_method +' (area = {:.3f})'.format(roc_auc[comp_method])],\n",
    "            extra_lines=[[[0, 1], [0, 1]]],\n",
    "            title='Simple NN ROC curve: classification of $\\pi^+$ vs. $\\pi^0$')\n",
    "\n",
    "# plt.cla(); plt.clf()\n",
    "# fig = plt.figure()\n",
    "# fig.patch.set_facecolor('white')\n",
    "# plt.plot(lc_fpr, lc_tpr, label='LC EMProb (area = {:.3f})'.format(lc_auc))\n",
    "# plt.plot(roc_fpr['EMB1'], roc_tpr['EMB1'], label='EMB1 (area = {:.3f})'.format(roc_auc['EMB1']))\n",
    "# plt.plot([0, 1], [0, 1], 'k--')\n",
    "# plt.xlim(0,1.1)\n",
    "# plt.ylim(0,1.1)\n",
    "# plt.title('Simple NN ROC curve: classification of $\\pi^+$ vs. $\\pi^0$')\n",
    "# ampl.set_xlabel('False positive rate')\n",
    "# ampl.set_ylabel('True positive rate')\n",
    "# plt.legend(loc='best')\n",
    "# plt.savefig(plotpath + 'roc_lc_only.pdf')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.cla(); plt.clf()\n",
    "# fig, ax = plt.subplots(1, 2, tight_layout=True, figsize=(10,4))\n",
    "# fig.patch.set_facecolor('white')\n",
    "\n",
    "# colors for our simple NN's\n",
    "colors = ['xkcd:red','xkcd:light orange','xkcd:gold','xkcd:green','xkcd:blue','xkcd:violet']\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(lc_fpr, lc_tpr, label='LC EMProb (area = {:.3f})'.format(lc_auc),linestyle='-.')\n",
    "for layer_i, layer_name in enumerate(layers):\n",
    "    plt.plot(roc_fpr[layer_name], roc_tpr[layer_name], label='{} (area = {:.3f})'.format(layer_name, roc_auc[layer_name]),color=colors[layer_i])\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('Simple NN ROC curve: classification of $\\pi^+$ vs. $\\pi^0$')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('Plots/roc_layers.pdf')\n",
    "plt.show()\n",
    "\n",
    "# Zoom in view of the upper left corner.\n",
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.xlim(0, 0.25)\n",
    "plt.ylim(0.6, 1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(lc_fpr, lc_tpr, label='LC EMProb (area = {:.3f})'.format(lc_auc),linestyle='-.')\n",
    "for layer_i, layer_name in enumerate(layers):\n",
    "    plt.plot(roc_fpr[layer_name], roc_tpr[layer_name], label='{} (area = {:.3f})'.format(layer_name, roc_auc[layer_name]),color=colors[layer_i])\n",
    "# ax[1].plot(fpr_nn, tpr_nn, label='Simple NN (area = {:.3f})'.format(auc_nn))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve (zoomed in at top left)')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('Plots/roc_zoom_layers.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve Scans\n",
    "<div style=\"text-align: right\"> <a href=\"#Image-Classification-of-ATLAS-Calorimeter-Topo-Clusters-Rewrite\">Top</a> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a convenience class for creating ROC curve scans\n",
    "display_digits=2\n",
    "class roc_var:\n",
    "    def __init__(self,\n",
    "                 name, # name of variable as it appears in the root file\n",
    "                 bins, # endpoints of bins as a list\n",
    "                 df,   # dataframe to construct subsets from\n",
    "                 latex='', # optional latex to display variable name with\n",
    "                 vlist=None, # optional list to append class instance to\n",
    "                ):\n",
    "        self.name = name\n",
    "        self.bins = bins\n",
    "        \n",
    "        if(latex == ''): self.latex = name\n",
    "        else:            self.latex = latex\n",
    "        \n",
    "        self.selections = []\n",
    "        self.labels = []\n",
    "        for i, point in enumerate(self.bins):\n",
    "            if(i == 0):\n",
    "                self.selections.append( df[name]<point )\n",
    "                self.labels.append(self.latex+'<'+str(round(point,display_digits)))\n",
    "            else:\n",
    "                self.selections.append( (df[name]>self.bins[i-1]) & (df[name]<self.bins[i]) )\n",
    "                self.labels.append(str(round(self.bins[i-1],display_digits))+'<'+self.latex+'<'+str(round(point,display_digits)))\n",
    "                if(i == len(bins)-1):\n",
    "                    self.selections.append( df[name]>point )\n",
    "                    self.labels.append(self.latex+'>'+str(round(point,display_digits)))\n",
    "        \n",
    "        if(vlist != None):\n",
    "            vlist.append(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_scan(varlist,scan_targets,labels):\n",
    "    '''\n",
    "    Creates a set of ROC curve plots by scanning over the specified variables.\n",
    "    One set is created for each target (neural net score dataset).\n",
    "    \n",
    "    varlist: a list of roc_var instances to scan over\n",
    "    scan_targets: a list of neural net score datasets to use\n",
    "    labels: a list of target names (strings); must be the same length as scan_targets\n",
    "    '''\n",
    "    for target, target_label in zip(scan_targets,labels):\n",
    "        for v in varlist:\n",
    "            # prepare matplotlib figure\n",
    "            plt.cla()\n",
    "            plt.clf()\n",
    "            fig = plt.figure()\n",
    "            fig.patch.set_facecolor('white')\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "            for binning, label in zip(v.selections,v.labels):\n",
    "                # first generate ROC curve\n",
    "                x, y, t = roc_curve(\n",
    "                    plabels[pdata_merged.test & binning][:,1],\n",
    "                    target[pdata_merged.test & binning],\n",
    "                    drop_intermediate=False,\n",
    "                )\n",
    "                var_auc = auc(x,y)\n",
    "                plt.plot(x, y, label=label+' (area = {:.3f})'.format(var_auc))\n",
    "\n",
    "            plt.title('ROC Scan of '+target_label+' over '+v.latex)\n",
    "            plt.xlim(0,1.1)\n",
    "            plt.ylim(0,1.1)\n",
    "            ampl.set_xlabel('False positive rate')\n",
    "            ampl.set_ylabel('True positive rate')\n",
    "            plt.legend()\n",
    "            plt.savefig(plotpath+'roc_scan_'+target_label+'_'+v.name+'.pdf')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify variables we are interested in scanning over\n",
    "varlist = []\n",
    "cluster_e = roc_var(\n",
    "    name='clusterE',\n",
    "    bins=[1,10,50,500],\n",
    "    df=pdata_merged,\n",
    "    vlist=varlist,\n",
    ")\n",
    "\n",
    "pdata_merged['abs_clusterEta'] = np.abs(pdata_merged.clusterEta)\n",
    "cluster_eta = roc_var(\n",
    "    name='abs_clusterEta',\n",
    "    bins=[0.2,0.4,0.6],\n",
    "    df=pdata_merged,\n",
    "    vlist=varlist,\n",
    "    latex='abs(clusterEta)'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin the scan\n",
    "targets = [model_scores[layer][:,1] for layer in layers]+[1-pdata_merged[\"cluster_EM_PROBABILITY\"]]\n",
    "labels = layers+['LC']\n",
    "roc_scan(varlist,targets,labels)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also train an instance of ResNet50.\n",
    "\n",
    "To do this, we will want to appropriately up/downscale all of our calorimeter images, so they are all of the same dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMB1 (128, 4)\n",
      "EMB2 (16, 16)\n",
      "EMB3 (8, 16)\n",
      "TileBar0 (4, 4)\n",
      "TileBar1 (4, 4)\n",
      "TileBar2 (2, 4)\n"
     ]
    }
   ],
   "source": [
    "for key, val in cell_shapes.items():\n",
    "    print(key,val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above, it seems like a reasonable choice would be rescaling all images to be of shape `(128,16)`. That corresponds with the maximum dimensions along each axis, so we'll just need to do some upscaling. The nice thing of avoiding downscaling is that we are not losing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import resnet\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "lr = 5e-5\n",
    "input_shape = (128,16)\n",
    "model_resnet = resnet(strategy, lr=lr)(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minor data prep -- key names match those defined within resnet model in models.py!\n",
    "pcells_merged_unflattened = {'input' + str(i):pcells_merged[key].reshape(tuple([-1] + list(cell_shapes[key]))) for i,key in enumerate(pcells_merged.keys())}\n",
    "\n",
    "rn_train = {key:val[pdata_merged.train] for key,val in pcells_merged_unflattened.items()}\n",
    "rn_valid = {key:val[pdata_merged.val] for key,val in pcells_merged_unflattened.items()}\n",
    "rn_test = {key:val[pdata_merged.test] for key,val in pcells_merged_unflattened.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 2956/14840 [====>.........................] - ETA: 10:14 - loss: 0.4077 - acc: 0.8615"
     ]
    }
   ],
   "source": [
    "nepochs = 10\n",
    "batch_size = 20 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "\n",
    "model_key = 'resnet'\n",
    "\n",
    "# train+validate model\n",
    "model_history[model_key] = model_resnet.fit(\n",
    "    x=rn_train,\n",
    "    y=plabels[pdata_merged.train],\n",
    "    validation_data=(\n",
    "        rn_valid,\n",
    "        plabels[pdata_merged.val]\n",
    "    ),\n",
    "    epochs=nepochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=verbose\n",
    ")\n",
    "    \n",
    "model_history[model_key] = model_history[model_key].history\n",
    "    \n",
    "# get overall performance metric\n",
    "model_performance[model_key] = model_resnet.evaluate(\n",
    "    x=rn_test,\n",
    "    y=plabels[pdata_merged.test],\n",
    "    verbose=0\n",
    ")\n",
    "    \n",
    "# get network scores for the dataset\n",
    "model_scores[model_key] = models[layer].predict(\n",
    "    pcells_merged_unflattened\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination Network\n",
    "<div style=\"text-align: right\"> <a href=\"#Image-Classification-of-ATLAS-Calorimeter-Topo-Clusters-Rewrite\">Top</a> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train a simple combination network... its inputs will be the *outputs* of our simple, feed-forward neural networks from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import simple_combine_model\n",
    "\n",
    "model_scores_stack = np.column_stack( [model_scores[layer][:,1] for layer in model_scores] )\n",
    "lr = 1e-3\n",
    "n_input = model_scores_stack.shape[1]\n",
    "model_simpleCombine = simple_combine_model(strategy, lr=lr, n_input = n_input)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 200*ngpu\n",
    "verbose = 2\n",
    "\n",
    "simpleCombine_history = model_simpleCombine.fit(model_scores_stack[pdata_merged.train], plabels[pdata_merged.train],\n",
    "                                                validation_data=(model_scores_stack[pdata_merged.val],\n",
    "                                                                 plabels[pdata_merged.val]),\n",
    "                                                epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "simpleCombine_history = simpleCombine_history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and save the results to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "simple_dir = modelpath + 'simple' # directory for our \"simple\" network\n",
    "try: os.makedirs(simple_dir)\n",
    "except: pass\n",
    "\n",
    "model_simpleCombine.save(simple_dir + '/' +'model_simple_do20.h5')\n",
    "with open(simple_dir + '/' + 'model_simple_do20.history','wb') as model_history_file:\n",
    "    pickle.dump(simpleCombine_history, model_history_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also load the model from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "simple_dir = modelpath + 'simple' # directory for our \"simple\" network\n",
    "model_simpleCombine = tf.keras.models.load_model(simple_dir + '/' + 'model_simple_do20.h5')\n",
    "with open(simple_dir + '/' + 'model_simple_do20.history','rb') as model_history_file:\n",
    "    simpleCombine_history = pickle.load(model_history_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot some results from this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pu.make_plot(\n",
    "    [simpleCombine_history['acc'],simpleCombine_history['val_acc']],\n",
    "    figfile = plotpath+'accuracy_simpleCombine.pdf',\n",
    "    xlabel = 'epoch', ylabel = 'accuracy',\n",
    "    x_log = False, y_log = False,\n",
    "    labels = ['train','test'],\n",
    "    title = 'Model accuracy for simple combination',\n",
    ")\n",
    "\n",
    "# summarize history for loss\n",
    "pu.make_plot(\n",
    "    [simpleCombine_history['loss'],simpleCombine_history['val_loss']],\n",
    "    figfile = plotpath+'loss_simpleCombine.pdf',\n",
    "    xlabel = 'epoch', ylabel = 'loss',\n",
    "    x_log = False, y_log = False,\n",
    "    labels = ['train','test'],\n",
    "    title = 'Model loss for simple combination',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "simpleCombine_score = model_simpleCombine.predict(model_scores_stack)\n",
    "simpleCombine_fpr, simpleCombine_tpr, simpleCombine_thresh = roc_curve(\n",
    "    plabels[pdata_merged.test,1], simpleCombine_score[pdata_merged.test,1]\n",
    ")\n",
    "simpleCombine_auc = auc(simpleCombine_fpr, simpleCombine_tpr)\n",
    "print(\"Area under curve for simpleCombine: \" + str(simpleCombine_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pu.roc_plot(\n",
    "    [simpleCombine_fpr,lc_fpr]+[roc_fpr[layer] for layer in layers],\n",
    "    [simpleCombine_tpr,lc_tpr]+[roc_tpr[layer] for layer in layers],\n",
    "    figfile = plotpath+'roc_simpleCombine.pdf',\n",
    "    extra_lines=[[[0, 1], [0, 1]]], labels=[\n",
    "        'simpleCombine (area = {:.3f})'.format(simpleCombine_auc),\n",
    "        'LC EMProb (area = {:.3f})'.format(lc_auc),\n",
    "    ]+[layer+' (area = {:.3f})'.format(roc_auc[layer]) for layer in layers],\n",
    "    title='Simple NN ROC curve: classification of $\\pi^+$ vs. $\\pi^0$')\n",
    "\n",
    "pu.roc_plot(\n",
    "    [simpleCombine_fpr,lc_fpr]+[roc_fpr[layer] for layer in layers],\n",
    "    [simpleCombine_tpr,lc_tpr]+[roc_tpr[layer] for layer in layers],\n",
    "    figfile = plotpath+'roc_simpleCombine.pdf',\n",
    "    x_max=0.25, y_min=0.6,\n",
    "    extra_lines=[[[0, 1], [0, 1]]], labels=[\n",
    "        'simpleCombine (area = {:.3f})'.format(simpleCombine_auc),\n",
    "        'LC EMProb (area = {:.3f})'.format(lc_auc),\n",
    "    ]+[layer+' (area = {:.3f})'.format(roc_auc[layer]) for layer in layers],\n",
    "    title='Simple NN ROC curve: zoomed in at top left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_scan(varlist,[simpleCombine_score[:,1]],['simpleCombine'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
