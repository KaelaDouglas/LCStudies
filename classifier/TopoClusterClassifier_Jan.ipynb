{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification of ATLAS Calorimeter Topo-Clusters (Jan)\n",
    "\n",
    "## This is a stripped-down version of Max's re-write, so I have removed *some* functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick Navigation:\n",
    "- [Simple feed-forward Neural Network](#Simple-feed-forward-Neural-Network.)\n",
    "- [ROC Curve Scans](#ROC-Curve-Scans)\n",
    "- [Combination Network](#Combination-Network)\n",
    "- [Convolutional Neural Network](#Convolutional-Neural-Network)\n",
    "- [Correlation Plots](#Correlation-Plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make sure that we have `latex` set up correctly. \n",
    "\n",
    "We will need this for the `atlas_mpl_style` package, which is used throughout some of ML4Pion's pre-existing utilities. As of October 22, 2020, the [UChicago ML platform](ml.maniac.uchicago.edu) does *not* have `latex` pre-installed. We can take care of this with our own installation script -- note that installed `latex` with `conda` [does not work well at the moment](https://github.com/conda-forge/texlive-core-feedstock/issues/19) so we fall back on the slower, regular method for installing `texlive` -- but we haven't made the necessary addition to our `$PATH` for `latex` so we must set it now locally for the notebook. (We avoid touching the `.bash_profile` on the ML platform or [making a custom Jupyter kernel](https://stackoverflow.com/a/53595397) for now.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/opt/conda/envs/ml4p/bin:/opt/conda/condabin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/texlive/2020/bin/x86_64-linux\n"
     ]
    }
   ],
   "source": [
    "# Check if latex is set up already.\n",
    "# We use some Jupyter magic -- alternatively one could use python's subprocess here.\n",
    "has_latex = !command -v latex\n",
    "has_latex = (not has_latex == [])\n",
    "\n",
    "# If latex was not a recognized command, our setup script should have installed\n",
    "# at a fixed location, but it is not on the $PATH. Now let's use some Jupyter magic.\n",
    "# See https://ipython.readthedocs.io/en/stable/interactive/shell.html for info.\n",
    "if(not has_latex):\n",
    "    latex_prefix = '/usr/local/texlive/2020/bin/x86_64-linux'\n",
    "    jupyter_env = %env\n",
    "    path = jupyter_env['PATH']\n",
    "    path = path + ':' + latex_prefix\n",
    "    %env PATH = $path\n",
    "    jupyter_env = %env\n",
    "    path = jupyter_env['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.22/02\n"
     ]
    }
   ],
   "source": [
    "#import libraries and some constants\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "#import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize, LogNorm\n",
    "import pandas as pd\n",
    "import ROOT as rt # I will use this for some plotting\n",
    "import uproot as ur\n",
    "import atlas_mpl_style as ampl\n",
    "ampl.use_atlas_style()\n",
    "\n",
    "params = {'legend.fontsize': 13,\n",
    "          'axes.labelsize': 18}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "path_prefix = '/workspace/LCStudies/'\n",
    "plotpath = path_prefix+'classifier/Plots/'\n",
    "modelpath = path_prefix+'classifier/Models/'\n",
    "# %config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# metadata\n",
    "layers = [\"EMB1\", \"EMB2\", \"EMB3\", \"TileBar0\", \"TileBar1\", \"TileBar2\"]\n",
    "cell_size_phi = [0.098, 0.0245, 0.0245, 0.1, 0.1, 0.1]\n",
    "cell_size_eta = [0.0031, 0.025, 0.05, 0.1, 0.1, 0.2]\n",
    "len_phi = [4, 16, 16, 4, 4, 4]\n",
    "len_eta = [128, 16, 8, 4, 4, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fancy display names for each pion type\n",
    "pi_latex = {\n",
    "    'pi0': '\\(\\pi^{0}\\)',\n",
    "    'piplus': '\\(\\pi^{+}\\)',\n",
    "    'piminus': '\\(\\pi^{-}\\)',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import our resolution utilities. These take care of some plotting, using `matplotlib` and the `atlas_mpl_style` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(path_prefix)\n",
    "sys.path\n",
    "from  util import resolution_util as ru\n",
    "from  util import plot_util as pu\n",
    "from  util import ml_util as mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will import our data from the `ROOT` files into a `pandas` DataFrame. The first cell takes care of scalars, and the second takes care of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pi0 events: 263891\n",
      "Number of pi+ events: 435967\n",
      "Number of pi- events: 434627\n",
      "Total: 1134485\n"
     ]
    }
   ],
   "source": [
    "# import pi+- vs. pi0 images\n",
    "\n",
    "inputpath = path_prefix+'data/pion/'\n",
    "#path = '/eos/user/m/mswiatlo/images/'\n",
    "branches = ['runNumber', 'eventNumber', 'truthE', 'truthPt', 'truthEta', 'truthPhi', 'clusterIndex', 'nCluster', 'clusterE', 'clusterECalib', 'clusterPt', 'clusterEta', 'clusterPhi', 'cluster_nCells', 'cluster_sumCellE', 'cluster_ENG_CALIB_TOT', 'cluster_ENG_CALIB_OUT_T', 'cluster_ENG_CALIB_DEAD_TOT', 'cluster_EM_PROBABILITY', 'cluster_HAD_WEIGHT', 'cluster_OOC_WEIGHT', 'cluster_DM_WEIGHT', 'cluster_CENTER_MAG', 'cluster_FIRST_ENG_DENS', 'cluster_cell_dR_min', 'cluster_cell_dR_max', 'cluster_cell_dEta_min', 'cluster_cell_dEta_max', 'cluster_cell_dPhi_min', 'cluster_cell_dPhi_max', 'cluster_cell_centerCellEta', 'cluster_cell_centerCellPhi', 'cluster_cell_centerCellLayer', 'cluster_cellE_norm']\n",
    "rootfiles = [\"pi0\", \"piplus\", \"piminus\"]\n",
    "trees = {\n",
    "    rfile : ur.open(inputpath+rfile+\".root\")['ClusterTree']\n",
    "    for rfile in rootfiles\n",
    "}\n",
    "pdata = {\n",
    "    ifile : itree.pandas.df(branches, flatten=False)\n",
    "    for ifile, itree in trees.items()\n",
    "}\n",
    "\n",
    "np0 = len(pdata['pi0'])\n",
    "npp = len(pdata['piplus'])\n",
    "npm = len(pdata['piminus'])\n",
    "\n",
    "print(\"Number of pi0 events: {}\".format(np0))\n",
    "print(\"Number of pi+ events: {}\".format(npp))\n",
    "print(\"Number of pi- events: {}\".format(npm))\n",
    "print(\"Total: {}\".format(np0+npp+npm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_shapes = {\n",
    "    'EMB1': (128,4),\n",
    "    'EMB2': (16,16),\n",
    "    'EMB3': (8,16),\n",
    "    'TileBar0': (4,4),\n",
    "    'TileBar1': (4,4),\n",
    "    'TileBar2': (2,4),\n",
    "}\n",
    "\n",
    "pcells = {\n",
    "    ifile : {\n",
    "        layer : mu.setupCells(itree, layer)\n",
    "        for layer in layers\n",
    "    }\n",
    "    for ifile, itree in trees.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a few example images.\n",
    "\n",
    "These are the images that we will use to train our network (together with a few other variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots for E = 0.5-2000 GeV pi0/pi+/pi- samples\n",
    "\n",
    "# specify which cluster to plot\n",
    "cluster = 100\n",
    "\n",
    "# make the plot\n",
    "plt.cla(); plt.clf()\n",
    "fig = plt.figure(figsize=(60,20))\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "i = 0\n",
    "for ptype, pcell in pcells.items():\n",
    "    for layer in layers:\n",
    "        i = i+1\n",
    "        plt.subplot(3,6,i)\n",
    "        plt.imshow(pcell[layer][cluster].reshape(cell_shapes[layer]), extent=[-0.2, 0.2, -0.2, 0.2],\n",
    "            cmap=plt.get_cmap('Blues'), origin='lower', interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "        plt.title(ptype+ 'in '+str(layer))\n",
    "        ampl.set_xlabel(\"$\\Delta\\phi$\")\n",
    "        ampl.set_ylabel(\"$\\Delta\\eta$\")\n",
    "\n",
    "# show the plots\n",
    "plt.savefig(plotpath+'plots_pi0_plus_minus.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a few histograms.\n",
    "\n",
    "These are a bit uglier than the `matplotlib` ones Max made, but it's perhaps even easier to see any differences between $\\pi^\\pm$ and $\\pi^0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt.gStyle.SetOptStat(0)\n",
    "\n",
    "# Set up a canvas.\n",
    "canvas = rt.TCanvas('cluster_hists','c1',1000,800)\n",
    "nx = 3\n",
    "ny = 2\n",
    "n_pad = nx * ny\n",
    "canvas.Divide(nx,ny)\n",
    "\n",
    "# For storing histograms and legends, to prevent overwriting. (TODO: Probably better ways to do this in PyROOT)\n",
    "histos = []\n",
    "legends = []\n",
    "\n",
    "qtys = ['cluster_nCells', 'clusterE', 'clusterEta', 'clusterPhi', 'cluster_EM_PROBABILITY', 'cluster_sumCellE']\n",
    "qty_labels = ['Cells/Cluster', 'Cluster Energy [GeV]', 'Cluster #eta', 'Cluster #phi', 'Cluster EMProb', 'Cluster SumCellE']\n",
    "qty_ranges = [(0,500), (50,200), (-0.8,0.8), (-4.,4.), (0.,1.), (0.,2500.)]\n",
    "colors = {'piplus':rt.kRed,'piminus':rt.kBlue,'pi0':rt.kOrange}\n",
    "styles = {'piplus':3440, 'piminus':3404, 'pi0':1001}\n",
    "\n",
    "n_bins=20\n",
    "for i, (qty, label, rng) in enumerate(zip(qtys, qty_labels, qty_ranges)):\n",
    "    canvas.cd(i+1)\n",
    "    leg = rt.TLegend(0.7,0.8,0.9,0.9)\n",
    "    for ptype, p in pdata.items():\n",
    "        hist = rt.TH1F('h_'+str(ptype)+'_'+str(qtys[i]),'',n_bins,qty_ranges[i][0],qty_ranges[i][1])\n",
    "        for entry in p[qty]: hist.Fill(entry)\n",
    "        hist.Scale(1./hist.Integral())\n",
    "        hist.SetLineColor(colors[ptype])\n",
    "        hist.SetLineWidth(2)\n",
    "        hist.SetFillColorAlpha(colors[ptype],0.5)\n",
    "        hist.SetFillStyle(styles[ptype])\n",
    "        hist.Draw('HIST SAME')\n",
    "        hist.GetXaxis().SetTitle(label)\n",
    "        hist.GetYaxis().SetTitle('Normalised events')\n",
    "        hist.SetMaximum(1.5 * hist.GetMaximum())\n",
    "        leg.AddEntry(hist,pi_latex[ptype],'f')\n",
    "        leg.Draw()\n",
    "        histos.append(hist)\n",
    "        legends.append(leg)\n",
    "canvas.Draw()\n",
    "canvas.SaveAs(plotpath+'hist_pi0_plus_minus.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple feed-forward Neural Network.\n",
    "<div style=\"text-align: right\"> <a href=\"#Image-Classification-of-ATLAS-Calorimeter-Topo-Clusters-Rewrite\">Top</a> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we're going to train a simple, feed-foward neural network. This will be our \"baseline network\".\n",
    "\n",
    "Let's import `TensorFlow`, and get our GPU ready. We assume that there's only one available, otherwise you can modify the list below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngpu = 1\n",
    "gpu_list = [\"/gpu:\"+str(i) for i in range(ngpu)]\n",
    "#gpu_list = [\"/gpu:0\"] #[\"/gpu:0\",\"/gpu:1\",\"/gpu:2\",\"/gpu:3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of devices: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-25 21:12:36.759960: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2020-11-25 21:12:36.777996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:3e:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2020-11-25 21:12:36.778364: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-11-25 21:12:36.781812: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-11-25 21:12:36.784875: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-11-25 21:12:36.785379: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-11-25 21:12:36.788773: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-11-25 21:12:36.790401: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-11-25 21:12:36.797062: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-11-25 21:12:36.798740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2020-11-25 21:12:36.799675: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2020-11-25 21:12:36.815077: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2100000000 Hz\n",
      "2020-11-25 21:12:36.818548: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x565484571bd0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-11-25 21:12:36.818573: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-11-25 21:12:36.819625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:3e:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2020-11-25 21:12:36.819674: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-11-25 21:12:36.819694: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-11-25 21:12:36.819712: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-11-25 21:12:36.819730: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-11-25 21:12:36.819748: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-11-25 21:12:36.819765: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-11-25 21:12:36.819783: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-11-25 21:12:36.821249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2020-11-25 21:12:36.821307: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-11-25 21:12:36.983308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-11-25 21:12:36.983334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
      "2020-11-25 21:12:36.983346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
      "2020-11-25 21:12:36.985553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 283 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:3e:00.0, compute capability: 7.0)\n",
      "2020-11-25 21:12:36.988211: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x565485eb1ce0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2020-11-25 21:12:36.988231: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-16GB, Compute Capability 7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "strategy = tf.distribute.MirroredStrategy(devices=gpu_list)\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "ngpu = strategy.num_replicas_in_sync\n",
    "print ('Number of devices: {}'.format(ngpu))\n",
    "#sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train the network on $\\pi^+$ and $\\pi^0$ events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "training_dataset = ['pi0','piplus']\n",
    "\n",
    "# create train/validation/test subsets containing 70%/10%/20%\n",
    "# of events from each type of pion event\n",
    "for p_index, plabel in enumerate(training_dataset):\n",
    "    mu.splitFrameTVT(pdata[plabel],trainfrac=0.7)\n",
    "    pdata[plabel]['label'] = p_index\n",
    "\n",
    "# merge pi0 and pi+ events\n",
    "pdata_merged = pd.concat([pdata[ptype] for ptype in training_dataset])\n",
    "pcells_merged = {\n",
    "    layer : np.concatenate([pcells[ptype][layer]\n",
    "                            for ptype in training_dataset])\n",
    "    for layer in layers\n",
    "}\n",
    "plabels = np_utils.to_categorical(pdata_merged['label'],len(training_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define baseline fully-connected NN model\n",
    "def baseline_nn_model(number_pixels):\n",
    "    # create model\n",
    "    with strategy.scope():    \n",
    "        model = Sequential()\n",
    "        used_pixels = number_pixels\n",
    "#     if number_pixels < 128:\n",
    "#         used_pixels = 128\n",
    "        model.add(Dense(number_pixels, input_dim=number_pixels, kernel_initializer='normal', activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(used_pixels, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(int(used_pixels/2), activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(2, kernel_initializer='normal', activation='softmax'))\n",
    "        # compile model\n",
    "        optimizer = Adam(lr=5e-5)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "models = {}\n",
    "for layer in layers:\n",
    "    npix = cell_shapes[layer][0]*cell_shapes[layer][1]\n",
    "    models[layer] = baseline_nn_model(npix)\n",
    "    models[layer].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 100\n",
    "batch_size = 200 * ngpu\n",
    "verbose = 1 # 2 for a lot of printouts\n",
    "\n",
    "model_history = {}\n",
    "model_performance = {}\n",
    "model_scores = {}\n",
    "for layer in layers:\n",
    "    print('On layer ' + layer + '.')\n",
    "    \n",
    "    # train+validate model\n",
    "    model_history[layer] = models[layer].fit(\n",
    "        pcells_merged[layer][pdata_merged.train], plabels[pdata_merged.train],\n",
    "        validation_data = (\n",
    "            pcells_merged[layer][pdata_merged.val], plabels[pdata_merged.val]\n",
    "        ),\n",
    "        epochs = nepochs, batch_size = batch_size, verbose = verbose,\n",
    "    )\n",
    "    \n",
    "    model_history[layer] = model_history[layer].history\n",
    "    \n",
    "    # get overall performance metric\n",
    "    model_performance[layer] = models[layer].evaluate(\n",
    "        pcells_merged[layer][pdata_merged.test], plabels[pdata_merged.test],\n",
    "        verbose = 0,\n",
    "    )\n",
    "    \n",
    "    # get network scores for the dataset\n",
    "    model_scores[layer] = models[layer].predict(\n",
    "        pcells_merged[layer]\n",
    "    )\n",
    "    print('Finished layer ' + layer + '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save this model to a set of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "flat_dir = modelpath + 'flat' # directory for our \"flat\", single calo-layer networks\n",
    "try: os.makedirs(flat_dir)\n",
    "except: pass\n",
    "\n",
    "\n",
    "for layer in layers:\n",
    "    print('Saving ' + layer)\n",
    "    models[layer].save(flat_dir + '/' +'model_' + layer + '_flat_do20.h5')\n",
    "    \n",
    "    with open(flat_dir + '/' + 'model_' + layer + '_flat_do20.history','wb') as model_history_file:\n",
    "        pickle.dump(model_history[layer], model_history_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively we can load a saved model from a set of files (use this to skip training above, if it's been done before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EMB1\n",
      "Loading EMB2\n",
      "Loading EMB3\n",
      "Loading TileBar0\n",
      "Loading TileBar1\n",
      "Loading TileBar2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-25 21:13:01.229805: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "flat_dir = modelpath + 'flat' # directory for our \"flat\", single calo-layer networks\n",
    "models = {}\n",
    "model_history = {}\n",
    "model_scores = {}\n",
    "for layer in layers:\n",
    "    print('Loading ' + layer)\n",
    "    models[layer] = tf.keras.models.load_model(flat_dir + '/'+'model_' + layer + '_flat_do20.h5')\n",
    "    \n",
    "    # load history object\n",
    "    with open(flat_dir + '/' + 'model_' + layer + '_flat_do20.history','rb') as model_history_file:\n",
    "        model_history[layer] = pickle.load(model_history_file)\n",
    "    \n",
    "    # recalculate network scores for the dataset\n",
    "    model_scores[layer] = models[layer].predict(\n",
    "        pcells_merged[layer]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot model accuracy and loss as a function of training epoch, for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Log scale doesn't seem to actually affect the curves. Why? (weird mpl behaviour)\n",
    "use_log = False\n",
    "x_lim = [0.,100.]\n",
    "y_lim = {'acc':[0.5,1.],'loss':[0.2,0.7]}\n",
    "for layer in layers:\n",
    "#     print(history_flat[layer_i].history.keys())\n",
    "    plt.cla(); plt.clf()\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,6))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    if(use_log): \n",
    "        ax1.set_yscale('log')\n",
    "        ax2.set_yscale('log')\n",
    "        \n",
    "    ax1.plot(model_history[layer]['acc'])\n",
    "    ax1.plot(model_history[layer]['val_acc'])\n",
    "    ax1.set_title('model accuracy for ' + layer)\n",
    "    ax1.set(xlabel = 'epoch',ylabel='accuracy')\n",
    "    ax1.set_xlim(x_lim)\n",
    "    ax1.set_ylim(y_lim['acc'])\n",
    "    ax1.legend(['train', 'test'], loc='upper left')\n",
    "    ax1.grid(True)\n",
    "    extent = ax1.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "    plt.savefig('Plots/accuracy_' + layer + '.pdf',bbox_inches=extent)\n",
    "\n",
    "    # summarize history for loss\n",
    "    ax2.plot(model_history[layer]['loss'])\n",
    "    ax2.plot(model_history[layer]['val_loss'])\n",
    "    ax2.set_title('model loss for ' + layer)\n",
    "    ax2.set(xlabel = 'epoch',ylabel='loss')\n",
    "    ax2.set_xlim(x_lim)\n",
    "    ax2.set_ylim(y_lim['loss'])\n",
    "    ax2.legend(['train', 'test'], loc='upper right')\n",
    "    ax2.grid(True)\n",
    "    extent = ax2.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "    plt.savefig(plotpath + 'loss_' + layer + '.pdf',bbox_inches=extent)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at ROC curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "roc_fpr = {}\n",
    "roc_tpr = {}\n",
    "roc_thresh = {}\n",
    "roc_auc = {}\n",
    "\n",
    "for layer in layers:\n",
    "    roc_fpr[layer], roc_tpr[layer], roc_thresh[layer] = roc_curve(\n",
    "        plabels[pdata_merged.test][:,1],\n",
    "        model_scores[layer][pdata_merged.test,1],\n",
    "        drop_intermediate=False,\n",
    "    )\n",
    "    roc_auc[layer] = auc(roc_fpr[layer], roc_tpr[layer])\n",
    "    print('Area under curve for ' + layer + ': ' + str(roc_auc[layer]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the area under the curve for the old method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_fpr, lc_tpr, lc_thresh = roc_curve(\n",
    "    plabels[pdata_merged.test][:,1],\n",
    "    1-pdata_merged[\"cluster_EM_PROBABILITY\"][pdata_merged.test],\n",
    ")\n",
    "lc_auc = auc(lc_fpr, lc_tpr)\n",
    "print(\"Area under curve for cluster_EM_PROB: \" + str(lc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare one of our new, simple networks, against the old method `EMProb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_method = 'EMB1'\n",
    "pu.roc_plot([lc_fpr,roc_fpr[comp_method]],[lc_tpr,roc_tpr[comp_method]],\n",
    "            figfile=plotpath + 'roc_lc_only.pdf',\n",
    "            labels=['LC EMProb (area = {:.3f})'.format(lc_auc),\n",
    "                    comp_method +' (area = {:.3f})'.format(roc_auc[comp_method])],\n",
    "            extra_lines=[[[0, 1], [0, 1]]],\n",
    "            title='Simple NN ROC curve: classification of $\\pi^+$ vs. $\\pi^0$')\n",
    "\n",
    "# plt.cla(); plt.clf()\n",
    "# fig = plt.figure()\n",
    "# fig.patch.set_facecolor('white')\n",
    "# plt.plot(lc_fpr, lc_tpr, label='LC EMProb (area = {:.3f})'.format(lc_auc))\n",
    "# plt.plot(roc_fpr['EMB1'], roc_tpr['EMB1'], label='EMB1 (area = {:.3f})'.format(roc_auc['EMB1']))\n",
    "# plt.plot([0, 1], [0, 1], 'k--')\n",
    "# plt.xlim(0,1.1)\n",
    "# plt.ylim(0,1.1)\n",
    "# plt.title('Simple NN ROC curve: classification of $\\pi^+$ vs. $\\pi^0$')\n",
    "# ampl.set_xlabel('False positive rate')\n",
    "# ampl.set_ylabel('True positive rate')\n",
    "# plt.legend(loc='best')\n",
    "# plt.savefig(plotpath + 'roc_lc_only.pdf')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.cla(); plt.clf()\n",
    "# fig, ax = plt.subplots(1, 2, tight_layout=True, figsize=(10,4))\n",
    "# fig.patch.set_facecolor('white')\n",
    "\n",
    "# colors for our simple NN's\n",
    "colors = ['xkcd:red','xkcd:light orange','xkcd:gold','xkcd:green','xkcd:blue','xkcd:violet']\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(lc_fpr, lc_tpr, label='LC EMProb (area = {:.3f})'.format(lc_auc),linestyle='-.')\n",
    "for layer_i, layer_name in enumerate(layers):\n",
    "    plt.plot(roc_fpr[layer_name], roc_tpr[layer_name], label='{} (area = {:.3f})'.format(layer_name, roc_auc[layer_name]),color=colors[layer_i])\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('Simple NN ROC curve: classification of $\\pi^+$ vs. $\\pi^0$')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('Plots/roc_layers.pdf')\n",
    "plt.show()\n",
    "\n",
    "# Zoom in view of the upper left corner.\n",
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.xlim(0, 0.25)\n",
    "plt.ylim(0.6, 1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(lc_fpr, lc_tpr, label='LC EMProb (area = {:.3f})'.format(lc_auc),linestyle='-.')\n",
    "for layer_i, layer_name in enumerate(layers):\n",
    "    plt.plot(roc_fpr[layer_name], roc_tpr[layer_name], label='{} (area = {:.3f})'.format(layer_name, roc_auc[layer_name]),color=colors[layer_i])\n",
    "# ax[1].plot(fpr_nn, tpr_nn, label='Simple NN (area = {:.3f})'.format(auc_nn))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve (zoomed in at top left)')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('Plots/roc_zoom_layers.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve Scans\n",
    "<div style=\"text-align: right\"> <a href=\"#Image-Classification-of-ATLAS-Calorimeter-Topo-Clusters-Rewrite\">Top</a> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a convenience class for creating ROC curve scans\n",
    "display_digits=2\n",
    "class roc_var:\n",
    "    def __init__(self,\n",
    "                 name, # name of variable as it appears in the root file\n",
    "                 bins, # endpoints of bins as a list\n",
    "                 df,   # dataframe to construct subsets from\n",
    "                 latex='', # optional latex to display variable name with\n",
    "                 vlist=None, # optional list to append class instance to\n",
    "                ):\n",
    "        self.name = name\n",
    "        self.bins = bins\n",
    "        \n",
    "        if(latex == ''): self.latex = name\n",
    "        else:            self.latex = latex\n",
    "        \n",
    "        self.selections = []\n",
    "        self.labels = []\n",
    "        for i, point in enumerate(self.bins):\n",
    "            if(i == 0):\n",
    "                self.selections.append( df[name]<point )\n",
    "                self.labels.append(self.latex+'<'+str(round(point,display_digits)))\n",
    "            else:\n",
    "                self.selections.append( (df[name]>self.bins[i-1]) & (df[name]<self.bins[i]) )\n",
    "                self.labels.append(str(round(self.bins[i-1],display_digits))+'<'+self.latex+'<'+str(round(point,display_digits)))\n",
    "                if(i == len(bins)-1):\n",
    "                    self.selections.append( df[name]>point )\n",
    "                    self.labels.append(self.latex+'>'+str(round(point,display_digits)))\n",
    "        \n",
    "        if(vlist != None):\n",
    "            vlist.append(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_scan(varlist,scan_targets,labels):\n",
    "    '''\n",
    "    Creates a set of ROC curve plots by scanning over the specified variables.\n",
    "    One set is created for each target (neural net score dataset).\n",
    "    \n",
    "    varlist: a list of roc_var instances to scan over\n",
    "    scan_targets: a list of neural net score datasets to use\n",
    "    labels: a list of target names (strings); must be the same length as scan_targets\n",
    "    '''\n",
    "    for target, target_label in zip(scan_targets,labels):\n",
    "        for v in varlist:\n",
    "            # prepare matplotlib figure\n",
    "            plt.cla()\n",
    "            plt.clf()\n",
    "            fig = plt.figure()\n",
    "            fig.patch.set_facecolor('white')\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "            for binning, label in zip(v.selections,v.labels):\n",
    "                # first generate ROC curve\n",
    "                x, y, t = roc_curve(\n",
    "                    plabels[pdata_merged.test & binning][:,1],\n",
    "                    target[pdata_merged.test & binning],\n",
    "                    drop_intermediate=False,\n",
    "                )\n",
    "                var_auc = auc(x,y)\n",
    "                plt.plot(x, y, label=label+' (area = {:.3f})'.format(var_auc))\n",
    "\n",
    "            plt.title('ROC Scan of '+target_label+' over '+v.latex)\n",
    "            plt.xlim(0,1.1)\n",
    "            plt.ylim(0,1.1)\n",
    "            ampl.set_xlabel('False positive rate')\n",
    "            ampl.set_ylabel('True positive rate')\n",
    "            plt.legend()\n",
    "            plt.savefig(plotpath+'roc_scan_'+target_label+'_'+v.name+'.pdf')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify variables we are interested in scanning over\n",
    "varlist = []\n",
    "cluster_e = roc_var(\n",
    "    name='clusterE',\n",
    "    bins=[1,10,50,500],\n",
    "    df=pdata_merged,\n",
    "    vlist=varlist,\n",
    ")\n",
    "\n",
    "pdata_merged['abs_clusterEta'] = np.abs(pdata_merged.clusterEta)\n",
    "cluster_eta = roc_var(\n",
    "    name='abs_clusterEta',\n",
    "    bins=[0.2,0.4,0.6],\n",
    "    df=pdata_merged,\n",
    "    vlist=varlist,\n",
    "    latex='abs(clusterEta)'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin the scan\n",
    "targets = [model_scores[layer][:,1] for layer in layers]+[1-pdata_merged[\"cluster_EM_PROBABILITY\"]]\n",
    "labels = layers+['LC']\n",
    "roc_scan(varlist,targets,labels)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination Network\n",
    "<div style=\"text-align: right\"> <a href=\"#Image-Classification-of-ATLAS-Calorimeter-Topo-Clusters-Rewrite\">Top</a> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train a simple combination network... its inputs will be the *outputs* of our simple, feed-forward neural networks from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2450/2450 - 6s - loss: 0.2515 - acc: 0.9060 - val_loss: 0.1888 - val_acc: 0.9347\n",
      "Epoch 2/50\n",
      "2450/2450 - 5s - loss: 0.1791 - acc: 0.9372 - val_loss: 0.1823 - val_acc: 0.9345\n",
      "Epoch 3/50\n",
      "2450/2450 - 5s - loss: 0.1758 - acc: 0.9374 - val_loss: 0.1809 - val_acc: 0.9348\n",
      "Epoch 4/50\n",
      "2450/2450 - 5s - loss: 0.1750 - acc: 0.9375 - val_loss: 0.1806 - val_acc: 0.9350\n",
      "Epoch 5/50\n",
      "2450/2450 - 5s - loss: 0.1747 - acc: 0.9374 - val_loss: 0.1802 - val_acc: 0.9347\n",
      "Epoch 6/50\n",
      "2450/2450 - 5s - loss: 0.1746 - acc: 0.9375 - val_loss: 0.1800 - val_acc: 0.9349\n",
      "Epoch 7/50\n",
      "2450/2450 - 6s - loss: 0.1745 - acc: 0.9375 - val_loss: 0.1799 - val_acc: 0.9349\n",
      "Epoch 8/50\n",
      "2450/2450 - 6s - loss: 0.1744 - acc: 0.9375 - val_loss: 0.1798 - val_acc: 0.9349\n",
      "Epoch 9/50\n",
      "2450/2450 - 5s - loss: 0.1744 - acc: 0.9374 - val_loss: 0.1800 - val_acc: 0.9346\n",
      "Epoch 10/50\n",
      "2450/2450 - 5s - loss: 0.1744 - acc: 0.9376 - val_loss: 0.1801 - val_acc: 0.9349\n",
      "Epoch 11/50\n",
      "2450/2450 - 5s - loss: 0.1743 - acc: 0.9375 - val_loss: 0.1797 - val_acc: 0.9349\n",
      "Epoch 12/50\n",
      "2450/2450 - 5s - loss: 0.1743 - acc: 0.9375 - val_loss: 0.1797 - val_acc: 0.9349\n",
      "Epoch 13/50\n",
      "2450/2450 - 5s - loss: 0.1742 - acc: 0.9375 - val_loss: 0.1804 - val_acc: 0.9351\n",
      "Epoch 14/50\n",
      "2450/2450 - 5s - loss: 0.1742 - acc: 0.9376 - val_loss: 0.1796 - val_acc: 0.9350\n",
      "Epoch 15/50\n",
      "2450/2450 - 5s - loss: 0.1742 - acc: 0.9375 - val_loss: 0.1797 - val_acc: 0.9351\n",
      "Epoch 16/50\n",
      "2450/2450 - 5s - loss: 0.1741 - acc: 0.9376 - val_loss: 0.1796 - val_acc: 0.9348\n",
      "Epoch 17/50\n",
      "2450/2450 - 5s - loss: 0.1741 - acc: 0.9375 - val_loss: 0.1800 - val_acc: 0.9352\n",
      "Epoch 18/50\n",
      "2450/2450 - 5s - loss: 0.1742 - acc: 0.9374 - val_loss: 0.1799 - val_acc: 0.9352\n",
      "Epoch 19/50\n",
      "2450/2450 - 5s - loss: 0.1741 - acc: 0.9375 - val_loss: 0.1794 - val_acc: 0.9350\n",
      "Epoch 20/50\n",
      "2450/2450 - 5s - loss: 0.1741 - acc: 0.9376 - val_loss: 0.1796 - val_acc: 0.9349\n",
      "Epoch 21/50\n",
      "2450/2450 - 5s - loss: 0.1741 - acc: 0.9376 - val_loss: 0.1793 - val_acc: 0.9349\n",
      "Epoch 22/50\n",
      "2450/2450 - 5s - loss: 0.1741 - acc: 0.9375 - val_loss: 0.1794 - val_acc: 0.9349\n",
      "Epoch 23/50\n",
      "2450/2450 - 5s - loss: 0.1741 - acc: 0.9375 - val_loss: 0.1796 - val_acc: 0.9350\n",
      "Epoch 24/50\n",
      "2450/2450 - 5s - loss: 0.1740 - acc: 0.9376 - val_loss: 0.1802 - val_acc: 0.9350\n",
      "Epoch 25/50\n",
      "2450/2450 - 5s - loss: 0.1741 - acc: 0.9375 - val_loss: 0.1793 - val_acc: 0.9349\n",
      "Epoch 26/50\n",
      "2450/2450 - 5s - loss: 0.1741 - acc: 0.9375 - val_loss: 0.1795 - val_acc: 0.9349\n",
      "Epoch 27/50\n",
      "2450/2450 - 5s - loss: 0.1741 - acc: 0.9374 - val_loss: 0.1804 - val_acc: 0.9346\n",
      "Epoch 28/50\n",
      "2450/2450 - 5s - loss: 0.1741 - acc: 0.9375 - val_loss: 0.1792 - val_acc: 0.9349\n",
      "Epoch 29/50\n",
      "2450/2450 - 5s - loss: 0.1740 - acc: 0.9375 - val_loss: 0.1797 - val_acc: 0.9351\n",
      "Epoch 30/50\n",
      "2450/2450 - 5s - loss: 0.1740 - acc: 0.9376 - val_loss: 0.1792 - val_acc: 0.9350\n",
      "Epoch 31/50\n",
      "2450/2450 - 5s - loss: 0.1740 - acc: 0.9376 - val_loss: 0.1795 - val_acc: 0.9347\n",
      "Epoch 32/50\n",
      "2450/2450 - 5s - loss: 0.1740 - acc: 0.9376 - val_loss: 0.1795 - val_acc: 0.9348\n",
      "Epoch 33/50\n",
      "2450/2450 - 5s - loss: 0.1740 - acc: 0.9376 - val_loss: 0.1793 - val_acc: 0.9349\n",
      "Epoch 34/50\n",
      "2450/2450 - 5s - loss: 0.1740 - acc: 0.9375 - val_loss: 0.1792 - val_acc: 0.9348\n",
      "Epoch 35/50\n",
      "2450/2450 - 5s - loss: 0.1739 - acc: 0.9376 - val_loss: 0.1793 - val_acc: 0.9352\n",
      "Epoch 36/50\n",
      "2450/2450 - 5s - loss: 0.1739 - acc: 0.9376 - val_loss: 0.1795 - val_acc: 0.9351\n",
      "Epoch 37/50\n",
      "2450/2450 - 5s - loss: 0.1740 - acc: 0.9375 - val_loss: 0.1792 - val_acc: 0.9347\n",
      "Epoch 38/50\n",
      "2450/2450 - 5s - loss: 0.1740 - acc: 0.9376 - val_loss: 0.1793 - val_acc: 0.9349\n",
      "Epoch 39/50\n",
      "2450/2450 - 5s - loss: 0.1740 - acc: 0.9376 - val_loss: 0.1792 - val_acc: 0.9350\n",
      "Epoch 40/50\n",
      "2450/2450 - 5s - loss: 0.1739 - acc: 0.9377 - val_loss: 0.1792 - val_acc: 0.9350\n",
      "Epoch 41/50\n",
      "2450/2450 - 5s - loss: 0.1739 - acc: 0.9376 - val_loss: 0.1793 - val_acc: 0.9352\n",
      "Epoch 42/50\n",
      "2450/2450 - 5s - loss: 0.1739 - acc: 0.9377 - val_loss: 0.1798 - val_acc: 0.9351\n",
      "Epoch 43/50\n",
      "2450/2450 - 5s - loss: 0.1739 - acc: 0.9376 - val_loss: 0.1793 - val_acc: 0.9351\n",
      "Epoch 44/50\n",
      "2450/2450 - 5s - loss: 0.1739 - acc: 0.9377 - val_loss: 0.1792 - val_acc: 0.9350\n",
      "Epoch 45/50\n",
      "2450/2450 - 5s - loss: 0.1739 - acc: 0.9377 - val_loss: 0.1798 - val_acc: 0.9348\n",
      "Epoch 46/50\n",
      "2450/2450 - 5s - loss: 0.1740 - acc: 0.9376 - val_loss: 0.1794 - val_acc: 0.9347\n",
      "Epoch 47/50\n",
      "2450/2450 - 5s - loss: 0.1738 - acc: 0.9376 - val_loss: 0.1790 - val_acc: 0.9350\n",
      "Epoch 48/50\n",
      "2450/2450 - 5s - loss: 0.1739 - acc: 0.9377 - val_loss: 0.1800 - val_acc: 0.9353\n",
      "Epoch 49/50\n",
      "2450/2450 - 5s - loss: 0.1739 - acc: 0.9376 - val_loss: 0.1799 - val_acc: 0.9352\n",
      "Epoch 50/50\n",
      "2450/2450 - 5s - loss: 0.1739 - acc: 0.9376 - val_loss: 0.1795 - val_acc: 0.9347\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model_scores_stack = np.column_stack( [model_scores[layer][:,1] for layer in model_scores] )\n",
    "model_simpleCombine = Sequential()\n",
    "model_simpleCombine.add(Dense(6, input_dim=6, kernel_initializer='normal', activation='relu'))\n",
    "model_simpleCombine.add(Dense(4, activation='relu'))\n",
    "model_simpleCombine.add(Dense(2, kernel_initializer='normal', activation='softmax'))\n",
    "model_simpleCombine.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n",
    "simpleCombine_history = model_simpleCombine.fit(model_scores_stack[pdata_merged.train], plabels[pdata_merged.train],\n",
    "                                                validation_data=(model_scores_stack[pdata_merged.val],\n",
    "                                                                 plabels[pdata_merged.val]),\n",
    "                                                epochs = 50, batch_size = 200*ngpu, verbose = 2)\n",
    "simpleCombine_history = simpleCombine_history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and save the results to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/LCStudies/classifier/Models/simple/model_simple_do20.h5\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "simple_dir = modelpath + 'simple' # directory for our \"simple\" network\n",
    "try: os.makedirs(simple_dir)\n",
    "except: pass\n",
    "\n",
    "model_simpleCombine.save(simple_dir + '/' +'model_simple_do20.h5')\n",
    "with open(simple_dir + '/' + 'model_simple_do20.history','wb') as model_history_file:\n",
    "    pickle.dump(simpleCombine_history, model_history_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also load the model from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "simple_dir = modelpath + 'simple' # directory for our \"simple\" network\n",
    "model_simpleCombine = tf.keras.models.load_model(simple_dir + '/' + 'model_simple_do20.h5')\n",
    "with open(simple_dir + '/' + 'model_simple_do20.history','rb') as model_history_file:\n",
    "    simpleCombine_history = pickle.load(model_history_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot some results from this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pu.make_plot(\n",
    "    [simpleCombine_history['acc'],simpleCombine_history['val_acc']],\n",
    "    figfile = plotpath+'accuracy_simpleCombine.pdf',\n",
    "    xlabel = 'epoch', ylabel = 'accuracy',\n",
    "    x_log = False, y_log = False,\n",
    "    labels = ['train','test'],\n",
    "    title = 'Model accuracy for simple combination',\n",
    ")\n",
    "\n",
    "# summarize history for loss\n",
    "pu.make_plot(\n",
    "    [simpleCombine_history['loss'],simpleCombine_history['val_loss']],\n",
    "    figfile = plotpath+'loss_simpleCombine.pdf',\n",
    "    xlabel = 'epoch', ylabel = 'loss',\n",
    "    x_log = False, y_log = False,\n",
    "    labels = ['train','test'],\n",
    "    title = 'Model loss for simple combination',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "simpleCombine_score = model_simpleCombine.predict(model_scores_stack)\n",
    "simpleCombine_fpr, simpleCombine_tpr, simpleCombine_thresh = roc_curve(\n",
    "    plabels[pdata_merged.test,1], simpleCombine_score[pdata_merged.test,1]\n",
    ")\n",
    "simpleCombine_auc = auc(simpleCombine_fpr, simpleCombine_tpr)\n",
    "print(\"Area under curve for simpleCombine: \" + str(simpleCombine_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pu.roc_plot(\n",
    "    [simpleCombine_fpr,lc_fpr]+[roc_fpr[layer] for layer in layers],\n",
    "    [simpleCombine_tpr,lc_tpr]+[roc_tpr[layer] for layer in layers],\n",
    "    figfile = plotpath+'roc_simpleCombine.pdf',\n",
    "    extra_lines=[[[0, 1], [0, 1]]], labels=[\n",
    "        'simpleCombine (area = {:.3f})'.format(simpleCombine_auc),\n",
    "        'LC EMProb (area = {:.3f})'.format(lc_auc),\n",
    "    ]+[layer+' (area = {:.3f})'.format(roc_auc[layer]) for layer in layers],\n",
    "    title='Simple NN ROC curve: classification of $\\pi^+$ vs. $\\pi^0$')\n",
    "\n",
    "pu.roc_plot(\n",
    "    [simpleCombine_fpr,lc_fpr]+[roc_fpr[layer] for layer in layers],\n",
    "    [simpleCombine_tpr,lc_tpr]+[roc_tpr[layer] for layer in layers],\n",
    "    figfile = plotpath+'roc_simpleCombine.pdf',\n",
    "    x_max=0.25, y_min=0.6,\n",
    "    extra_lines=[[[0, 1], [0, 1]]], labels=[\n",
    "        'simpleCombine (area = {:.3f})'.format(simpleCombine_auc),\n",
    "        'LC EMProb (area = {:.3f})'.format(lc_auc),\n",
    "    ]+[layer+' (area = {:.3f})'.format(roc_auc[layer]) for layer in layers],\n",
    "    title='Simple NN ROC curve: zoomed in at top left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_scan(varlist,[simpleCombine_score[:,1]],['simpleCombine'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also train a deep combination network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    \n",
    "    model_combine = Sequential()\n",
    "    inputs = [\n",
    "        Input(\n",
    "            shape=(cell_shapes[layer][0]*cell_shapes[layer][1]),\n",
    "            name=layer\n",
    "        )\n",
    "        for layer in layers\n",
    "    ]\n",
    "    \n",
    "    combine_inputs = []\n",
    "    for x, layer in zip(inputs, layers):\n",
    "        input_shape = cell_shapes[layer][0]*cell_shapes[layer][1]\n",
    "        x_condensed = Dense(input_shape, activation='relu')(x)\n",
    "        x_condensed = Dropout(0.2)(x_condensed)\n",
    "        x_condensed = Dense(input_shape/2, activation='relu')(x_condensed)\n",
    "        x_condensed = Dropout(0.2)(x_condensed)\n",
    "        x_condensed = Dense(input_shape/4, activation='relu')(x_condensed)\n",
    "        x_condensed = Dropout(0.2)(x_condensed)\n",
    "        combine_inputs.append(x_condensed)\n",
    "        \n",
    "    combine_output = concatenate([x for x in combine_inputs])\n",
    "    combine_output = Dense(100, activation='relu')(combine_output)\n",
    "    combine_output = Dropout(0.2)(combine_output)\n",
    "    combine_output = Dense(50, activation='relu')(combine_output)\n",
    "    combine_output = Dropout(0.2)(combine_output)\n",
    "    combine_output = Dense(2, activation='softmax')(combine_output)\n",
    "    \n",
    "    model_combine = Model(inputs=inputs, outputs=[combine_output])\n",
    "    model_combine.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=Adam(lr=5e-5),\n",
    "        metrics=['acc']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 100\n",
    "batch_size = 200*ngpu\n",
    "verbose = 2\n",
    "combine_history = model_combine.fit(\n",
    "    [pcells_merged[layer][pdata_merged.train] for layer in layers],\n",
    "    [plabels[pdata_merged.train]],\n",
    "    validation_data=(\n",
    "        [pcells_merged[layer][pdata_merged.val] for layer in layers],\n",
    "        [plabels[pdata_merged.val]]\n",
    "    ),\n",
    "    epochs=nepochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=verbose\n",
    ")\n",
    "combine_history = combine_history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the network to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model_combine.save(modelpath+\"model_deep_do20.h5\")\n",
    "with open(modelpath + 'model_deep_do20.history','wb') as model_history_file:\n",
    "    pickle.dump(combine_history, model_history_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model_combine = tf.keras.models.load_model(modelpath+\"model_deep_do20.h5\")\n",
    "with open(modelpath + 'model_deep_do20.history','rb') as model_history_file:\n",
    "    combine_history = pickle.load(model_history_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pu.make_plot(\n",
    "    [combine_history['acc'],combine_history['val_acc']],\n",
    "    figfile = plotpath+'accuracy_deepCombine.pdf',\n",
    "    xlabel = 'epoch', ylabel = 'accuracy',\n",
    "    x_log = False, y_log = False,\n",
    "    labels = ['train','test'],\n",
    "    title = 'Model accuracy for deep combination',\n",
    ")\n",
    "\n",
    "# summarize history for loss\n",
    "pu.make_plot(\n",
    "    [combine_history['loss'],combine_history['val_loss']],\n",
    "    figfile = plotpath+'loss_deepCombine.pdf',\n",
    "    xlabel = 'epoch', ylabel = 'loss',\n",
    "    x_log = False, y_log = False,\n",
    "    labels = ['train','test'],\n",
    "    title = 'Model loss for deep combination',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "combine_score = model_combine.predict(\n",
    "    [ pcells_merged[layer] for layer in layers ]\n",
    ")\n",
    "combine_fpr, combine_tpr, combine_thresh = roc_curve(\n",
    "    plabels[pdata_merged.test][:,1],\n",
    "    combine_score[pdata_merged.test,1]\n",
    ")\n",
    "combine_auc = auc(combine_fpr, combine_tpr)\n",
    "print(\"Area under curve: {}\".format(combine_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pu.roc_plot(\n",
    "    [simpleCombine_fpr,combine_fpr,lc_fpr,roc_fpr['EMB1']],\n",
    "    [simpleCombine_tpr,combine_tpr,lc_tpr,roc_tpr['EMB1']],\n",
    "    figfile=plotpath + 'roc_combine.pdf',\n",
    "    labels=[\n",
    "        'simpleCombine (area = {:.3f})'.format(simpleCombine_auc),\n",
    "        'combine (area = {:.3f})'.format(combine_auc),\n",
    "        'LC EMProb (area = {:.3f})'.format(lc_auc),\n",
    "        'EMB1 (area = {:.3f})'.format(roc_auc['EMB1'])\n",
    "    ],\n",
    "    extra_lines=[[[0, 1], [0, 1]]],\n",
    "    title='ROC curve comparison: classification of $\\pi^+$ vs. $\\pi^0$'\n",
    ")\n",
    "\n",
    "pu.roc_plot(\n",
    "    [simpleCombine_fpr,combine_fpr,lc_fpr,roc_fpr['EMB1']],\n",
    "    [simpleCombine_tpr,combine_tpr,lc_tpr,roc_tpr['EMB1']],\n",
    "    figfile=plotpath + 'roc_combine_zoom.pdf',\n",
    "    x_max=0.25, y_min=0.6, y_max=1,\n",
    "    labels=[\n",
    "        'simpleCombine (area = {:.3f})'.format(simpleCombine_auc),\n",
    "        'combine (area = {:.3f})'.format(combine_auc),\n",
    "        'LC EMProb (area = {:.3f})'.format(lc_auc),\n",
    "        'EMB1 (area = {:.3f})'.format(roc_auc['EMB1'])\n",
    "    ],\n",
    "    extra_lines=[[[0, 1], [0, 1]]],\n",
    "    title='ROC curve comparison: zoomed in at top left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_scan(\n",
    "    varlist,\n",
    "    [combine_score[:,1]],\n",
    "    ['deep combine']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.cla(); plt.clf()\n",
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.yscale('log')\n",
    "plt.plot(tpr_lc, 1/fpr_lc, label='LC EMProb')\n",
    "for layer_i, layer_name in enumerate(layers):\n",
    "    plt.plot(tpr_flat_nn[layer_i], 1/fpr_flat_nn[layer_i], label='{}'.format(layer_name))\n",
    "plt.plot(tpr_simpleCombine, 1/fpr_simpleCombine, label='Layers Simple')\n",
    "plt.plot(tpr_combine, 1/fpr_combine, label='Layers Deep')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('True Efficiency')\n",
    "plt.ylabel('Bkgd Rejection')\n",
    "plt.savefig(plotpath+'roc_reg_combine2_layers.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to split apart a prediction list according to the one hot values.\n",
    "#really, could have done this by evaluating the nn with a non-concatted nparray, but we already have this handy\n",
    "def split_pred(onehot, pred):\n",
    "    list_one = [element_n for element_n, element in enumerate(onehot) if element==1]\n",
    "    one_vals = [element for element_n, element in enumerate(pred) if element_n in list_one]\n",
    "    zer_vals = [element for element_n, element in enumerate(pred) if element_n not in list_one]\n",
    "    \n",
    "    return one_vals, zer_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 50\n",
    "#time for histogramming\n",
    "plt.cla(); plt.clf()\n",
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.hist(1-p0['cluster_EM_PROBABILITY'], bins=n_bins, normed=True, alpha=0.7, label='$\\pi^0$')\n",
    "plt.hist(1-pp['cluster_EM_PROBABILITY'], bins=n_bins, normed=True, alpha=0.7, label='$\\pi^+$')\n",
    "plt.xlabel('1-Cluster EMProb')\n",
    "plt.legend()\n",
    "plt.savefig('Plots/hist_emprob.pdf')\n",
    "plt.show()\n",
    "\n",
    "pip_vals_combine, p0_vals_combine = split_pred(y_flat_te_onehot[layer_i][:,1], y_combine_pred[:,1])\n",
    "\n",
    "plt.cla(); plt.clf()\n",
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.hist(p0_vals_combine, bins=n_bins, normed=True, alpha=0.7, label='$\\pi^0$')\n",
    "plt.hist(pip_vals_combine, bins=n_bins, normed=True, alpha=0.7, label='$\\pi^+$')\n",
    "plt.xlabel('Deep Combined NN')\n",
    "plt.legend()\n",
    "plt.savefig('Plots/hist_deep.pdf')\n",
    "plt.show()\n",
    "\n",
    "pip_vals_simpleCombine, p0_vals_simpleCombine = split_pred(y_flat_vl_onehot[layer_i][:,1], combined_pred[:,1])\n",
    "plt.cla(); plt.clf()\n",
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.hist(p0_vals_simpleCombine, bins=n_bins, normed=True, alpha=0.7, label='$\\pi^0$')\n",
    "plt.hist(pip_vals_simpleCombine, bins=n_bins, normed=True, alpha=0.7, label='$\\pi^+$')\n",
    "plt.xlabel('Simple Combined NN')\n",
    "plt.legend()\n",
    "plt.savefig('Plots/hist_simple.pdf')\n",
    "plt.show()\n",
    "\n",
    "pip_vals_flat = {}\n",
    "p0_vals_flat = {}\n",
    "for layer_i, layer_name in enumerate(layers):\n",
    "    plt.cla(); plt.clf()\n",
    "    pip_vals_flat[layer_i], p0_vals_flat[layer_i] = split_pred(y_flat_te_onehot[layer_i][:,1], y_flat_pred[layer_i][:,1])\n",
    "    fig = plt.figure()\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.hist(p0_vals_flat[layer_i], bins=n_bins, normed=True, alpha=0.7, label='$\\pi^0$')\n",
    "    plt.hist(pip_vals_flat[layer_i], bins=n_bins, normed=True, alpha=0.7, label='$\\pi^+$')\n",
    "    plt.xlabel('{} Flat NN'.format(layer_name))\n",
    "    plt.legend()\n",
    "    plt.savefig('Plots/hist_flatnn_{}.pdf'.format(layer_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "<div style=\"text-align: right\"> <a href=\"#Image-Classification-of-ATLAS-Calorimeter-Topo-Clusters-Rewrite\">Top</a> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_cnn(data_0, data_1, selected_layer, len_phi = len_phi, len_eta = len_eta):\n",
    "    layer = layers[selected_layer]\n",
    "    num_pixels = int(len_phi[selected_layer]*len_eta[selected_layer])\n",
    "    X = np.append(data_0[layer], data_1[layer], axis=0)\n",
    "    X = X.reshape(X.shape[0], 1, len_phi[selected_layer], len_eta[selected_layer])\n",
    "    \n",
    "    Y = np.append(data_0['label'], data_1['label'], axis=0)\n",
    "    return X, Y, num_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#old, one by one method\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# Train on pT = 100 GeV electron/pion samples\n",
    "# mswiatlo-- let's do this with the pi+/pi0 samples!\n",
    "\n",
    "# include the EMB2 layer as a 2D image\n",
    "selected_layer = 1\n",
    "layer = layers[selected_layer]\n",
    "num_pixels = int(len_phi[selected_layer]*len_eta[selected_layer])\n",
    "\n",
    "# X = np.append(elec[layer], pion[layer], axis=0)\n",
    "X = np.append(pp[layer], p0[layer], axis=0)\n",
    "# reshape to be [samples][width][height][pixels]\n",
    "#X = X.reshape(X.shape[0], 1, 128, 4)\n",
    "X = X.reshape(X.shape[0], 1, 16, 16)\n",
    "\n",
    "# y = np.append(elec['label'], pion['label'], axis=0)\n",
    "y = np.append(pp['label'], p0['label'], axis=0)\n",
    "\n",
    "# split up into training and test datasets\n",
    "test_size = 4000\n",
    "validation_size = 1000\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_te_vl, y_tr, y_te_vl, = train_test_split(X, y, test_size=test_size+validation_size, random_state=1)\n",
    "X_te, X_vl, y_te, y_vl, = train_test_split(X_te_vl, y_te_vl, test_size=validation_size, random_state=1)\n",
    "\n",
    "print (X_tr.shape)\n",
    "print (y_tr.shape)\n",
    "\n",
    "# one hot encode outputs\n",
    "from keras.utils import np_utils\n",
    "y_tr_onehot = np_utils.to_categorical(y_tr, 2)\n",
    "y_te_onehot = np_utils.to_categorical(y_te, 2)\n",
    "y_vl_onehot = np_utils.to_categorical(y_vl, 2)\n",
    "num_classes = y_te_onehot.shape[1]\n",
    "print(\"y_train.shape: \",y_tr.shape)\n",
    "print(\"y_train_onehot.shape: \",y_tr_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras import backend as K\n",
    "# K.image_data_format('th')\n",
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "x_cnn = {}\n",
    "y_cnn = {}\n",
    "num_pixels_cnn = {}\n",
    "for layer_i, layer_name in enumerate(layers):\n",
    "    x_cnn[layer_i], y_cnn[layer_i], num_pixels_cnn[layer_i] = format_cnn(p0, pp, layer_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up into training and test datasets, using functions\n",
    "test_size = 24000\n",
    "validation_size = 24000\n",
    "\n",
    "x_cnn_tr = {}\n",
    "x_cnn_te = {}\n",
    "x_cnn_vl = {}\n",
    "y_cnn_tr_onehot = {}\n",
    "y_cnn_te_onehot = {}\n",
    "y_cnn_vl_onehot = {}\n",
    "for layer_i, layer_name in enumerate(layers):\n",
    "    x_cnn_tr[layer_i], x_cnn_te[layer_i], x_cnn_vl[layer_i], y_cnn_tr_onehot[layer_i], y_cnn_te_onehot[layer_i], y_cnn_vl_onehot[layer_i] = make_split_samples(x_cnn[layer_i], y_cnn[layer_i], test_size, validation_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple convolutional model\n",
    "def baseline_ccn_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (4, 4), input_shape=(1,16,16), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(2, kernel_initializer='normal', activation='softmax'))\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple convolutional model\n",
    "\n",
    "filters = [(2,4), (4,4), (4,4), (2,2), (2,2), (2,1)]\n",
    "\n",
    "def baseline_cnn_model_layers(layer_i):\n",
    "    # create model\n",
    "    with strategy.scope():\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, filters[layer_i], input_shape=(1,len_phi[layer_i],len_eta[layer_i]), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(2, kernel_initializer='normal', activation='softmax'))\n",
    "        # compile model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_base_layers = {}\n",
    "for layer_i, layer_name in enumerate(layers):\n",
    "    print(layer_name)\n",
    "    print(len_phi[layer_i])\n",
    "    print(len_eta[layer_i])\n",
    "    model_cnn_base_layers[layer_i] = baseline_cnn_model_layers(layer_i)\n",
    "    model_cnn_base_layers[layer_i].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a deep-ish convolutional model\n",
    "def larger_cnn_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (4, 4), input_shape=(1,16,16), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(16, (2, 2), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(2, kernel_initializer='normal', activation='softmax'))\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "model_cnn_large = larger_cnn_model()\n",
    "model_cnn_large.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cnn_base_layers = {}\n",
    "for layer_i, layer_name in enumerate(layers):\n",
    "    print('On layer {}'.format(layer_name))\n",
    "    history_cnn_base_layers[layer_i] = model_cnn_base_layers[layer_i].fit(x_cnn_tr[layer_i], y_cnn_tr_onehot[layer_i], validation_data=(x_cnn_vl[layer_i], y_cnn_vl_onehot[layer_i]), epochs=200, batch_size=200*ngpu, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_i, layer_name in enumerate(layers):\n",
    "#     print(history_flat[layer_i].history.keys())\n",
    "    plt.cla(); plt.clf()\n",
    "    fig = plt.figure()\n",
    "    fig.patch.set_facecolor('white')\n",
    "\n",
    "    plt.plot(history_cnn_base_layers[layer_i].history['acc'])\n",
    "    plt.plot(history_cnn_base_layers[layer_i].history['val_acc'])\n",
    "    plt.title('model accuracy for {}'.format(layer_name))\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig('Plots/accuracy_cnn_{}.pdf'.format(layer_name))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # summarize history for loss\n",
    "    fig = plt.figure()\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(history_cnn_base_layers[layer_i].history['loss'])\n",
    "    plt.plot(history_cnn_base_layers[layer_i].history['val_loss'])\n",
    "    plt.title('model loss for {}'.format(layer_name))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig('Plots/loss_cnn_{}.pdf'.format(layer_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit (train) the larger cnn model\n",
    "history_cnn_large = model_cnn_large.fit(X_tr, y_tr_onehot, validation_data=(X_vl, y_vl_onehot), epochs=40, batch_size=200, verbose=2)\n",
    "\n",
    "# final evaluation of the model\n",
    "scores_cnn_large = model_cnn_large.evaluate(X_te, y_te_onehot, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores_cnn_large[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy and loss vs. epoch\n",
    "\n",
    "# list all data in history\n",
    "print(history_cnn_large.history.keys())\n",
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history_cnn_large.history['acc'])\n",
    "plt.plot(history_cnn_large.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.plot(history_cnn_large.history['loss'])\n",
    "plt.plot(history_cnn_large.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_baseline_pred = model_cnn_base.predict(X_te)\n",
    "fpr_baseline, tpr_baseline, thres_baseline = roc_curve(y_te_onehot[:,1], y_baseline_pred[:,1])\n",
    "auc_baseline = auc(fpr_baseline, tpr_baseline)\n",
    "print(\"Area under curve: {}\".format(auc_baseline))\n",
    "\n",
    "y_large_pred = model_cnn_large.predict(X_te)\n",
    "fpr_large, tpr_large, thres_large = roc_curve(y_te_onehot[:,1], y_large_pred[:,1])\n",
    "auc_large = auc(fpr_large, tpr_large)\n",
    "print(\"Area under curve: {}\".format(auc_large))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_baseline_layers_pred = {}\n",
    "fpr_baseline_layers = {}\n",
    "tpr_baseline_layers = {}\n",
    "thres_baseline_layers = {}\n",
    "auc_baseline_layers = {}\n",
    "for layer_i, layer_name in enumerate(layers):\n",
    "    y_baseline_layers_pred[layer_i] = model_cnn_base_layers[layer_i].predict(x_cnn_te[layer_i])\n",
    "    fpr_baseline_layers[layer_i], tpr_baseline_layers[layer_i], thres_baseline_layers[layer_i] = roc_curve(y_cnn_te_onehot[layer_i][:,1], y_baseline_layers_pred[layer_i][:,1])\n",
    "    auc_baseline_layers[layer_i] = auc(fpr_baseline_layers[layer_i], tpr_baseline_layers[layer_i])\n",
    "    print(\"Area under curve: {}\".format(auc_baseline_layers[layer_i]))\n",
    "#     y_flat_pred[layer_i] = models_flat[layer_i].predict(x_flat_te[layer_i])\n",
    "#     fpr_flat_nn[layer_i], tpr_flat_nn[layer_i], thres_flat_nn[layer_i] = roc_curve(y_flat_te_onehot[layer_i][:,1], y_flat_pred[layer_i][:,1])\n",
    "#     auc_flat_nn[layer_i] = auc(fpr_flat_nn[layer_i], tpr_flat_nn[layer_i])\n",
    "#     print(\"Area under curve: {}\".format(auc_flat_nn[layer_i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_i, layer_name in enumerate(layers):\n",
    "    print('Saving {}'.format(layer_name))\n",
    "    model_cnn_base_layers[layer_i].save(modelpath+\"model_{}_cnn_base.h5\".format(layer_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.cla(); plt.clf()\n",
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_lc, tpr_lc, label='LC EMProb (area = {:.3f})'.format(auc_lc))\n",
    "plt.plot(fpr_simpleCombine, tpr_simpleCombine, label='{} (area = {:.3f})'.format(\"Layers (Simple)\", auc_combined))\n",
    "plt.plot(fpr_combine, tpr_combine, label='{} (area = {:.3f})'.format(\"Layers (Deep)\", auc_combine))\n",
    "plt.plot(fpr_baseline, tpr_baseline, label='{} (area = {:.3f})'.format(\"EMB2 CNN Baseline\", auc_baseline))\n",
    "plt.plot(fpr_large, tpr_large, label='{} (area = {:.3f})'.format(\"EMB2 CNN Large\", auc_large))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('Simple NN ROC curve: classification of $\\pi^+$ vs. $\\pi^0$')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('Plots/roc_combine2_cnn.pdf')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.cla(); plt.clf()\n",
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('white')\n",
    "# Zoom in view of the upper left corner.\n",
    "plt.xlim(0, 0.25)\n",
    "plt.ylim(0.6, 1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_lc, tpr_lc, label='LC EMProb (area = {:.3f})'.format(auc_lc))\n",
    "plt.plot(fpr_simpleCombine, tpr_simpleCombine, label='{} (area = {:.3f})'.format(\"Layers (Simple)\", auc_combined))\n",
    "plt.plot(fpr_combine, tpr_combine, label='{} (area = {:.3f})'.format(\"Layers (Deep)\", auc_combine))\n",
    "plt.plot(fpr_baseline, tpr_baseline, label='{} (area = {:.3f})'.format(\"EMB2 CNN Baseline\", auc_baseline))\n",
    "plt.plot(fpr_large, tpr_large, label='{} (area = {:.3f})'.format(\"EMB2 CNN Large\", auc_large))\n",
    "# ax[1].plot(fpr_nn, tpr_nn, label='Simple NN (area = {:.3f})'.format(auc_nn))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve (zoomed in at top left)')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('Plots/roc_zoom_combine2_cnn.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_i, layer_name in enumerate(layers):\n",
    "    plt.cla(); plt.clf()\n",
    "    fig = plt.figure()\n",
    "    fig.patch.set_facecolor('white')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr_lc, tpr_lc, label='LC EMProb (area = {:.3f})'.format(auc_lc))\n",
    "    plt.plot(fpr_simpleCombine, tpr_simpleCombine, label='{} (area = {:.3f})'.format(\"Layers (Simple)\", auc_combined))\n",
    "    plt.plot(fpr_combine, tpr_combine, label='{} (area = {:.3f})'.format(\"Layers (Deep)\", auc_combine))\n",
    "    plt.plot(fpr_flat_nn[layer_i], tpr_flat_nn[layer_i], label='{} Flat (area = {:.3f})'.format(layer_name, auc_flat_nn[layer_i]))\n",
    "    plt.plot(fpr_baseline_layers[layer_i], tpr_baseline_layers[layer_i], label='{} CNN (area = {:.3f})'.format(layer_name, auc_baseline_layers[layer_i]))\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('Simple NN ROC curve: classification of $\\pi^+$ vs. $\\pi^0$')\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('Plots/roc_layers_cnn_{}.pdf'.format(layer_name))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    plt.cla(); plt.clf()\n",
    "    fig = plt.figure()\n",
    "    fig.patch.set_facecolor('white')\n",
    "    # Zoom in view of the upper left corner.\n",
    "    plt.xlim(0, 0.25)\n",
    "    plt.ylim(0.6, 1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr_lc, tpr_lc, label='LC EMProb (area = {:.3f})'.format(auc_lc))\n",
    "    plt.plot(fpr_simpleCombine, tpr_simpleCombine, label='{} (area = {:.3f})'.format(\"Layers (Simple)\", auc_combined))\n",
    "    plt.plot(fpr_combine, tpr_combine, label='{} (area = {:.3f})'.format(\"Layers (Deep)\", auc_combine))\n",
    "    plt.plot(fpr_flat_nn[layer_i], tpr_flat_nn[layer_i], label='{} Flat (area = {:.3f})'.format(layer_name, auc_flat_nn[layer_i]))\n",
    "    plt.plot(fpr_baseline_layers[layer_i], tpr_baseline_layers[layer_i], label='{} CNN (area = {:.3f})'.format(layer_name, auc_baseline_layers[layer_i]))\n",
    "    # ax[1].plot(fpr_nn, tpr_nn, label='Simple NN (area = {:.3f})'.format(auc_nn))\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve (zoomed in at top left)')\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('Plots/roc_zoom_layers_cnn_{}.pdf'.format(layer_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train multiple ConvNets on the images and merge\n",
    "\n",
    "Next, try to train convolutional nets on multiple input layers, then merge and flatten the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# Train on pT = 100 GeV electron/pion samples\n",
    "# Mswiatlo-- let's do it on pi+ vs pi0\n",
    "\n",
    "# This will allow it to be images, instead of flatenned, in emb1\n",
    "# X0 = np.append(pp['EMB1'], p0['EMB1'], axis=0)\n",
    "\n",
    "# flatten first layer (for now...) \n",
    "X0 = np.append(pp['EMB1'].reshape((len(pp['EMB1']),512)),\n",
    "              p0['EMB1'].reshape((len(p0['EMB1']),512)), axis=0)\n",
    "\n",
    "X1 = np.append(pp['EMB2'], p0['EMB2'], axis=0)\n",
    "X2 = np.append(pp['EMB3'], p0['EMB3'], axis=0)\n",
    "\n",
    "# reshape to be [samples][width][height][pixels]\n",
    "# X0 = X0.reshape(X0.shape[0], 1, 128, 4) #uncomment for images mode\n",
    "X1 = X1.reshape(X1.shape[0], 1, 16, 16)\n",
    "X2 = X2.reshape(X2.shape[0], 1, 8, 16)\n",
    "\n",
    "y = np.append(pp['label'], p0['label'], axis=0)\n",
    "\n",
    "print X0.shape\n",
    "print X1.shape\n",
    "print X2.shape\n",
    "print y.shape\n",
    "\n",
    "# split up into training and test datasets\n",
    "# test_size = 4000\n",
    "# validation_size = 1000\n",
    "test_size = 8000 # double this, mswiatlo\n",
    "validation_size = 2000 #double this, mswiatlo\n",
    "from sklearn.model_selection import train_test_split\n",
    "X0_tr, X0_te_vl, X1_tr, X1_te_vl, X2_tr, X2_te_vl, y_tr, y_te_vl, = train_test_split(X0, X1, X2, y, test_size=test_size+validation_size, random_state=1)\n",
    "X0_te, X0_vl, X1_te, X1_vl, X2_te, X2_vl, y_te, y_vl, = train_test_split(X0_te_vl, X1_te_vl, X2_te_vl, y_te_vl, test_size=validation_size, random_state=1)\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X0_tr = X0_tr / 255\n",
    "X0_te = X0_te / 255\n",
    "X0_vl = X0_vl / 255\n",
    "X1_tr = X1_tr / 255\n",
    "X1_te = X1_te / 255\n",
    "X1_vl = X1_vl / 255\n",
    "X2_tr = X2_tr / 255\n",
    "X2_te = X2_te / 255\n",
    "X2_vl = X2_vl / 255\n",
    "\n",
    "print (X0_tr.shape)\n",
    "print (y_tr.shape)\n",
    "\n",
    "# one hot encode outputs\n",
    "from keras.utils import np_utils\n",
    "y_tr_onehot = np_utils.to_categorical(y_tr, 2)\n",
    "y_te_onehot = np_utils.to_categorical(y_te, 2)\n",
    "y_vl_onehot = np_utils.to_categorical(y_vl, 2)\n",
    "num_classes = y_te_onehot.shape[1]\n",
    "print(\"y_train.shape: \",y_tr.shape)\n",
    "print(\"y_train_onehot.shape: \",y_tr_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define convolutional model for multiple input images\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "\n",
    "def merged_model_emb12():\n",
    "    \n",
    "    # EMB1 image (flat, fully-connected)\n",
    "    input1 = Input(shape=(512,), name='input1')\n",
    "    x1 = Dense(512, activation='relu')(input1)\n",
    "    x1 = Dense(256, activation='relu')(x1) \n",
    "    x1 = Dense(128, activation='relu')(x1) \n",
    "\n",
    "    # EMB2 image (convolutional)\n",
    "    input2 = Input(shape=(1,16,16), name='input2')\n",
    "    x2 = Conv2D(32, (4, 4), activation='relu')(input2)\n",
    "    x2 = MaxPooling2D(pool_size=(2, 2))(x2)\n",
    "    x2 = Dropout(0.2)(x2)\n",
    "    x2 = Flatten()(x2)\n",
    "    x2 = Dense(128, activation='relu')(x2)\n",
    "\n",
    "    # concatenate outputs from the two networks above\n",
    "    x = concatenate([x1, x2]) \n",
    "    x = Dense(50, activation='relu')(x)    \n",
    "\n",
    "    # final output\n",
    "    output = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs = [input1, input2], outputs = [output])\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "model_merged_emb12 = merged_model_emb12()\n",
    "model_merged_emb12.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit (train) the merged model\n",
    "history = model_merged_emb12.fit([X0_tr, X1_tr], [y_tr_onehot], validation_data=([X0_vl, X1_vl], [y_vl_onehot]), epochs=80, batch_size=200, verbose=2)\n",
    "\n",
    "# final evaluation of the model\n",
    "scores = model_merged_emb12.evaluate([X0_te, X1_te], [y_te_onehot], verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy and loss vs. epoch\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "plt.cla(); plt.clf()\n",
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "#plt.savefig('merged_cnn_accuracy.pdf')\n",
    "\n",
    "# summarize history for loss\n",
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "plt.show()\n",
    "#plt.savefig('merged_cnn_loss.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "y_pred = model_merged_emb12.predict([X0_te, X1_te])\n",
    "fpr_merge_emb12, tpr_merge_emb12, thres_merge = roc_curve(y_te_onehot[:,0], y_pred[:,0])\n",
    "auc_merge = auc(fpr_merge_emb12, tpr_merge_emb12)\n",
    "print(\"Area under curve: {}\".format(auc_merge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.cla(); plt.clf()\n",
    "fig, ax = plt.subplots(1, 2, tight_layout=True, figsize=(10,4))\n",
    "fig.patch.set_facecolor('white')\n",
    "ax[0].plot([0, 1], [0, 1], 'k--')\n",
    "ax[0].plot(fpr_nn, tpr_nn, label='Simple NN (area = {:.3f})'.format(auc_nn))\n",
    "ax[0].plot(fpr_merge_emb12, tpr_merge_emb12, label='Merged EMB1f + EMB2ConvNet (area = {:.3f})'.format(auc_merge))\n",
    "ax[0].set_xlabel('False positive rate')\n",
    "ax[0].set_ylabel('True positive rate')\n",
    "ax[0].set_title('ROC curve: classification of pi0 (vs. charged pions)')\n",
    "ax[0].legend(loc='best')\n",
    "\n",
    "# Zoom in view of the upper left corner.\n",
    "ax[1].set_xlim(0, 0.2)\n",
    "ax[1].set_ylim(0.8, 1)\n",
    "ax[1].plot([0, 1], [0, 1], 'k--')\n",
    "ax[1].plot(fpr_nn, tpr_nn, label='Simple NN (area = {:.3f})'.format(auc_nn))\n",
    "ax[1].plot(fpr_merge_emb12, tpr_merge_emb12, label='Merged EMB1f + EMB2 ConvNet (area = {:.3f})'.format(auc_merge))\n",
    "ax[1].set_xlabel('False positive rate')\n",
    "ax[1].set_ylabel('True positive rate')\n",
    "ax[1].set_title('ROC curve (zoomed in at top left)')\n",
    "ax[1].legend(loc='best')\n",
    "\n",
    "plt.show()\n",
    "#plt.savefig('ROC_curve_final.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next: Include all input images (calorimeter layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the filters: \n",
    " https://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats-5cc01b214e59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "\n",
    "def merged_model_emb123():\n",
    "    \n",
    "    # EMB1 image (flat, fully-connected)\n",
    "    input1 = Input(shape=(512,), name='input1')\n",
    "    x1 = Dense(512, activation='relu')(input1)\n",
    "    x1 = Dense(256, activation='relu')(x1) \n",
    "    x1 = Dense(128, activation='relu')(x1) \n",
    "\n",
    "    # EMB2 image (convolutional)\n",
    "    input2 = Input(shape=(1,16,16), name='input2')\n",
    "    x2 = Conv2D(32, (4, 4), activation='relu')(input2)\n",
    "    x2 = MaxPooling2D(pool_size=(2, 2))(x2)\n",
    "    x2 = Dropout(0.2)(x2)\n",
    "    x2 = Flatten()(x2)\n",
    "    x2 = Dense(128, activation='relu')(x2)\n",
    "    \n",
    "    # EMB3 image (convolutional)\n",
    "    input3 = Input(shape=(1,8,16), name='input3')\n",
    "    x3 = Conv2D(32, (2, 4), activation='relu')(input3)\n",
    "    x3 = MaxPooling2D(pool_size=(1, 2))(x3)\n",
    "    x3 = Dropout(0.2)(x3)\n",
    "    x3 = Flatten()(x3)\n",
    "    x3 = Dense(128, activation='relu')(x3)\n",
    "\n",
    "    # concatenate outputs from the three networks above\n",
    "    x = concatenate([x1, x2, x3]) \n",
    "    x = Dense(50, activation='relu')(x)    \n",
    "\n",
    "    # final output\n",
    "    output = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs = [input1, input2, input3], outputs = [output])\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_merged_emb123 = merged_model_emb123()\n",
    "model_merged_emb123.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit (train) the merged model\n",
    "history_emb123 = model_merged_emb123.fit([X0_tr, X1_tr, X2_tr], [y_tr_onehot], validation_data=([X0_vl, X1_vl, X2_vl], [y_vl_onehot]), epochs=80, batch_size=200, verbose=2)\n",
    "\n",
    "# final evaluation of the model\n",
    "scores_emb123 = model_merged_emb123.evaluate([X0_te, X1_te, X2_te], [y_te_onehot], verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "y_pred = model_merged_emb123.predict([X0_te, X1_te, X2_te])\n",
    "fpr_merge_emb123, tpr_merge_emb123, thres_merge = roc_curve(y_te_onehot[:,0], y_pred[:,0])\n",
    "auc_merge_emb123 = auc(fpr_merge_emb123, tpr_merge_emb123)\n",
    "print(\"Area under curve: {}\".format(auc_merge_emb123))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.cla(); plt.clf()\n",
    "fig, ax = plt.subplots(1, 2, tight_layout=True, figsize=(10,4))\n",
    "fig.patch.set_facecolor('white')\n",
    "ax[0].plot([0, 1], [0, 1], 'k--')\n",
    "ax[0].plot(fpr_nn, tpr_nn, label='Simple NN (area = {:.3f})'.format(auc_nn))\n",
    "ax[0].plot(fpr_merge_emb12, tpr_merge_emb12, label='Merged EMB1f + EMB2ConvNet (area = {:.3f})'.format(auc_merge))\n",
    "ax[0].plot(fpr_merge_emb123, tpr_merge_emb123, label='Merged EMB1f + EMB2ConvNet +EMB3ConvNet (area = {:.3f})'.format(auc_merge_emb123))\n",
    "ax[0].set_xlabel('False positive rate')\n",
    "ax[0].set_ylabel('True positive rate')\n",
    "ax[0].set_title('ROC curve: classification of pi0 (vs. charged pions)')\n",
    "ax[0].legend(loc='best')\n",
    "\n",
    "# Zoom in view of the upper left corner.\n",
    "ax[1].set_xlim(0, 0.2)\n",
    "ax[1].set_ylim(0.8, 1)\n",
    "ax[1].plot([0, 1], [0, 1], 'k--')\n",
    "ax[1].plot(fpr_nn, tpr_nn, label='Simple NN (area = {:.3f})'.format(auc_nn))\n",
    "ax[1].plot(fpr_merge_emb12, tpr_merge_emb12, label='Merged EMB1f + EMB2 ConvNet (area = {:.3f})'.format(auc_merge))\n",
    "ax[1].plot(fpr_merge_emb123, tpr_merge_emb123, label='Merged EMB1f + EMB2ConvNet +EMB3ConvNet (area = {:.3f})'.format(auc_merge_emb123))\n",
    "ax[1].set_xlabel('False positive rate')\n",
    "ax[1].set_ylabel('True positive rate')\n",
    "ax[1].set_title('ROC curve (zoomed in at top left)')\n",
    "ax[1].legend(loc='best')\n",
    "\n",
    "plt.show()\n",
    "#plt.savefig('ROC_curve_final.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Plots\n",
    "<div style=\"text-align: right\"> <a href=\"#Image-Classification-of-ATLAS-Calorimeter-Topo-Clusters-Rewrite\">Top</a> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get network scores\n",
    "combine_score = {}\n",
    "for ptype, pcell in pcells.items():\n",
    "    combine_score[ptype] = model_combine.predict(\n",
    "        [ pcell[layer] for layer in layers ]\n",
    "    )\n",
    "combine_score['piplus'][:,0] = 1 - combine_score['piplus'][:,0]\n",
    "combine_score['piminus'][:,0] = 1 - combine_score['piminus'][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "cor = {}\n",
    "test_subset = {\n",
    "    'pi0': pdata['pi0'].test,\n",
    "    'piplus': pdata['piplus'].test,\n",
    "    'piminus': [True for i in range(len(pdata['piminus']))]\n",
    "}\n",
    "\n",
    "# calculate inclusive correlations\n",
    "for ptype, pcell in pcells.items():\n",
    "    cor[ptype] = {}\n",
    "    for layer in layers:\n",
    "        cor[ptype][layer] = np.zeros(shape=(pcell[layer].shape[1],2))\n",
    "        for i in range(pcell[layer].shape[1]):\n",
    "            cor[ptype][layer][i] = scipy.stats.pearsonr(\n",
    "                pcell[layer][test_subset[ptype]][:,i],\n",
    "                combine_score[ptype][test_subset[ptype]][:,0]\n",
    "            )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colourmap generator\n",
    "def get_cmap(xmin,xmax):\n",
    "    if(xmin > 0):\n",
    "        # all positive\n",
    "        cdict = {\n",
    "            'red': ((0, 1, 1),\n",
    "                   (1, 0, 0)),\n",
    "            'green': ((0, 1, 1),\n",
    "                      (1, 0, 0)),\n",
    "            'blue': ((0, 1, 1),\n",
    "                     (1, 1, 1))\n",
    "        }\n",
    "    elif(xmax < 0):\n",
    "        # all negative\n",
    "        cdict = {\n",
    "            'red': ((0, 1, 1),\n",
    "                    (1, 1, 1)),\n",
    "            'green': ((0, 0, 0),\n",
    "                     (1, 1, 1)),\n",
    "            'blue': ((0, 0, 0),\n",
    "                     (1, 1, 1))\n",
    "        }\n",
    "    else:\n",
    "        # two-sided\n",
    "        xrange = xmax - xmin\n",
    "        white_point = -xmin/xrange\n",
    "        \n",
    "        cdict = {\n",
    "            'red': ((0.0, 1.0, 1.0),\n",
    "                    (white_point, 1.0, 1.0),\n",
    "                    (1.0, 0.0, 0.0)),\n",
    "         \n",
    "            'green': ((0.0, 0.0, 0.0),\n",
    "                      (white_point, 1.0, 1.0),\n",
    "                      (1.0, 0.0, 0.0)),\n",
    "             \n",
    "            'blue': ((0.0, 0.0, 0.0),\n",
    "                     (white_point, 1.0, 1.0),\n",
    "                     (1.0, 1.0, 1.0)),\n",
    "        }\n",
    "    return mpl.colors.LinearSegmentedColormap('rwb', cdict, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ptype in cor:\n",
    "    for layer in layers:\n",
    "        plt.cla(); plt.clf()\n",
    "        fig = plt.figure()\n",
    "        fig.patch.set_facecolor('white')\n",
    "        data = cor[ptype][layer][:,0].reshape(cell_shapes[layer])\n",
    "        data[np.isnan(data)] = 0\n",
    "        xmin = np.amin(data)\n",
    "        xmax = np.amax(data)\n",
    "        plt.imshow(data, extent=[-0.2, 0.2, -0.2, 0.2],\n",
    "                   norm=mpl.colors.SymLogNorm(linthresh=0.002, linscale=1,\n",
    "                       vmin=-1.0, vmax=1.0, base=10),\n",
    "                   cmap='RdBu', origin='lower',\n",
    "                   interpolation='nearest')\n",
    "#         plt.title('DNN Correlation for '+ptype+' events in '+layer)\n",
    "        ampl.set_xlabel(\"$\\Delta\\phi$\")\n",
    "        ampl.set_ylabel(\"$\\Delta\\eta$\")\n",
    "        ampl.draw_atlas_label(0.05, 0.95, simulation = True, fontsize = 18)\n",
    "        fig.axes[0].text(-0.18, 0.135, 'DNN Correlation for '+pi_latex[ptype]+' events')\n",
    "        fig.axes[0].text(-0.18, 0.105, layer)\n",
    "        cbar = plt.colorbar()\n",
    "        cbar.set_label('Pearson correlation', rotation=270, labelpad=20)\n",
    "        plt.savefig(plotpath+'cor_'+ptype+'_'+layer+'.pdf')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "energy_bins = [0, 1, 5, 20, 100, 500]\n",
    "cor_binned = {}\n",
    "test_subset = {\n",
    "    'pi0': pdata['pi0'].test,\n",
    "    'piplus': pdata['piplus'].test,\n",
    "    'piminus': [True for i in range(len(pdata['piminus']))]\n",
    "}\n",
    "\n",
    "# calculate and plot energy-binned correlations\n",
    "for ptype, pcell in pcells.items():\n",
    "    cor_binned[ptype] = {}\n",
    "    for layer in layers:\n",
    "        cor_binned[ptype][layer] = []\n",
    "        for ibin in range(len(energy_bins)):\n",
    "            if( ibin == len(energy_bins)-1 ):\n",
    "                sel = pdata[ptype]['clusterE'] > energy_bins[ibin]\n",
    "                label = 'Energy / GeV > '+str(energy_bins[ibin])\n",
    "            else:        \n",
    "                sel = (pdata[ptype]['clusterE'] > energy_bins[ibin]) & (pdata[ptype]['clusterE'] > energy_bins[ibin+1])\n",
    "                label = str(energy_bins[ibin])+' < Energy / GeV < '+str(energy_bins[ibin+1])\n",
    "            \n",
    "            cor_binned[ptype][layer].append(\n",
    "                np.zeros(shape=(pcell[layer].shape[1],2))\n",
    "            )\n",
    "            for pixel in range(pcell[layer].shape[1]):\n",
    "                cor_binned[ptype][layer][ibin][pixel] = scipy.stats.pearsonr(\n",
    "                    pcell[layer][test_subset[ptype] & sel][:,pixel],\n",
    "                    combine_score[ptype][test_subset[ptype] & sel][:,0]\n",
    "                )\n",
    "            \n",
    "            plt.cla(); plt.clf()\n",
    "            fig = plt.figure()\n",
    "            fig.patch.set_facecolor('white')\n",
    "            \n",
    "            data = cor_binned[ptype][layer][ibin][:,0].reshape(cell_shapes[layer])\n",
    "            data[np.isnan(data)] = 0\n",
    "            xmin = np.amin(data)\n",
    "            xmax = np.amax(data)\n",
    "            plt.imshow(data,\n",
    "                       extent=[-0.2, 0.2, -0.2, 0.2],\n",
    "                       norm=mpl.colors.SymLogNorm(linthresh=0.002, linscale=1,\n",
    "                       vmin=-1.0, vmax=1.0, base=10),\n",
    "                       cmap='RdBu', origin='lower',\n",
    "                       interpolation='nearest')\n",
    "\n",
    "#             plt.title('DNN Correlation for '+ptype+' events in '+layer+', '+label)\n",
    "            ampl.set_xlabel(\"$\\Delta\\phi$\")\n",
    "            ampl.set_ylabel(\"$\\Delta\\eta$\")\n",
    "            ampl.draw_atlas_label(0.05, 0.95, simulation = True, fontsize = 18)\n",
    "            fig.axes[0].text(-0.18, 0.135, 'DNN Correlation for '+pi_latex[ptype]+' events')\n",
    "            fig.axes[0].text(-0.18, 0.105, layer+', '+label)\n",
    "            cbar = plt.colorbar()\n",
    "            cbar.set_label('Pearson correlation', rotation=270, labelpad=20)\n",
    "            plt.savefig(plotpath+'cor_'+ptype+'_'+layer+'_clusterE'+str(ibin)+'.pdf')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml4p]",
   "language": "python",
   "name": "conda-env-ml4p-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
