{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy Evaluation\n",
    "\n",
    "This notebook just evaluates our classification + energy regressions on our jet data. The same processes are done in `JetClustering.ipynb`, but here we leave out all the stuff with jet clustering (which is only interesting once we have good performance on the topo-clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag for skipping file preparation.\n",
    "skip_scores = False\n",
    "\n",
    "# Debug: Uses only one input file, which will speed things up.\n",
    "debug = False\n",
    "\n",
    "# classification threshold -- above = charged, below = neutral\n",
    "classification_threshold = 0.6\n",
    "\n",
    "# select our network source -- we have sets of networks trained in different ways\n",
    "source = 'pion_reweighted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports - generic stuff\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "import ROOT as rt\n",
    "import uproot as ur # uproot for accessing ROOT files quickly (and in a Pythonic way)\n",
    "import sys, os, glob, uuid # glob for searching for files, uuid for random strings to name ROOT objects and avoid collisions\n",
    "import subprocess as sub\n",
    "from numba import jit\n",
    "from pathlib import Path\n",
    "from IPython.utils import io # For suppressing some print statements from functions.\n",
    "\n",
    "path_prefix = os.getcwd() + '/../'\n",
    "if(path_prefix not in sys.path): sys.path.append(path_prefix)\n",
    "from util import ml_util as mu # for passing calo images to regression networks\n",
    "from util import qol_util as qu # for progress bar\n",
    "from util import jet_util as ju"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display our plots, let's get a dark style that will look nice in presentations (and JupyterLab in dark mode).\n",
    "dark_style = qu.PlotStyle('dark')\n",
    "light_style = qu.PlotStyle('light')\n",
    "plot_style = dark_style\n",
    "plot_style.SetStyle() # sets style for plots - still need to adjust legends, paves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import `tensorflow` (and some of its `keras` stuff), as well as some stuff from `sklearn` for neural network I/O scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and setup for TensorFlow and Keras.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # disable some of the tensorflow info printouts, only display errors\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ngpu = 1\n",
    "gpu_list = [\"/gpu:\"+str(i) for i in range(ngpu)]\n",
    "strategy = tf.distribute.MirroredStrategy(devices=gpu_list)\n",
    "ngpu = strategy.num_replicas_in_sync\n",
    "print ('Number of devices: {}'.format(ngpu))\n",
    "\n",
    "# Dictionary for storing all our neural network models that will be evaluated\n",
    "network_models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up a whole bunch of paths. We use `source = pion` by default, whereby we use networks trained on the single-pion data. We can alternatively use `source = jet` to use our \"facsimile jet training data\" -- a subset of jet data where we have tried to match topo-clusters to pions -- but this is not fully implemented yet. For example, this workflow explicitly re-derives the network scalers using the `pion` data, so to use the `jet` data we will have to modify that code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(source == 'pion' or source == 'jet'):\n",
    "    classification_dir = path_prefix + 'classifier/Models/' + source\n",
    "    regression_dir = path_prefix + 'regression/Models/' + source\n",
    "    \n",
    "elif(source == 'pion_reweighted'):\n",
    "    classification_dir = path_prefix + 'classifier/Models/' + 'pion'\n",
    "    regression_dir = path_prefix + 'regression/Models/' + source\n",
    "\n",
    "data_dir = path_prefix + 'data/jet'\n",
    "training_data_dir = path_prefix + 'data/pion' # TODO: deal with situation where source is jet\n",
    "fj_dir = path_prefix + '/setup/fastjet/fastjet-install/lib/python3.8/site-packages'\n",
    "plot_dir = path_prefix + 'jets/Plots/' + source\n",
    "\n",
    "try: os.makedirs(plot_dir)\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the Plot directory if it does not exist yet\n",
    "if(not os.path.exists(plot_dir)): os.makedirs(plot_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Calorimeter meta-data -----\n",
    "layers = [\"EMB1\", \"EMB2\", \"EMB3\", \"TileBar0\", \"TileBar1\", \"TileBar2\"]\n",
    "nlayers = len(layers)\n",
    "cell_size_phi = [0.098, 0.0245, 0.0245, 0.1, 0.1, 0.1]\n",
    "cell_size_eta = [0.0031, 0.025, 0.05, 0.1, 0.1, 0.2]\n",
    "len_phi = [4, 16, 16, 4, 4, 4]\n",
    "len_eta = [128, 16, 8, 4, 4, 2]\n",
    "assert(len(len_phi) == nlayers)\n",
    "assert(len(len_eta) == nlayers)\n",
    "meta_data = {\n",
    "    layers[i]:{\n",
    "        'cell_size':(cell_size_eta[i],cell_size_phi[i]),\n",
    "        'dimensions':(len_eta[i],len_phi[i])\n",
    "    }\n",
    "    for i in range(nlayers)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat classifiers\n",
    "print('Loading flat classification models... ')\n",
    "flat_model_files = glob.glob(classification_dir + '/flat/' + '*.h5')\n",
    "flat_model_files.sort()\n",
    "flat_model_names = []\n",
    "for model in flat_model_files:\n",
    "    model_name = model.split('model_')[-1].split('_flat')[0]\n",
    "    print('\\tLoading ' + model_name + '... ',end='')\n",
    "    flat_model_names.append(model_name)\n",
    "    network_models[model_name] = tf.keras.models.load_model(model)\n",
    "    print('Done.')\n",
    "\n",
    "# combo classifier\n",
    "print('Loading simple combo classification model... ',end='')\n",
    "combo_model_file = classification_dir + '/simple/' + 'model_simple_do20.h5'\n",
    "network_models['combo'] = tf.keras.models.load_model(combo_model_file)\n",
    "print('Done.')\n",
    "\n",
    "# energy regression networks\n",
    "print('Loading charged-pion energy regression model... ',end='')\n",
    "charged_energy_model_file = regression_dir + '/' + 'all_charged.h5'\n",
    "network_models['e_charged'] = tf.keras.models.load_model(charged_energy_model_file)\n",
    "print('Done.')\n",
    "\n",
    "print('Loading neutral-pion energy regression model... ',end='')\n",
    "neutral_energy_model_file = regression_dir + '/' + 'all_neutral.h5'\n",
    "network_models['e_neutral'] = tf.keras.models.load_model(neutral_energy_model_file)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make a \"local\" copy of the jet data. We will only copy over certain branches, and we will skip any files that don't contain an `eventTree` in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our \"local\" data dir, where we create modified data files\n",
    "jet_data_dir = path_prefix + 'jets/cluster_data/' + source\n",
    "Path(jet_data_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if(skip_scores):\n",
    "    data_filenames = glob.glob(jet_data_dir + '/*.root')\n",
    "    \n",
    "    # debugging - take only one file, for speed\n",
    "    if(debug): data_filenames = [data_filenames[0]]\n",
    "    \n",
    "else:\n",
    "    data_filenames = glob.glob(data_dir + '/' + '*.root')\n",
    "\n",
    "    # debugging - lets us use a single file to speed stuff up a lot.\n",
    "    if(debug): data_filenames = [data_dir + '/' + 'user.angerami.21685345.OutputStream._000062.root']\n",
    "\n",
    "    # Get the original data.\n",
    "    files = {name:rt.TFile(name,'READ') for name in data_filenames}\n",
    "\n",
    "    # Some data files might be missing an EventTree.\n",
    "    # For now, we will skip these because our methods count on an existing EventTree.\n",
    "    delete_keys = []\n",
    "    for key, val in files.items():\n",
    "        file_keys = [x.GetName() for x in val.GetListOfKeys()]\n",
    "        if('ClusterTree' not in file_keys or 'EventTree' not in file_keys):\n",
    "            delete_keys.append(key)\n",
    "\n",
    "    for key in delete_keys: \n",
    "        print('Ignoring file:',key,'(no EventTree/ClusterTree found).')\n",
    "        del files[key]\n",
    "\n",
    "    # now we make a local copy of the files in the jet_data_dir, keeping only certain branches\n",
    "    active_branches = {}\n",
    "    active_branches['cluster'] = [\n",
    "        'runNumber',\n",
    "        'eventNumber',\n",
    "        'truthE',\n",
    "        'truthPt',\n",
    "        'truthEta',\n",
    "        'truthPhi',\n",
    "        'clusterIndex',\n",
    "        'nCluster',\n",
    "        'clusterE',\n",
    "        'clusterECalib',\n",
    "        'clusterPt',\n",
    "        'clusterEta',\n",
    "        'clusterPhi',\n",
    "        'cluster_nCells',\n",
    "        'cluster_ENG_CALIB_TOT',\n",
    "        'EMB1',\n",
    "        'EMB2',\n",
    "        'EMB3',\n",
    "        'TileBar0',\n",
    "        'TileBar1',\n",
    "        'TileBar2'\n",
    "    ]\n",
    "\n",
    "    tree_names = {'cluster':'ClusterTree'}\n",
    "    data_filenames = []\n",
    "\n",
    "    l = len(files.keys())\n",
    "    i = 0\n",
    "    qu.printProgressBarColor(i, l, prefix='Copying data files:', suffix='Complete', length=50)\n",
    "\n",
    "    for path, tfile in files.items():\n",
    "        filename_new = jet_data_dir + '/' + path.split('/')[-1]\n",
    "        old_trees = {x:tfile.Get(tree_names[x]) for x in tree_names.keys()}\n",
    "    \n",
    "        for key, tree in old_trees.items():\n",
    "            tree.SetBranchStatus('*',0)\n",
    "            for bname in active_branches[key]: tree.SetBranchStatus(bname,1)\n",
    "    \n",
    "        tfile_new = rt.TFile(filename_new,'RECREATE')\n",
    "        new_trees = {x:old_trees[x].CloneTree() for x in old_trees.keys()}\n",
    "        tfile_new.Write()\n",
    "        data_filenames.append(filename_new)\n",
    "        i += 1\n",
    "        qu.printProgressBarColor(i, l, prefix='Copying data files:', suffix='Complete', length=50)\n",
    "        del old_trees\n",
    "        del new_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the files & trees with uproot\n",
    "tree_names = {'cluster':'ClusterTree'}\n",
    "ur_trees = {file:{tree_key:ur.open(file)[tree_name] for tree_key,tree_name in tree_names.items()} for file in data_filenames}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides our models and the data, we also need the *scalers* associated with the regression models. We will apply these to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib as jl\n",
    "\n",
    "# fetch the scalers associated with the regression models\n",
    "scaler_file = 'scalers.save'\n",
    "scalers = jl.load(regression_dir + '/' + scaler_file)\n",
    "\n",
    "scaler_e = scalers['e']\n",
    "scaler_cal = scalers['cal']\n",
    "scaler_eta = scalers['eta']\n",
    "\n",
    "# do some renaming of the scalers' keys\n",
    "for scaler in [scaler_e, scaler_cal, scaler_eta]:\n",
    "    scaler['charged'] = scaler['pp']\n",
    "    del scaler['pp']\n",
    "    scaler['neutral'] = scaler['p0']\n",
    "    del scaler['p0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting network outputs for all clusters\n",
    "\n",
    "Now we will loop over our data files, and get network scores (classification and predicted energies) for all clusters. Note that the latter involves *scaling* of the data, which we will achieve using the scalers that we extracted from the training data above.\n",
    "\n",
    "This isn't the most notebook-esque code, as we're preparing a bunch of inputs *within* the big for loop below (and not saving them or printing them) but it should avoid \"out of memory\" issues: As we are dealing with a large amount of data, preparing all the data in memory before operating on it will result in very high memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# branch buffer for filling our score trees\n",
    "    # make our branch buffer\n",
    "branch_buffer = {\n",
    "    'charged_likelihood_combo': np.zeros(1,dtype=np.dtype('f8')),\n",
    "    'clusterE_charged': np.zeros(1,dtype=np.dtype('f8')),\n",
    "    'clusterE_neutral': np.zeros(1,dtype=np.dtype('f8'))\n",
    "}\n",
    "\n",
    "# Name for the tree that will contain network scores.\n",
    "tree_name = 'ScoreTree'\n",
    "\n",
    "for dfile, trees in ur_trees.items():\n",
    "    \n",
    "    if(skip_scores): \n",
    "        # Explicitly check if ScoreTree is present, otherwise we recompute.\n",
    "        # Useful if score computation was previously interrupted.\n",
    "        file_keys = [str(x,'utf-8') for x in list(ur.open(dfile).keys())]\n",
    "        skip = [tree_name in fkey for fkey in file_keys]\n",
    "        if(True in skip): continue    \n",
    "        \n",
    "    print ('File:',dfile)\n",
    "    # Prepare the calo images.\n",
    "    print('\\tPrepping calo images...')\n",
    "    calo_images = {}\n",
    "    for layer in layers:\n",
    "        calo_images[layer] = mu.setupCells(trees['cluster'],layer)\n",
    "    combined_images = np.concatenate(tuple([calo_images[layer] for layer in layers]), axis=1)\n",
    "\n",
    "    # Prepare some extra combined input for the energy regressions.\n",
    "    print('\\tPrepping extra inputs...')\n",
    "    \n",
    "    e = trees['cluster'].array('clusterE')\n",
    "    e_calib = trees['cluster'].array('cluster_ENG_CALIB_TOT')\n",
    "    eta = trees['cluster'].array('clusterEta')\n",
    "    \n",
    "    # cleaning for e_calib (empirically needed for e_calib to remove values that are too small)\n",
    "    #energy_cut = 5.0e-1 # GeV # TODO: this may or may not match the cut used in regression training\n",
    "    epsilon = 1.0e-12 #1.0e-12 # TODO: Should I set this to energy_cut? Would that make sense?\n",
    "    e_calib = np.where(e_calib < epsilon, epsilon, e_calib)\n",
    "    \n",
    "    s_combined,scaler_combined = mu.standardCells(combined_images, layers) # Note: scaler_combined is unused\n",
    "    \n",
    "    regression_input = {}\n",
    "    for key in scaler_e.keys():\n",
    "        regression_cols = {}\n",
    "        regression_cols['s_logE'] = scaler_e[key].transform(np.log(e).reshape(-1,1))\n",
    "        regression_cols['s_eta'] = scaler_eta[key].transform(eta.reshape(-1,1))\n",
    "        regression_input[key] = np.column_stack((regression_cols['s_logE'], regression_cols['s_eta'],s_combined))\n",
    "\n",
    "    # now find network scores\n",
    "    print('\\tCalculating network outputs...')\n",
    "    model_scores = {}\n",
    "    \n",
    "    print('\\t\\tClassification... ', end='')\n",
    "    # 1) flat networks\n",
    "    for layer in flat_model_names:\n",
    "        model = network_models[layer]\n",
    "        model_scores[layer] = model.predict(calo_images[layer])[:,1] # [:,1] based on Max's code, this is input to combo network. Likelihood of being charged (vs. neutral)\n",
    "    \n",
    "    # 2) combo network\n",
    "    name = 'combo'\n",
    "    model = network_models[name]\n",
    "    input_scores = np.column_stack([model_scores[layer] for layer in layers])\n",
    "    model_scores[name] = model.predict(input_scores)[:,1] # likelihood of being charged pion (versus neutral pion)\n",
    "    print('Done.')\n",
    "    \n",
    "    print('\\t\\tRegression... ', end='')\n",
    "    # 3) energy regression networks\n",
    "    name = 'e_charged'\n",
    "    model = network_models[name]\n",
    "    model_scores[name] = np.exp(scaler_cal['charged'].inverse_transform(model.predict(regression_input['charged'])))\n",
    "    \n",
    "    name = 'e_neutral'\n",
    "    model = network_models[name]\n",
    "    model_scores[name] = np.exp(scaler_cal['neutral'].inverse_transform(model.predict(regression_input['neutral'])))\n",
    "    print('Done.')\n",
    "    \n",
    "    # Now we should save these scores to a new tree.\n",
    "    f = rt.TFile(dfile, 'UPDATE')\n",
    "    t = rt.TTree(tree_name, tree_name)\n",
    "    \n",
    "    print('Saving network scores to tree ' + tree_name + '... ',end='')    \n",
    "    # --- Setup the branches using our buffer. This is a rather general/flexible code block. ---\n",
    "    branches = {}\n",
    "    for bname, val in branch_buffer.items():\n",
    "        descriptor = bname\n",
    "        bshape = val.shape\n",
    "        if(bshape != (1,)):\n",
    "            for i in range(len(bshape)):\n",
    "                descriptor += '[' + str(bshape[i]) + ']'\n",
    "        descriptor += '/'\n",
    "        if(val.dtype == np.dtype('i2')): descriptor += 'S'\n",
    "        elif(val.dtype == np.dtype('i4')): descriptor += 'I'\n",
    "        elif(val.dtype == np.dtype('i8')): descriptor += 'L'\n",
    "        elif(val.dtype == np.dtype('f4')): descriptor += 'F'\n",
    "        elif(val.dtype == np.dtype('f8')): descriptor += 'D'\n",
    "        else:\n",
    "            print('Warning, setup issue for branch: ', key, '. Skipping.')\n",
    "            continue\n",
    "        branches[bname] = t.Branch(bname,val,descriptor)\n",
    "    \n",
    "    # Fill the model score tree, and save it to the local data file.\n",
    "    nentries = model_scores['combo'].shape[0]\n",
    "    for i in range(nentries):\n",
    "        branch_buffer['charged_likelihood_combo'][0] = model_scores['combo'][i]\n",
    "        branch_buffer['clusterE_charged'][0] = model_scores['e_charged'][i]\n",
    "        branch_buffer['clusterE_neutral'][0] = model_scores['e_neutral'][i]\n",
    "        t.Fill()\n",
    "    \n",
    "    t.Write(tree_name, rt.TObject.kOverwrite)\n",
    "    f.Close()\n",
    "    print('Done.')\n",
    "    \n",
    "tree_names['score'] = tree_name\n",
    "ur_trees = {file:{tree_key:ur.open(file)[tree_name] for tree_key,tree_name in tree_names.items()} for file in data_filenames}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on to jet clustering, we can already check to see if our energy regressions seem sensible. Let's make distributions of:\n",
    "- The classification score\n",
    "- Each regressed energy, for **all** clusters (i.e. charged and neutral energy regressions for all clusters regardless of their classifications)\n",
    "- Regressed energy / reco energy, where we choose the regressed energy for each cluster based on its classification score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.5\n",
    "\n",
    "c = rt.TCanvas(str(uuid.uuid4()),'network checks',800,1800)\n",
    "c.Divide(1,4)\n",
    "\n",
    "# classification scores\n",
    "class_hist = rt.TH1F(str(uuid.uuid4()), 'Classification score (charged likelihood);Score;Count',100,0.,1.)\n",
    "for dfile, trees in ur_trees.items():\n",
    "    for score in ur_trees[dfile]['score'].array('charged_likelihood_combo'): class_hist.Fill(score)\n",
    "class_hist.SetFillColorAlpha(rt.kGreen,alpha)\n",
    "class_hist.SetLineColorAlpha(rt.kGreen,alpha)\n",
    "c.cd(1)\n",
    "class_hist.Draw('HIST')\n",
    "rt.gPad.SetLogy()\n",
    "\n",
    "# energy regressions - for all clusters (score-agnostic)\n",
    "\n",
    "true_e_hist = rt.TH1F(str(uuid.uuid4()), 'E_{CALIB}^{TOT} (all clusters);E_{CALIB}^{TOT} [GeV];Count',60,0.,60.)\n",
    "charged_hist = rt.TH1F(str(uuid.uuid4()), 'Predicted Charged Energy (all clusters);Energy [GeV];Count',60,0.,60.)\n",
    "neutral_hist = rt.TH1F(str(uuid.uuid4()), 'Predicted Neutral Energy (all clusters);Energy [GeV];Count',60,0.,60.)\n",
    "\n",
    "for dfile, trees in ur_trees.items():\n",
    "    for energy in ur_trees[dfile]['cluster'].array('cluster_ENG_CALIB_TOT'): true_e_hist.Fill(energy)\n",
    "    for energy in ur_trees[dfile]['score'].array('clusterE_charged'): charged_hist.Fill(energy)\n",
    "    for energy in ur_trees[dfile]['score'].array('clusterE_neutral'): neutral_hist.Fill(energy)\n",
    "\n",
    "true_e_hist.SetFillColorAlpha(rt.kGreen,alpha)\n",
    "true_e_hist.SetLineColorAlpha(rt.kGreen,alpha)  \n",
    "\n",
    "charged_hist.SetFillColorAlpha(rt.kBlue,1.5 * alpha)\n",
    "charged_hist.SetLineColorAlpha(rt.kBlue,1.5 * alpha)  \n",
    "\n",
    "neutral_hist.SetFillColorAlpha(rt.kRed,1.5 * alpha)\n",
    "neutral_hist.SetLineColorAlpha(rt.kRed,1.5 * alpha) \n",
    "\n",
    "c.cd(2)\n",
    "leg1 = rt.TLegend(0.7,0.6,0.9,0.9)\n",
    "leg1.SetTextSize(0.06)\n",
    "leg1.SetTextColor(plot_style.text)\n",
    "leg1.AddEntry(charged_hist,'E_{pred}^{#pm}','f')\n",
    "leg1.AddEntry(true_e_hist,'E_{CALIB}^{TOT}','f')\n",
    "\n",
    "charged_hist.Draw('HIST')\n",
    "true_e_hist.Draw('HIST SAME')\n",
    "leg1.Draw()\n",
    "rt.gPad.SetLogy()\n",
    "\n",
    "c.cd(3)\n",
    "leg2 = rt.TLegend(0.7,0.6,0.9,0.9)\n",
    "leg2.SetTextSize(0.06)\n",
    "leg2.SetTextColor(plot_style.text)\n",
    "leg2.AddEntry(neutral_hist,'E_{pred}^{0}','f')\n",
    "leg2.AddEntry(true_e_hist,'E_{CALIB}^{TOT}','f')\n",
    "\n",
    "neutral_hist.Draw('HIST')\n",
    "true_e_hist.Draw('HIST SAME')\n",
    "leg2.Draw()\n",
    "rt.gPad.SetLogy()\n",
    "\n",
    "# regressed energy (most likely) / calibration hits\n",
    "zero_energies = 0\n",
    "n_tot = 0\n",
    "energy_ratio_hist = rt.TH1F(str(uuid.uuid4()), 'Predicted Energy / E_{CALIB}^{TOT};E_{pred} / E_{CALIB}^{TOT};Count',10000,1.0e-3,1.0e2)\n",
    "for dfile, trees in ur_trees.items():\n",
    "    scores = trees['score'].array('charged_likelihood_combo')\n",
    "    charged_e = trees['score'].array('clusterE_charged')\n",
    "    neutral_e = trees['score'].array('clusterE_neutral')\n",
    "    true_e = trees['cluster'].array('cluster_ENG_CALIB_TOT')\n",
    "    n_tot += len(true_e)\n",
    "    \n",
    "    for i in range(len(scores)):\n",
    "        if(true_e[i] == 0.):\n",
    "            zero_energies += 1\n",
    "            continue\n",
    "        if(scores[i] > 0.5): energy_ratio_hist.Fill(charged_e[i] / true_e[i])\n",
    "        else: energy_ratio_hist.Fill(neutral_e[i] / true_e[i])\n",
    "\n",
    "energy_ratio_hist.SetFillColorAlpha(rt.kViolet-6,1.)\n",
    "energy_ratio_hist.SetLineColorAlpha(rt.kViolet-6,1.)\n",
    "c.cd(4)\n",
    "energy_ratio_hist.Draw('HIST')\n",
    "rt.gPad.SetLogx()\n",
    "rt.gPad.SetLogy()\n",
    "energy_ratio_hist.GetXaxis().SetRangeUser(1.0e-4, 1.0e2)\n",
    "\n",
    "rt.gStyle.SetOptStat(0)\n",
    "\n",
    "print('Number of clusters with ENG_CALIB_TOT == 0: {val1:.2e} ({val2:.2f}% of clusters)'.format(val1 = zero_energies, val2 = 100. * zero_energies / n_tot))\n",
    "\n",
    "c.SaveAs(plot_dir + '/' + 'cluster_plots.png')\n",
    "c.Draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots above, we see that things are *OK* in that our predicted energies fall within the same range as the targeted ones. However, as the differences between energy distributions (and the last plot, in purple) show, there are an appreciable number of events where our predicted energy is off by and order of magnitude or so. Note that, for the last plot, we're using a classification threshold of $0.5$ (the point in the classifier output where we make the split between topo-clusters classified as charged or neutral). There may be a more optimal choice of cut, but I think it's unlikely that small tweaks to this will totally eliminate the issue we see here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
