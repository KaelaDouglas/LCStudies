{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jet Clustering using \"Smart Topo-Clusters\"\n",
    "\n",
    "The big idea is to use neural networks for classification and/or energy calibration of topo-clusters, and use these topo-clusters for making jets. In this notebook I'll be playing around with some ideas for this, to see what works.\n",
    "\n",
    "In this notebook we will *not* be training neural networks. That's taken care of by other notebooks in the `/classifier` and `/regression` directories of this repo. We will instead be applying the existing, trained networks to some data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Setup\n",
    "\n",
    "First, let's import a bunch of packages we know we'll need right off-the-bat.\n",
    "\n",
    "Note that as we've set up our environment with `conda`, our `ROOT` installation has all the bells and whistles. This includes the `pythia8` library and its associated `ROOT` wrapper, `TPythia8`. We can optionally use this for jet-clustering, as it comes `fj-core`.\n",
    "Alternatively we could use the Pythonic interface for `fastjet` or [pyjet](https://github.com/scikit-hep/pyjet), but the latter requires linking an external fastjet build for speed and this doesn't seem to work when following their documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.22/02\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ROOT as rt\n",
    "import uproot as ur\n",
    "#import pandas as pd\n",
    "import sys, glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's also some slightly contrived setup for `latex`. We may need this for the `atlas_mpl_style` package, which is employed in some of Max's plotting utilities that we may want to borrow. Since `latex` isn't set up on the [UChicago ML platform](https://ml.maniac.uchicago.edu) by default, our setup script may install it separately but it's still not on `$PATH` since we don't touch our bash profile. This cell uses some `IPython` magic to adjust `$PATH` for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/opt/conda/envs/ml4p/bin:/opt/conda/condabin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/texlive/2020/bin/x86_64-linux\n"
     ]
    }
   ],
   "source": [
    "# Check if latex is set up already.\n",
    "# We use some Jupyter magic -- alternatively one could use python's subprocess here.\n",
    "has_latex = !command -v latex\n",
    "has_latex = (not has_latex == [])\n",
    "\n",
    "# If latex was not a recognized command, our setup script should have installed\n",
    "# at a fixed location, but it is not on the $PATH. Now let's use some Jupyter magic.\n",
    "# See https://ipython.readthedocs.io/en/stable/interactive/shell.html for info.\n",
    "if(not has_latex):\n",
    "    latex_prefix = '/usr/local/texlive/2020/bin/x86_64-linux'\n",
    "    jupyter_env = %env\n",
    "    path = jupyter_env['PATH']\n",
    "    path = path + ':' + latex_prefix\n",
    "    %env PATH = $path\n",
    "    jupyter_env = %env\n",
    "    path = jupyter_env['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some extra setup\n",
    "path_prefix = '/workspace/LCStudies/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Fetching the data\n",
    "\n",
    "Now we get our data. For now, our classifiers are being trained to distinguish between $\\pi^+$ and $\\pi^0$. Assuming that all charged pions behave the same way, we can really treat this as a $\\pi^\\pm$ vs. $\\pi^0$ classifier. **For our toy workflow, we'll say that we only want to cluster $\\pi^\\pm$ topo-clusters into jets.** We will treat $\\pi^0$ as a background.\n",
    "\n",
    "For our input data, we have `ROOT` files containing a tree called `ClusterTree`. In each tree, each entry corresponds with one topo-cluster, and the different files correspond with different topo-cluster parent particles (e.g. $3$ files for $\\pi^+$,$\\pi^-$ and $\\pi^0$). Each topo-cluster entry contains information on the event from which it came (\"runNumber\" and \"eventNumber\"), and many topo-clusters (within and across files) share the same event. Our ultimate goal is to regroup this data into one file where each entry corresponds with one *event*. This is a sensible way to arrange the data before performing any jet clustering (which is performed by event), and writing to a file will allow us to skip this whole process after doing it once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(path_prefix not in sys.path): sys.path.append(path_prefix)\n",
    "from  util import ml_util as mu\n",
    "\n",
    "# ----- Meta-data for our dataset -----\n",
    "layers = [\"EMB1\", \"EMB2\", \"EMB3\", \"TileBar0\", \"TileBar1\", \"TileBar2\"]\n",
    "nlayers = len(layers)\n",
    "cell_size_phi = [0.098, 0.0245, 0.0245, 0.1, 0.1, 0.1]\n",
    "cell_size_eta = [0.0031, 0.025, 0.05, 0.1, 0.1, 0.2]\n",
    "len_phi = [4, 16, 16, 4, 4, 4]\n",
    "len_eta = [128, 16, 8, 4, 4, 2]\n",
    "assert(len(len_phi) == nlayers)\n",
    "assert(len(len_eta) == nlayers)\n",
    "meta_data = {\n",
    "    layers[i]:{\n",
    "        'cell_size':(cell_size_eta[i],cell_size_phi[i]),\n",
    "        'dimensions':(len_eta[i],len_phi[i])\n",
    "    }\n",
    "    for i in range(nlayers)\n",
    "}\n",
    "# -------------------------------------\n",
    "\n",
    "# We open the files using uproot, and use our ml_util to get the images.\n",
    "data_dir = '/workspace/LCStudies/data'\n",
    "data_files = glob.glob(data_dir + '/*.root')\n",
    "data_files = {x.split('/')[-1].replace('.root',''):x for x in data_files}\n",
    "tree_name = 'ClusterTree'\n",
    "trees = {key: ur.open(file)[tree_name] for key, file in data_files.items()}\n",
    "\n",
    "pcells = {\n",
    "    ifile : {\n",
    "        layer : mu.setupCells(itree, layer)\n",
    "        for layer in layers\n",
    "    }\n",
    "    for ifile, itree in trees.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Applying the classifier network\n",
    "\n",
    "Our first step is to get the network scores for each topo-cluster. We will place these in \"friend trees\" of our input trees. (To be more precise, we will chain together the input trees, and the trees we make here, and then make these $2$ `TChain`s friends).\n",
    "\n",
    "To do this, let's import some `tensorflow` and `keras` stuff that we'll need for applying our trained networks to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # disable some of the tensorflow info printouts, only display errors\n",
    "ngpu = 1\n",
    "gpu_list = [\"/gpu:\"+str(i) for i in range(ngpu)]\n",
    "import tensorflow as tf\n",
    "strategy = tf.distribute.MirroredStrategy(devices=gpu_list)\n",
    "ngpu = strategy.num_replicas_in_sync\n",
    "print ('Number of devices: {}'.format(ngpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EMB1, EMB2, EMB3, TileBar0, TileBar1, TileBar2.\n",
      "Calculating network scores: |\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m| 100.0% Complete\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from  util import qol_util as qu\n",
    "modelpath = path_prefix + 'classifier/Models'\n",
    "model_postfix = '_flat_do20' # for our simple, per-layer NN's\n",
    "models = {}\n",
    "model_history = {}\n",
    "model_scores = {}\n",
    "\n",
    "i = 0\n",
    "for layer in layers:\n",
    "    if(i == 0): print('Loading ',end='')\n",
    "    print(layer,end='')\n",
    "    if(i!= len(layers)-1): print(', ',end='')\n",
    "    else: print('.')\n",
    "    i += 1\n",
    "    \n",
    "    models[layer] = tf.keras.models.load_model(modelpath+'/model_' + layer + model_postfix + '.h5')\n",
    "    # Load history object.\n",
    "    with open(modelpath + '/model_' + layer + model_postfix + '.history','rb') as model_history_file:\n",
    "        model_history[layer] = pickle.load(model_history_file)\n",
    "    \n",
    "# Recalculate network scores for the datasets.\n",
    "prefix = 'Calculating network scores:'\n",
    "l = len(pcells) * len(layers)\n",
    "qu.printProgressBar(0, l, prefix=prefix, suffix='Complete', length=50)\n",
    "i = 0\n",
    "for key in pcells.keys():\n",
    "    \n",
    "    model_scores[key] = {}\n",
    "    for layer in layers: \n",
    "        model_scores[key][layer] = models[layer].predict(pcells[key][layer])\n",
    "        i += 1\n",
    "        qu.printProgressBarColor(i, l, prefix=prefix, suffix='Complete', length=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each event, the corresponding `model_scores` entry is a tuple.\n",
    "The first entry is the \"background score\" -- how likely the cluster is to be a $\\pi^0$. The second is the \"signal score\" -- how likely the cluster is to be a $\\pi^\\pm$. At least this seems to be the correct interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Saving network scores and signal flags to clusters\n",
    "\n",
    "Now that we have our network scores for all our clusters, we need to group our clusters by event -- in a new file. As each entry will correspond with one event, most of our scalar branches will now become arrays, listing properties for each cluster in the event. We could try to be fancy and make these branches of C++-type `std::vector` instead, but as some are multi-dim arrays we would end up with vectors of C++-style arrays and handling these via the `PyROOT` interface seems problematic/tough to me.\n",
    "\n",
    "\n",
    "Note that besides the signal flag and network scores, we're also adding a \"file index\". This is a simple constant branch that keeps the entries from different files distinct. It could, in principle, let us distinguish underlying truth particle flavors beyond just \"signal\" and \"background\" (e.g. distinguish between $\\pi^+$ and $\\pi^-$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling friend tree for pi0:     |\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m| 100.0% Complete\n",
      "Filling friend tree for piminus: |\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m| 100.0% Complete\n",
      "Filling friend tree for piplus:  |\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m| 100.0% Complete\n"
     ]
    }
   ],
   "source": [
    "sig_definition = {'signal':['piminus','piplus'],'background':['pi0']}\n",
    "\n",
    "from  util import qol_util as qu\n",
    "from pathlib import Path\n",
    "jet_data_dir = path_prefix + 'jets/data'\n",
    "Path(jet_data_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get our original data files.\n",
    "files = {key:rt.TFile(file,'READ') for key, file in data_files.items()}\n",
    "trees = {key:file.Get(tree_name) for key, file in files.items()}\n",
    "\n",
    "# Now we want to effectively add some new columns. We accomplish this with \"friend\" trees.\n",
    "# We're not actually making these trees friends yet. Instead we will form TChains and friend those.\n",
    "\n",
    "# Creating our branch buffer.\n",
    "data = {\n",
    "    'signal':np.zeros(1,dtype=np.dtype('i2')),\n",
    "    'file_index'   :np.zeros(1,dtype=np.dtype('i2')) # TODO: Is this needed any longer?\n",
    "}\n",
    "for layer in layers:\n",
    "    bname = layer + '_NN'\n",
    "    data[bname] = np.zeros(2,dtype=np.dtype('f8'))\n",
    "\n",
    "friend_tree_name = tree_name + '_friend'\n",
    "friend_data_files = {}\n",
    "\n",
    "file_index = 0\n",
    "stride = 200\n",
    "\n",
    "for key in sorted(trees.keys()):\n",
    "    \n",
    "    friend_filename = data_files[key].split('/')[-1]\n",
    "    friend_filename = jet_data_dir + '/' + friend_filename\n",
    "    friend_file = rt.TFile(friend_filename,'RECREATE')\n",
    "    friend_data_files[key] = friend_filename\n",
    "    \n",
    "    friend_tree = rt.TTree(friend_tree_name,friend_tree_name)\n",
    "    branches = {}\n",
    "\n",
    "    # --- Setup the branches using our buffer. This is a rather general/flexible code block. ---\n",
    "    for bname, val in data.items():\n",
    "        descriptor = bname\n",
    "        bshape = val.shape\n",
    "        if(bshape != (1,)):\n",
    "            for i in range(len(bshape)):\n",
    "                descriptor += '[' + str(bshape[i]) + ']'\n",
    "        descriptor += '/'\n",
    "        if(val.dtype == np.dtype('i2')): descriptor += 'S'\n",
    "        elif(val.dtype == np.dtype('i4')): descriptor += 'I'\n",
    "        elif(val.dtype == np.dtype('i8')): descriptor += 'L'\n",
    "        elif(val.dtype == np.dtype('f4')): descriptor += 'F'\n",
    "        elif(val.dtype == np.dtype('f8')): descriptor += 'D'\n",
    "        else:\n",
    "            print('Warning, setup issue for branch: ', key, '. Skipping.')\n",
    "            continue\n",
    "        branches[key] = friend_tree.Branch(bname,val,descriptor)\n",
    "    # --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\n",
    "    \n",
    "    # Now we fill the friend tree.\n",
    "    nentries = trees[key].GetEntries()\n",
    "    for layer in layers: assert len(model_scores[key][layer]) == nentries\n",
    "\n",
    "    # Signal flag and file index will be constant since input trees are divided by particle identity.\n",
    "    sig = 0\n",
    "    if(key in sig_definition['signal']): sig = 1\n",
    "    nn_scores = model_scores[key]\n",
    "    \n",
    "    prefix = 'Filling friend tree for ' + key + ':'\n",
    "    if(len(prefix) < 32): prefix = prefix + ' ' * (32 - len(prefix))\n",
    "    qu.printProgressBar(0, int(nentries/100), prefix=prefix, suffix='Complete', length=50)\n",
    "    \n",
    "    for i in range(nentries):\n",
    "        data['signal'][0] = sig\n",
    "        for layer in layers: data[layer + '_NN'][:] = nn_scores[layer][i,:]\n",
    "        friend_tree.Fill()\n",
    "        if(i%stride ==0): qu.printProgressBarColor(i/stride, int(nentries/stride), prefix=prefix, suffix='Complete', length=50)\n",
    "    \n",
    "    friend_tree.Write()\n",
    "    friend_file.Close()\n",
    "    file_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Merging topo-cluster data, writing to a file and creating an eventNumber index\n",
    "\n",
    "Our trees containing the signal flag and network scores are now saved to disk. Now let's make `TChain`s and make them friends, so that we've effectively tacked on new columns to our original data. We will then save these `TChain`s as a `TTree` to an uncompressed file, and use the `TTreeIndex` functionality to sort our events. We do this conversion & saving because it appears to greatly speed up our reading when using the `TTreeIndex`. I assume this has to do something with the entries -- from both the main chain and its friend -- all being saved in the same file as opposed to being scattered across multiple ones. ([See here](https://root-forum.cern.ch/t/usage-of-tchainindex/19074/4) for a discussion of `TChainIndex` versus `TTreeIndex`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = rt.TChain(tree_name)\n",
    "friend_chain = rt.TChain(friend_tree_name)\n",
    "for key in data_files.keys(): \n",
    "    chain.AddFile(data_files[key],-1)\n",
    "    friend_chain.AddFile(friend_data_files[key],-1)\n",
    "    \n",
    "chain_filename = jet_data_dir + '/' + 'clusters.root'\n",
    "chain_file = rt.TFile(chain_filename,'RECREATE','',0) # uncompressed file\n",
    "clone = chain.CloneTree(-1,'FAST')\n",
    "friend_clone = friend_chain.CloneTree(-1,'FAST')\n",
    "clone.Write()\n",
    "friend_clone.Write()\n",
    "chain_file.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chains are now friends.\n"
     ]
    }
   ],
   "source": [
    "chain_file = rt.TFile(chain_filename,'READ')\n",
    "chain = chain_file.Get(tree_name)\n",
    "friend_chain = chain_file.Get(friend_tree_name)\n",
    "assert(chain.GetEntries() == friend_chain.GetEntries()) # number of entries must match, otherwise something has gone very wrong\n",
    "nentries = chain.GetEntries()\n",
    "chain.AddFriend(friend_chain)\n",
    "print('The chains are now friends.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Preparing to create an event TTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, one of the branches of the input data is of type `std::vector<Float_t>`, while all the others are either scalars or fixed-length arrays (sometimes with an effective variable length encoded in another branch). Let's see what the length of this vector branch looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drawing: |\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m| 100.0% Complete\n",
      "Max length is 786.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Investigating the cluster_cellE_norm branch, which is the only vector branch (variable-length). Why?\n",
    "histo = rt.TH1I('h0','len(cluster_cellE_norm)',250,0,1000)\n",
    "nentries = chain.GetEntriesFast()\n",
    "stride = 200\n",
    "qu.printProgressBar(0, int(nentries/stride), prefix='Drawing:', suffix='Complete', length=50)\n",
    "chain.SetBranchStatus('*',0)\n",
    "chain.SetBranchStatus('cluster_cellE_norm',1)\n",
    "for i in range(nentries):\n",
    "    chain.GetEntry(i)\n",
    "    histo.Fill(len(chain.cluster_cellE_norm))\n",
    "    if(i%stride == 0): qu.printProgressBarColor(i/stride, int(nentries/stride), prefix='Drawing:', suffix='Complete', length=50)\n",
    "\n",
    "canv = rt.TCanvas('c1,','c1',800,600)\n",
    "histo.Draw('HIST')\n",
    "canv.SetLogy()\n",
    "histo.SetMinimum(0.1)\n",
    "canv.Draw()\n",
    "\n",
    "max_length = int(histo.GetXaxis().GetBinCenter(histo.FindLastBinAbove(0)))\n",
    "print('Max length is ' + str(max_length) + '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a function to quickly get the maximum length of a vector branch in our tree. We only have one vector branch for now but there may be others with a future dataset.\n",
    "\n",
    "This will let us turn the branch from one of type vector to one of type array. It's useful if we're adding an extra dimension, as I'm currently having issues with making branches like vectors of arrays on the Python side of things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetMaxVectorLength(chain, branchname):\n",
    "    draw_string = branchname + '@.size()'\n",
    "    chain.Draw(draw_string)\n",
    "    h = rt.gPad.GetPrimitive('htemp') # some slightly idiomatic ROOT stuff, one of the few examples of weird default behavior\n",
    "    max_length = int(h.GetXaxis().GetBinCenter(h.FindLastBinAbove(0)))\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create our `TTreeIndex`. We will use the branch `eventNumber` as our `majornumber`, so that the index effectively sorts our tree by `eventNumber` and we have our events grouped together. Since there are many duplicate `eventNumber` entries, and we do not use any `minornumber`, this is *not* a unique index. But this is fine, because we do not care about the sorting within any single `eventNumber` value. It just means that we *cannot* access every single entry with a call to `TTreeIndex::GetEntryNumberWithIndex()`, but rather we'll have to loop through the elements of `TTreeIndex:GetIndex()` sequentially.\n",
    "\n",
    "**TODO:** As a consequence of our indexing, we do not protect against the possibility of two clusters having the same `eventNumber` but different `runNumber`s. We should add this at some point to avoid the possibility of mixing clusters from what are really separate events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting minornumber to 0 effectively gets rid of it\n",
    "chain.SetBranchStatus('*',0)\n",
    "chain.SetBranchStatus('eventNumber',1)\n",
    "chain_idx = rt.TTreeIndex(chain,'eventNumber','0') # TODO: consider changing to majornumber=runNumber, minorNumber=eventNumber. In this particular case runNumber is always the same.\n",
    "n_idx = chain_idx.GetN()\n",
    "assert(n_idx == chain.GetEntriesFast()) # ensure our TTreeIndex is of the right length, otherwise something is wrong\n",
    "chain_indices = chain_idx.GetIndex() # a C++-style array of (ROOT) type Long64_t...\n",
    "chain_indices = np.array([chain_indices[i] for i in range(n_idx)],dtype=np.dtype('i8')) #... now a numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine the maximum number of topo-clusters per event. We can set a naïve upper bound with nfiles * max(nCluster), but the actual upper bound is probably lower than this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding max nClusters |████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% Complete\n",
      "max(nClusters) per event = 34\n"
     ]
    }
   ],
   "source": [
    "chain.SetBranchStatus('*',0)\n",
    "chain.SetBranchStatus('eventNumber',1)\n",
    "chain.SetBranchStatus('nCluster',1)\n",
    "\n",
    "n_clusters_max = 0\n",
    "max_tmp = 0\n",
    "\n",
    "chain.GetEntry(0)\n",
    "eN_prev = chain.eventNumber\n",
    "\n",
    "nentries = chain.GetEntries()\n",
    "stride = int(nentries/100)\n",
    "l = int(nentries/stride)\n",
    "bar_length = 120\n",
    "prefix = 'Finding max nClusters'\n",
    "qu.printProgressBar(0, l, prefix=prefix, suffix='Complete', length=bar_length)\n",
    "for i in range(nentries):\n",
    "    idx = chain_indices[i]\n",
    "    chain.GetEntry(idx)\n",
    "    if(chain.eventNumber != eN_prev):\n",
    "        if(max_tmp > n_clusters_max): n_clusters_max = max_tmp\n",
    "        max_tmp = 0\n",
    "    max_tmp += 1\n",
    "    eN_prev = chain.eventNumber\n",
    "    if(i%stride == 0): qu.printProgressBar(int(i/stride), l, prefix=prefix, suffix='Complete', length=bar_length)\n",
    "    \n",
    "print('max(nClusters) per event =',n_clusters_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to copy our data to a new `ROOT` file, where each entry corresponds with an **event**. Our `chain_indices` lets us loop through the existing data in a sensible way.\n",
    "\n",
    "Certain variables are *per-event* variables, such as `runNumber`. These will remain as scalars. Other variables are *per-cluster* variables, such as `clusterE` (scalar) or `EMB1` (2D vector). These will become *arrays* of whatever their previous type was. The branch `nCluster` will keep track of their length for each event. Note that this means we be rewriting the contents of the `nCluster` branch, rather than copying it over -- it currently only keeps track of the number of clusters per event per file, not the total number of clusters per event.\n",
    "\n",
    "As one last note, we will have to be a little careful about looping through our `TChain` of input trees for the sake of speed. We're using a `TTreeIndex` that we built, but this will amount to hopping around a lot (reading entries in a very non-sequential order w.r.t. the chain/trees). [This can slow things down with a lot of file I/0](https://root-forum.cern.ch/t/ttree-getentry-with-a-ttreeindex-is-too-slow/17370/5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting from ROOT type names to leaflist decorators.\n",
    "# Vector decorator will not work, but gives a sensible string\n",
    "# telling us the depth (how many vectors).\n",
    "def RTypeConversion(type_string):\n",
    "    if(type_string == 'Short_t' or type_string == 'short'):    return 'S'\n",
    "    elif(type_string == 'Int_t' or type_string == 'int'):    return 'I'\n",
    "    elif(type_string == 'Float_t' or type_string == 'float'):  return 'F'\n",
    "    elif(type_string == 'Double_t' or type_string == 'double'): return 'D'\n",
    "    elif('vector' in type_string): # special case\n",
    "#         type_substring = '<'.join(type_string.split('<')[1:])\n",
    "#         type_substring = '>'.join(type_substring.split('>')[:-1])\n",
    "#         type_substring = RTypeConversion(type_substring)\n",
    "#         return 'v_' + type_substring\n",
    "        return type_string\n",
    "    else: return '?'\n",
    "\n",
    "def GetShape(shape_string):\n",
    "    dims = shape_string.replace('[',' ').replace(']', ' ').split()\n",
    "    return tuple([int(x) for x in dims])\n",
    "\n",
    "def RType2NType(type_string):\n",
    "    if(type_string == 'S'):   return np.dtype('i2')\n",
    "    elif(type_string == 'I'): return np.dtype('i4')\n",
    "    elif(type_string == 'L'): return np.dtype('i8')\n",
    "    elif(type_string == 'F'): return np.dtype('f4')\n",
    "    elif(type_string == 'D'): return np.dtype('f8')\n",
    "    else: raise ValueError('Input not understood.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly turn on all branches.\n",
    "chain.SetBranchStatus('*',1)\n",
    "friend_chain.SetBranchStatus('*',1)\n",
    "\n",
    "# n_clusters_max = int(len(data_files.keys()) * chain.GetMaximum('nCluster')) # safe upper limit, but might be unnecessarily high\n",
    "\n",
    "# Building our branch buffer for our new trees. This time we'll add leaflists to the buffer as well.\n",
    "# Slightly hacky but this should be pretty flexible for any basic-type branches.\n",
    "\n",
    "branch_info = [x.GetListOfLeaves()[0] for x in chain.GetListOfBranches()]\n",
    "branch_info = [(x.GetTitle(),x.GetTypeName()) for x in branch_info]\n",
    "branch_names = [x[0].split('[')[0] for x in branch_info]\n",
    "\n",
    "friend_branch_info = [x.GetListOfLeaves()[0] for x in friend_chain.GetListOfBranches()]\n",
    "friend_branch_info = [(x.GetTitle(),x.GetTypeName()) for x in friend_branch_info]\n",
    "friend_branch_names = [x[0].split('[')[0] for x in friend_branch_info]\n",
    "\n",
    "branch_names = branch_names + friend_branch_names\n",
    "branch_info = branch_info + friend_branch_info\n",
    "\n",
    "# Now let's consider removing some branches that we don't think we'll need for our event dataset.\n",
    "# This can potentially speed things up a lot. Especially true for branches of type std::vector at the moment,\n",
    "# as I am probably not handling them in the smartest way.\n",
    "branch_names_remove = ['cluster_cellE_norm']\n",
    "\n",
    "rtypes_forced = {'nCluster':'S'}\n",
    "\n",
    "perEvent = ['runNumber','eventNumber','nCluster','file_index'] # keep track of which branches only need one entry per event\n",
    "vector_branches = [] # keep track of any branches that are of (C++) type std::vector\n",
    "\n",
    "# We must also keep track of the original shapes of any array branches that we read, as they will be read out as 1D cppy arrays\n",
    "# and will need to be reshaped before being placed in our buffer.\n",
    "input_shapes = {}\n",
    "\n",
    "branch_buffer = {}\n",
    "for entry in branch_info: \n",
    "    name = entry[0]\n",
    "    shape = (1,)\n",
    "    shape_string = ''\n",
    "    if('[' in name): \n",
    "        shape_string = '[' + '['.join(name.split('[')[1:])\n",
    "        shape = GetShape(shape_string)\n",
    "    name = name.split('[')[0]\n",
    "    \n",
    "    rtype = RTypeConversion(entry[1])    \n",
    "    if(name in rtypes_forced.keys()): rtype = rtypes_forced[key]\n",
    "    \n",
    "    if(name in branch_names_remove): continue\n",
    "    \n",
    "#     report = 'Setting up branch ' + str(name)\n",
    "#     report_length = len(report)\n",
    "#     if(report_length < 50):\n",
    "#         report += (50-report_length) * ' '\n",
    "#     report2 = 'Original shape is ' + str(shape)\n",
    "#     if(len(report2)) < 30:\n",
    "#         report2 += (30-len(report2)) * ' '\n",
    "#     print(report,report2,end='')\n",
    "    \n",
    "    # save the original shapes of non-scalar branches\n",
    "    if(shape != (1,)): input_shapes[name] = shape # save the original shape\n",
    "\n",
    "    if(name not in perEvent):\n",
    "        if(shape == (1,)): \n",
    "            shape = (n_clusters_max,)\n",
    "            shape_string = '[' + 'nCluster' + ']'\n",
    "        else:\n",
    "            shape2 = [n_clusters_max]\n",
    "            for x in shape: shape2.append(x)\n",
    "            shape = tuple(shape2)\n",
    "            shape_string = '[' + 'nCluster' + ']' + shape_string\n",
    "      \n",
    "#         print('->\\t',shape)\n",
    "    if('vector' in rtype):\n",
    "        continue\n",
    "        # We will make the vector into an array. Assuming vector is of some basic type! (not vector of vectors, etc.)\n",
    "        rsubtype = '<'.join(rtype.split('<')[1:])\n",
    "        rsubtype = '>'.join(rsubtype.split('>')[:-1])\n",
    "        rtype = RTypeConversion(rsubtype)\n",
    "        n_max = GetMaxVectorLength(chain,name)\n",
    "        input_shapes[name]=(n_max,)\n",
    "        shape = tuple(list(shape) + [n_max])\n",
    "        shape_string += '[' + str(n_max) + ']'\n",
    "        vector_branches.append(name)\n",
    "        #branch_buffer[name] = [rt.vector(rsubtype)(),0]\n",
    "        #TODO: Add a branch for the vector length\n",
    "        \n",
    "    branch_buffer[name] = [np.zeros(shape,dtype=RType2NType(rtype)),name + shape_string + '/' + rtype]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Creating event TTree\n",
    "\n",
    "Now we're really ready to make our event TTree -- each entry will correspond with a full event, and hold all of its topo-clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to write 80000 events, corresponding to 343128 input topo-clusters.\n"
     ]
    }
   ],
   "source": [
    "# Determining how many events to write. By default we want them all, but for debugging we might only want some subset.\n",
    "nevents = 80000\n",
    "nentries = chain_indices.shape[0]\n",
    "if(nevents > 0):\n",
    "    # determine how many entries we need to get this many events\n",
    "    chain.SetBranchStatus('*',0)\n",
    "    chain.SetBranchStatus('eventNumber',1)\n",
    "    nevents_tally = 0\n",
    "    chain.GetEntry(chain_indices[0])\n",
    "    eN_prev = chain.eventNumber\n",
    "    for i in range(nentries):\n",
    "        chain.GetEntry(chain_indices[i])\n",
    "        if(chain.eventNumber != eN_prev): nevents_tally += 1\n",
    "        eN_prev = chain.eventNumber\n",
    "        if(nevents_tally == nevents):\n",
    "            nentries = i-1 # don't include the event we've just started\n",
    "            break\n",
    "            \n",
    "if(nevents <= 0): nevents = 'all'\n",
    "report_string = 'Preparing to write {nev} events, corresponding to {nen} input topo-clusters.'.format(nev=nevents,nen=nentries)\n",
    "print(report_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employing strategy 1 (load full input tree into memory).\n",
      "Employing strategy 2 (increasing basket size for input tree).\n"
     ]
    }
   ],
   "source": [
    "# Activate the branches we need, keep any ignored ones deactivated\n",
    "chain.SetBranchStatus('*',1)\n",
    "for name in branch_names_remove: chain.SetBranchStatus(name,0)\n",
    "    \n",
    "# Strategies for speeding things up more. \n",
    "strategy = 3\n",
    "\n",
    "if(strategy == 1 or strategy == 3):\n",
    "    filesize = chain_file.GetSize() # in bytes\n",
    "    chain.SetMaxVirtualSize(int(1.5 * filesize))\n",
    "    for name in branch_names:\n",
    "        if(name not in branch_names_remove): chain.GetBranch(name).LoadBaskets()\n",
    "    print('Employing strategy 1 (load full input tree into memory).')\n",
    "\n",
    "if(strategy == 2 or strategy == 3):\n",
    "    # Increasing basket_size will help a little. But because entry access is somewhat random,\n",
    "    # with each event's clusters generally spread out into multiple small groups of adjacent entries,\n",
    "    # performance will quickly plateau and big increases may not give any real gains.\n",
    "    basket_size_multiplier = 3\n",
    "    basket_size = 16000 * basket_size_multiplier\n",
    "    for name in branch_names:\n",
    "        if(name in branch_names_remove): continue\n",
    "        if(name in friend_branch_names): friend_chain.SetBasketSize(name,basket_size)\n",
    "        else: chain.SetBasketSize(name,basket_size)\n",
    "    print('Employing strategy 2 (increasing basket size for input tree).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're *really* ready to write our event tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing event tree: |\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m| 100.0% Complete\n",
      "345.9 seconds. (input data rate = 992.0 Hz)\n"
     ]
    }
   ],
   "source": [
    "#TODO: Deal with read/write bugging out at 99% when trying to write the full dataset. Works fine for subsets.\n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "Path(jet_data_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Make the new TFile and TTree.\n",
    "event_filename = path_prefix + 'jets/data' + '/' + 'events.root'\n",
    "event_treename = 'events'\n",
    "event_file = rt.TFile(event_filename,'RECREATE','',0) # TODO: Making this uncompressed for debugging purposes\n",
    "event_tree = rt.TTree(event_treename,event_treename)\n",
    "#event_tree.SetDirectory(0) # tree will start in memory!\n",
    "\n",
    "# Set up the branches. Note that we must add branches specifying lengths *before*\n",
    "# any branches whose lengths they specify. For now, that means nCluster must go first.\n",
    "# TODO: Make this less hacky.\n",
    "name = 'nCluster'\n",
    "assert(name in branch_buffer.keys())\n",
    "buffer   = branch_buffer[name][0]\n",
    "leaflist = branch_buffer[name][1]\n",
    "event_tree.Branch(name,buffer,leaflist)\n",
    "\n",
    "for key, value in branch_buffer.items():\n",
    "    if(key == 'nCluster'): continue\n",
    "    name = key\n",
    "    buffer = value[0]\n",
    "    leaflist = value[-1]\n",
    "    if(leaflist == 0): event_tree.Branch(name,value[0])\n",
    "    else: event_tree.Branch(name,buffer,leaflist)\n",
    "\n",
    "chain.GetEntry(chain_indices[0])\n",
    "eN_prev = chain.eventNumber\n",
    "cluster_idx = 0\n",
    "\n",
    "stride = int(nentries/100)\n",
    "l = int(nentries/stride)\n",
    "bar_length = 120\n",
    "prefix = 'Writing event tree:'\n",
    "qu.printProgressBar(0, l, prefix=prefix, suffix='Complete', length=bar_length)\n",
    "\n",
    "start = time.time()\n",
    "# dt = np.zeros(5)\n",
    "# Now loop through the input chain and write to our new tree.\n",
    "for i in range(nentries):\n",
    "\n",
    "#     t0 = time.time()\n",
    "    chain.GetEntry(chain_indices[i])\n",
    "    eN = chain.eventNumber\n",
    "#     dt[0] += time.time() - t0\n",
    "    \n",
    "    if(eN != eN_prev):\n",
    "        # We've finished an event and have just entered a new one.\n",
    "        # Write everything that's in the buffer. Corresponds to the previous event.\n",
    "#         t0 = time.time()\n",
    "        event_tree.Fill()\n",
    "        cluster_idx = 0 # reset cluster_idx\n",
    "#         dt[1] += time.time() - t0\n",
    "    \n",
    "    # Fill info from the current event.\n",
    "    for name in branch_buffer.keys():\n",
    "        shape = branch_buffer[name][0].shape\n",
    "        \n",
    "        # Our per-event branches\n",
    "        if(shape == (1,)):\n",
    "#             t0 = time.time()\n",
    "            if(name == 'nCluster'): branch_buffer[name][0][0] = cluster_idx + 1 # will reach max value before write\n",
    "            else: branch_buffer[name][0][0] = getattr(chain,name) # a more C-like way of getting branches\n",
    "#             dt[2] += time.time() - t0\n",
    "        # Our per-cluster branches\n",
    "        else:\n",
    "            ndim = branch_buffer[name][0].ndim\n",
    "            if(ndim == 1): \n",
    "#                 t0 = time.time()\n",
    "                branch_buffer[name][0][cluster_idx] = getattr(chain,name) # per-cluster scalar\n",
    "#                 dt[3] += time.time() - t0\n",
    "            else:\n",
    "                # Multi-dim branches.\n",
    "                # Wrapping with a numpy array works, \n",
    "                # but we must perform the right reshaping.\n",
    "                # std::vector-type branches have variable length - must be careful with reshaping.\n",
    "#                 t0 = time.time()\n",
    "#                 if(name in vector_branches): # Our one vector branch seems to be a culprit for slowdowns.\n",
    "#                     n = len(getattr(chain,name))\n",
    "#                     branch_buffer[name][0][cluster_idx,:n] = np.array(getattr(chain,name))[:]\n",
    "#                     continue\n",
    "                branch_buffer[name][0][cluster_idx,:] = np.array(getattr(chain,name)).reshape(input_shapes[name])[:] # per-cluster array\n",
    "#                 dt[4] += time.time() - t0\n",
    "    \n",
    "    cluster_idx += 1\n",
    "    eN_prev = eN\n",
    "    if(i%stride == 0): qu.printProgressBarColor(int(i/stride), l, prefix=prefix, suffix='Complete', length=bar_length)\n",
    "    if(i == nentries-1): # Make sure to call a Fill() if we're at the end of the chain\n",
    "        event_tree.Fill()\n",
    "        #qu.printProgressBar(l, l, prefix=prefix, suffix='Complete', length=bar_length)\n",
    "    \n",
    "event_file.cd()\n",
    "event_tree.Write('',rt.TObject.kOverwrite)\n",
    "# event_file.Write(event_treename,rt.TObject.kOverwrite)\n",
    "event_file.Close()\n",
    "\n",
    "end = time.time()\n",
    "dt_tot = end - start\n",
    "rate = (nentries / dt_tot)\n",
    "print('{val:.1f} seconds. (input data rate = {rate:.1f} Hz)'.format(val=dt_tot,rate=rate))\n",
    "\n",
    "# dt_names = ['GetEntry         ',\n",
    "#             'Write            ',\n",
    "#             'perEvent scalar  ',\n",
    "#             'perCluster scalar',\n",
    "#             'perCluster vector']\n",
    "\n",
    "# for i in range(len(dt)):\n",
    "#     print(dt_names[i] + ': {val:.1f}'.format(val=dt[i]) + ' ({val:.1f})%'.format(val=100. * dt[i] / dt_tot))\n",
    "\n",
    "# diff = dt_tot - np.sum(dt)\n",
    "# print('{val:.1f}'.format(val=100. * diff / dt_tot) + '% of time unaccounted for.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above method is still a little slow -- from a few runs I estimate it takes about $12$ - $15$ minutes to write our `events.root` file, with most of the slowdown having to do with reading entries from `cluster.root` in a non-sequential fashion (this would be much slower had we not exported the contents of our `TChain` to that file). Using our \"strategy \\#1\" from the previous cell really speeds things up, as does turning off the vector branch that we're not using.\n",
    "\n",
    "Note that our current problem does not translate well to multi-threading or parallelization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabulating some trials from writing events. There are some fluctuations from run to run.\n",
    "```\n",
    "5k  events -- 21929  clusters --  14.6 seconds -- 1499.3 Hz\n",
    "10k events -- 43386  clusters --    54 seconds -- 803.1 Hz\n",
    "20k events -- 86586  clusters -- 116.8 seconds -- 741.1 Hz\n",
    "40k events -- 172869 clusters -- 171.3 seconds -- 1009.4 Hz\n",
    "80k events -- 343128 clusters -- 345.9 seconds -- 992.0 Hz\n",
    "```\n",
    "\n",
    "**Note**: We originally had the output TTree (event tree) in memory first. But this caused an issue with nevents $\\geq 80\\text{k}$:\n",
    "```\n",
    "bad_alloc: void TFile::Close(const char* option = \"\") =>\n",
    "    bad_alloc: std::bad_alloc\n",
    "Error in <TBufferFile::WriteByteCount>: bytecount too large (more than 1073741822)\n",
    "Error in <TBufferFile::WriteByteCount>: bytecount too large (more than 1073741822)\n",
    "Error in <TList::Clear>: A list is accessing an object (0x7ffd1b959cc0) already deleted (list name = TList)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Jet clustering\n",
    "\n",
    "\n",
    "Now that we have things grouped by event, we should cluster the topo-clusters in each event into jets.\n",
    "\n",
    "We have a few possible ways of doing this:\n",
    "\n",
    "- `fastjet`\n",
    "    - We can use the Pythonic interface. It might be fast, however it takes Python lists of `fastjet.Pseudojet` objects as inputs to clustering and I'm not sure if building these will slow us down or not. Documentation is not very good.\n",
    "    \n",
    "- `pyjet`\n",
    "    - The 3rd-party interface between `fastjet` and `numpy`. Seems elegant but setup with external `fastjet` -- needed for the fastest clustering -- doesn't seem to work. Instructions are outdated and the project hasn't been updated in nearly a year.\n",
    "    \n",
    "- `TPythia8`\n",
    "    - Though it's kind of a hack, our `ROOT` installation from conda includes `pythia8` + `TPythia8`, which gives access to its `SlowJet` object. Despite the name this actually employs `fastjet` core as of a few versions ago so it's fast. It takes `Pythia8.event` objects as input, but we can artificially construct these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Python interface to the FastJet jet clustering package.  \n",
      "\n",
      "Usage is similar to the C++ case, with a few small changes noted\n",
      "below.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "\n",
      "- You can pass a python list such as [PseudoJet0, PseudoJet1, ...]\n",
      "  to any FastJet call that expects a vector of PseudoJets\n",
      "\n",
      "- Any FastJet call that in C++ returns a vector of PseudoJets will in\n",
      "  python return a tuple (or list) of PseudoJets\n",
      "\n",
      "- for many objects that provide definitions of some kind, __str__\n",
      "  call maps to description(). So, for example, you can just do\n",
      "\n",
      "       jet_def = fastjet.JetDefinition(fastjet.antikt_algorithm, 0.4)\n",
      "       print jet_def\n",
      "\n",
      "- for combinations of selectors, (&&, || and !) in C++ map to \n",
      "  (&, | and ~) in python\n",
      "\n",
      "- Selector::pass is remapped to Selector._pass\n",
      "\n",
      "- remember that python uses reference, e.g. a = b means that a is a\n",
      "  reference to b. If you need to copy a PseudoJet (pj), with a view to\n",
      "  altering it, do 'pjcopy = PseudoJet(pj)'\n",
      "\n",
      "- the python documentation has been automatically generated from the\n",
      "  C++ doxygen documentation: python/C++ differences are not indicated,\n",
      "  and certain methods and classes may be documented that were not\n",
      "  included in the python conversion and/or configured for this\n",
      "  particular installation.\n",
      "\n",
      "Example\n",
      "-------\n",
      "\n",
      "  from fastjet import *\n",
      "  particles = []\n",
      "  particles.append(PseudoJet(100.0, 0.0, 0.0, 100.0)) # px, py, pz, E\n",
      "  particles.append(PseudoJet(150.0, 0.0, 0.0, 150.0))\n",
      "\n",
      "  R = 0.4\n",
      "  jet_def = JetDefinition(antikt_algorithm, R)\n",
      "\n",
      "  jets = jet_def(particles)\n",
      "\n",
      "  print jet_def\n",
      "  for jet in jets: print jet\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#importing fastjet Python library, which should be made by our setup script\n",
    "fj_dir = path_prefix + '/setup/fastjet/fastjet-install/lib/python3.8/site-packages'\n",
    "sys.path.append(fj_dir)\n",
    "import fastjet as fj\n",
    "\n",
    "print(fj.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing jet clustering: |\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m| 100.0% Complete\n"
     ]
    }
   ],
   "source": [
    "# Jet clustering params\n",
    "R = 0.4\n",
    "jet_def = fj.JetDefinition(fj.antikt_algorithm, R)\n",
    "event_jets = []\n",
    "\n",
    "# Open our events file, and perform jet clustering using fastjet\n",
    "f = rt.TFile(jet_data_dir + '/' + 'events.root','read')\n",
    "t = f.Get('events')\n",
    "\n",
    "# only activate the branches we need for clustering\n",
    "tbranches = t.GetListOfBranches()\n",
    "active_branches = ['nCluster','clusterPt', 'clusterEta','clusterPhi','clusterE']\n",
    "t.SetBranchStatus('*',0)\n",
    "for branch in active_branches: t.SetBranchStatus(branch,1)\n",
    "\n",
    "vec_polar = rt.Math.PtEtaPhiEVector()\n",
    "nevents = t.GetEntriesFast()\n",
    "\n",
    "stride = 1000\n",
    "l = int(nevents/stride)\n",
    "bar_length = 50\n",
    "prefix = 'Performing jet clustering:'\n",
    "qu.printProgressBarColor(0, l, prefix=prefix, suffix='Complete', length=bar_length)\n",
    "\n",
    "for i in range(nevents):\n",
    "    t.GetEntry(i)\n",
    "    \n",
    "    nCluster = t.nCluster\n",
    "    particles = nCluster * [fj.PseudoJet(0.,0.,0.,0.)]\n",
    "    for j in range(nCluster):\n",
    "        vec_polar.SetCoordinates(t.clusterPt[j],t.clusterEta[j],t.clusterPhi[j],t.clusterE[j])\n",
    "        pj = fj.PseudoJet(vec_polar.Px(), vec_polar.Py(), vec_polar.Pz(), vec_polar.E()) # fastjet uses Cartesian\n",
    "        particles[j] = pj\n",
    "    event_jets.append(jet_def(particles))\n",
    "    \n",
    "    if(i%stride == 0): qu.printProgressBarColor(int(i/stride), l, prefix=prefix, suffix='Complete', length=bar_length)\n",
    "qu.printProgressBarColor(l, l, prefix=prefix, suffix='Complete', length=bar_length)\n",
    "\n",
    "f.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing jets to file: |\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m| 100.0% Complete\n"
     ]
    }
   ],
   "source": [
    "# Now make a TTree containing the jet info.\n",
    "njets_max = np.max(np.array([len(x) for x in event_jets],dtype=np.dtype('i2')))\n",
    "\n",
    "f = rt.TFile(jet_data_dir + '/' + 'events.root','update')\n",
    "t = rt.TTree('jets','jets')\n",
    "\n",
    "branch_buffer = {\n",
    "    'nJet': [np.zeros(1,dtype=np.dtype('i2')), 'nJet/S'],\n",
    "    'jetPt': [np.zeros(njets_max,dtype=np.dtype('f8')), 'jetPt[nJet]/D'],\n",
    "    'jetEta': [np.zeros(njets_max,dtype=np.dtype('f8')), 'jetEta[nJet]/D'],\n",
    "    'jetPhi': [np.zeros(njets_max,dtype=np.dtype('f8')), 'jetPhia[nJet]/D'],\n",
    "    'jetE': [np.zeros(njets_max,dtype=np.dtype('f8')), 'jetE[nJet]/D']\n",
    "}\n",
    "for key,val in branch_buffer.items(): t.Branch(key,val[0],val[1])\n",
    "stride = 1000\n",
    "l = int(nevents/stride)\n",
    "prefix = 'Writing jets to file:'\n",
    "\n",
    "qu.printProgressBarColor(0, l, prefix=prefix, suffix='Complete', length=bar_length)\n",
    "for i in range(nevents):\n",
    "    n = len(event_jets[i])\n",
    "    branch_buffer['nJet'][0][0] = n\n",
    "    for j in range(n):\n",
    "        branch_buffer['jetPt'][0][j] = event_jets[i][j].pt()\n",
    "        branch_buffer['jetEta'][0][j] = event_jets[i][j].eta()\n",
    "        branch_buffer['jetPhi'][0][j] = event_jets[i][j].phi()\n",
    "        branch_buffer['jetE'][0][j] = event_jets[i][j].e()\n",
    "    t.Fill()\n",
    "    \n",
    "    if(i%stride == 0): qu.printProgressBarColor(int(i/stride), l, prefix=prefix, suffix='Complete', length=bar_length)\n",
    "    \n",
    "t.Write('',rt.TObject.kOverwrite)\n",
    "qu.printProgressBarColor(l, l, prefix=prefix, suffix='Complete', length=bar_length)\n",
    "f.Close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code below here is unused/testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Little RDataFrame demo.\n",
    "\n",
    "a = range(10)\n",
    "b = np.random.rand(10)\n",
    "df = ROOT.RDataFrame(10)\n",
    "df = df.Define(\"x\", 'auto to_eval = std::string(\"a[\") + std::to_string(rdfentry_) + \"]\"; return float(TPython::Eval(to_eval.c_str()));')\n",
    "df = df.Define(\"y\", 'auto to_eval = std::string(\"b[\") + std::to_string(rdfentry_) + \"]\"; return float(TPython::Eval(to_eval.c_str()));')\n",
    "display = df.Display()\n",
    "display.Print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check showing that clusterIndex is not unique between files.\n",
    "trees = {key:file.Get(tree_name) for key, file in files.items()}\n",
    "t1 = trees['piminus']\n",
    "t2 = trees['piplus']\n",
    "\n",
    "t1_range = range(3,6)\n",
    "t2_range = range(316,318)\n",
    "\n",
    "for i in t1_range:\n",
    "    t1.GetEntry(i)\n",
    "    print(t1.eventNumber,'\\t',t1.clusterIndex)\n",
    "\n",
    "print('---')\n",
    "for i in t2_range:\n",
    "    t2.GetEntry(i)\n",
    "    print(t2.eventNumber,'\\t',t2.clusterIndex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml4p]",
   "language": "python",
   "name": "conda-env-ml4p-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
