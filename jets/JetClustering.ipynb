{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jet Clustering using \"Smart Topo-Clusters\"\n",
    "\n",
    "The big idea is to use neural networks for classification and/or energy calibration of topo-clusters, and use these topo-clusters for making jets. In this notebook I'll be playing around with some ideas for this, to see what works.\n",
    "\n",
    "In this notebook we will *not* be training neural networks. That's taken care of by other notebooks in the `/classifier` and `/regression` directories of this repo. We will instead be applying the existing, trained networks to some data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Setup\n",
    "\n",
    "First, let's import a bunch of packages we know we'll need right off-the-bat.\n",
    "\n",
    "Note that as we've set up our environment with `conda`, our `ROOT` installation has all the bells and whistles. This includes the `pythia8` library and its associated `ROOT` wrapper, `TPythia8`. We can optionally use this for jet-clustering, as it comes `fj-core`.\n",
    "Alternatively we could use the Pythonic interface for `fastjet` or [pyjet](https://github.com/scikit-hep/pyjet), but the latter requires linking an external fastjet build for speed and this doesn't seem to work when following their documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.22/02\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ROOT as rt\n",
    "import uproot as ur\n",
    "#import pandas as pd\n",
    "import sys, glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's also some slightly contrived setup for `latex`. We may need this for the `atlas_mpl_style` package, which is employed in some of Max's plotting utilities that we may want to borrow. Since `latex` isn't set up on the [UChicago ML platform](https://ml.maniac.uchicago.edu) by default, our setup script may install it separately but it's still not on `$PATH` since we don't touch our bash profile. This cell uses some `IPython` magic to adjust `$PATH` for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/opt/conda/envs/ml4p/bin:/opt/conda/condabin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/texlive/2020/bin/x86_64-linux\n"
     ]
    }
   ],
   "source": [
    "# Check if latex is set up already.\n",
    "# We use some Jupyter magic -- alternatively one could use python's subprocess here.\n",
    "has_latex = !command -v latex\n",
    "has_latex = (not has_latex == [])\n",
    "\n",
    "# If latex was not a recognized command, our setup script should have installed\n",
    "# at a fixed location, but it is not on the $PATH. Now let's use some Jupyter magic.\n",
    "# See https://ipython.readthedocs.io/en/stable/interactive/shell.html for info.\n",
    "if(not has_latex):\n",
    "    latex_prefix = '/usr/local/texlive/2020/bin/x86_64-linux'\n",
    "    jupyter_env = %env\n",
    "    path = jupyter_env['PATH']\n",
    "    path = path + ':' + latex_prefix\n",
    "    %env PATH = $path\n",
    "    jupyter_env = %env\n",
    "    path = jupyter_env['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some extra setup\n",
    "path_prefix = '/workspace/LCStudies/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Fetching the data\n",
    "\n",
    "Now we get our data. For now, our classifiers are being trained to distinguish between $\\pi^+$ and $\\pi^0$. Assuming that all charged pions behave the same way, we can really treat this as a $\\pi^\\pm$ vs. $\\pi^0$ classifier. **For our toy workflow, we'll say that we only want to cluster $\\pi^\\pm$ topo-clusters into jets.** We will treat $\\pi^0$ as a background.\n",
    "\n",
    "Note that our original data is saved in `ROOT` files where each entry corresponds with one topo-cluster. To perform jet clustering, we want to reorganize this so that we have topo-clusters grouped by event. Our data does contain info on run numbers and event numbers, so we should be able to perform this recombination. To speed things up, we will save the clusters (grouped by event) to a new set of `ROOT` files, so that we don't necessarily need to perform this recombination every time we run our workflow -- we'll check to see if these files have been made previously.\n",
    "\n",
    "*Before* we perform this recombination, we might as well get our network scores for each cluster. This will only require accessing the calorimeter images from the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(path_prefix not in sys.path): sys.path.append(path_prefix)\n",
    "from  util import ml_util as mu\n",
    "\n",
    "# ----- Meta-data for our dataset -----\n",
    "layers = [\"EMB1\", \"EMB2\", \"EMB3\", \"TileBar0\", \"TileBar1\", \"TileBar2\"]\n",
    "nlayers = len(layers)\n",
    "cell_size_phi = [0.098, 0.0245, 0.0245, 0.1, 0.1, 0.1]\n",
    "cell_size_eta = [0.0031, 0.025, 0.05, 0.1, 0.1, 0.2]\n",
    "len_phi = [4, 16, 16, 4, 4, 4]\n",
    "len_eta = [128, 16, 8, 4, 4, 2]\n",
    "assert(len(len_phi) == nlayers)\n",
    "assert(len(len_eta) == nlayers)\n",
    "# -------------------------------------\n",
    "\n",
    "# We open the files using uproot, and use our ml_util to get the images.\n",
    "data_dir = '/workspace/LCStudies/data'\n",
    "data_files = glob.glob(data_dir + '/*.root')\n",
    "data_files = {x.split('/')[-1].replace('.root',''):x for x in data_files}\n",
    "tree_name = 'ClusterTree'\n",
    "trees = {key: ur.open(file)[tree_name] for key, file in data_files.items()}\n",
    "\n",
    "pcells = {\n",
    "    ifile : {\n",
    "        layer : mu.setupCells(itree, layer)\n",
    "        for layer in layers\n",
    "    }\n",
    "    for ifile, itree in trees.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Applying the classifier network\n",
    "\n",
    "\n",
    "Now, let's import some `tensorflow` and `keras` stuff that we'll need for applying our trained networks to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of devices: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-29 06:18:33.637044: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2020-10-29 06:18:33.689971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:3e:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2020-10-29 06:18:33.690323: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-10-29 06:18:33.693031: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-10-29 06:18:33.695482: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-10-29 06:18:33.695868: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-10-29 06:18:33.698372: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-10-29 06:18:33.699466: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-10-29 06:18:33.705224: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-10-29 06:18:33.708189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2020-10-29 06:18:33.708739: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2020-10-29 06:18:33.720049: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3200000000 Hz\n",
      "2020-10-29 06:18:33.722353: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562d94407970 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-10-29 06:18:33.722372: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-10-29 06:18:33.723942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:3e:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2020-10-29 06:18:33.723986: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-10-29 06:18:33.724002: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-10-29 06:18:33.724017: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-10-29 06:18:33.724031: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-10-29 06:18:33.724046: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-10-29 06:18:33.724060: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-10-29 06:18:33.724074: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-10-29 06:18:33.726827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2020-10-29 06:18:33.726870: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-10-29 06:18:33.909805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-10-29 06:18:33.909830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
      "2020-10-29 06:18:33.909842: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
      "2020-10-29 06:18:33.914419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10211 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:3e:00.0, compute capability: 7.5)\n",
      "2020-10-29 06:18:33.916944: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562d95e49150 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2020-10-29 06:18:33.916961: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\n"
     ]
    }
   ],
   "source": [
    "ngpu = 1\n",
    "gpu_list = [\"/gpu:\"+str(i) for i in range(ngpu)]\n",
    "import tensorflow as tf\n",
    "strategy = tf.distribute.MirroredStrategy(devices=gpu_list)\n",
    "ngpu = strategy.num_replicas_in_sync\n",
    "print ('Number of devices: {}'.format(ngpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EMB1, EMB2, EMB3, TileBar0, TileBar1, TileBar2.\n",
      "Calculating network scores: |██████████████████████████████████████████████████| 100.0% Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-29 06:18:36.257388: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from  util import qol_util as qu\n",
    "modelpath = path_prefix + 'classifier/Models'\n",
    "model_postfix = '_flat_do20' # for our simple, per-layer NN's\n",
    "models = {}\n",
    "model_history = {}\n",
    "model_scores = {}\n",
    "\n",
    "i = 0\n",
    "for layer in layers:\n",
    "    if(i == 0): print('Loading ',end='')\n",
    "    print(layer,end='')\n",
    "    if(i!= len(layers)-1): print(', ',end='')\n",
    "    else: print('.')\n",
    "    i += 1\n",
    "    \n",
    "    models[layer] = tf.keras.models.load_model(modelpath+'/model_' + layer + model_postfix + '.h5')\n",
    "    # Load history object.\n",
    "    with open(modelpath + '/model_' + layer + model_postfix + '.history','rb') as model_history_file:\n",
    "        model_history[layer] = pickle.load(model_history_file)\n",
    "    \n",
    "# Recalculate network scores for the datasets.\n",
    "prefix = 'Calculating network scores:'\n",
    "l = len(pcells) * len(layers)\n",
    "qu.printProgressBar(0, l, prefix=prefix, suffix='Complete', length=50)\n",
    "i = 0\n",
    "for key in pcells.keys():\n",
    "    \n",
    "    model_scores[key] = {}\n",
    "    for layer in layers: \n",
    "        model_scores[key][layer] = models[layer].predict(pcells[key][layer])\n",
    "        i += 1\n",
    "        qu.printProgressBar(i, l, prefix=prefix, suffix='Complete', length=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each event, the corresponding `model_scores` entry is a tuple.\n",
    "The first entry is the \"background score\" -- how likely the cluster is to be a $\\pi^0$. The second is the \"signal score\" -- how likely the cluster is to be a $\\pi^\\pm$. At least this seems to be the correct interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our network scores for all our clusters, we need to group our clusters by event -- in a new file. As each entry will correspond with one event, most of our scalar branches will now become vectors, listing properties for each cluster in the event. Besides adding network scores for each cluster, we will also add truth info -- to what kind of pion it actually corresponds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that as we're including a signal/background flag as a new branch, this is where we determine the signal/background split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling friend tree for pi0:     |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Filling friend tree for piminus: |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Filling friend tree for piplus:  |██████████████████████████████████████████████████| 100.0% Complete\n"
     ]
    }
   ],
   "source": [
    "sig_definition = {'signal':['piminus','piplus'],'background':['pi0']}\n",
    "\n",
    "from pathlib import Path\n",
    "jet_data_dir = path_prefix + 'jets/data'\n",
    "Path(jet_data_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get our original data files.\n",
    "files = {key:rt.TFile(file,'READ') for key, file in data_files.items()}\n",
    "trees = {key:file.Get(tree_name) for key, file in files.items()}\n",
    "\n",
    "# Now we want to effectively add some new columns. We accomplish this with friend trees.\n",
    "data = {\n",
    "    'signal':np.zeros(1,dtype=np.dtype('i2')),\n",
    "}\n",
    "for layer in layers:\n",
    "    bname = layer + '_NN'\n",
    "    data[bname] = np.zeros(2,dtype=np.dtype('f8'))\n",
    "\n",
    "for key in sorted(trees.keys()):\n",
    "    \n",
    "    friend_filename = data_files[key].split('/')[-1]\n",
    "    friend_filename = jet_data_dir + '/' + friend_filename\n",
    "    friend_file = rt.TFile(friend_filename,'RECREATE')\n",
    "    \n",
    "    friend_tree = rt.TTree(tree_name + '_friend',tree_name + '_friend')\n",
    "    branches = {}\n",
    "\n",
    "    # --- Setup the branches. This is a rather general/flexible code block. ---\n",
    "    for bname, val in data.items():\n",
    "        descriptor = bname\n",
    "        bshape = val.shape\n",
    "        if(bshape != (1,)):\n",
    "            for i in range(len(bshape)):\n",
    "                descriptor += '[' + str(bshape[i]) + ']'\n",
    "        descriptor += '/'\n",
    "        if(val.dtype == np.dtype('i2')): descriptor += 'S'\n",
    "        elif(val.dtype == np.dtype('i4')): descriptor += 'I'\n",
    "        elif(val.dtype == np.dtype('i8')): descriptor += 'L'\n",
    "        elif(val.dtype == np.dtype('f4')): descriptor += 'F'\n",
    "        elif(val.dtype == np.dtype('f8')): descriptor += 'D'\n",
    "        else:\n",
    "            print('Warning, setup issue for branch: ', key, '. Skipping.')\n",
    "            continue\n",
    "        branches[key] = friend_tree.Branch(bname,val,descriptor)\n",
    "    # --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\n",
    "    \n",
    "    # Now we fill the friend tree.\n",
    "    nentries = trees[key].GetEntries()\n",
    "    for layer in layers: assert len(model_scores[key][layer]) == nentries\n",
    "\n",
    "    # Signal flag will be constant since input trees are divided by particle identity.\n",
    "    sig = 0\n",
    "    if(key in sig_definition['signal']): sig = 1\n",
    "    nn_scores = model_scores[key]\n",
    "    \n",
    "    prefix = 'Filling friend tree for ' + key + ':'\n",
    "    if(len(prefix) < 32): prefix = prefix + ' ' * (32 - len(prefix))\n",
    "    qu.printProgressBar(0, int(nentries/100), prefix=prefix, suffix='Complete', length=50)\n",
    "    for i in range(nentries):\n",
    "        data['signal'][0] = sig\n",
    "        for layer in layers: data[layer + '_NN'][:] = nn_scores[layer][i,:]\n",
    "        friend_tree.Fill()\n",
    "        if(i%100 ==0): qu.printProgressBar(i/100, int(nentries/100), prefix=prefix, suffix='Complete', length=50)\n",
    "    friend_tree.Write()\n",
    "    friend_file.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdf_array_str = 'auto to_eval = std::string(\"$NAME[\") + std::to_string(rdfentry_) + \"]\"; return float(TPython::Eval(to_eval.c_str()));'\n",
    "\n",
    "\n",
    "# for key, file in data_files.items():\n",
    "#     if(key != 'pi0'): continue\n",
    "#     print(file)\n",
    "#     f = rt.TFile(file,'READ')\n",
    "#     t = f.Get(tree_name)\n",
    "\n",
    "#     # Now we make an RDataFrame. This might be the simplest way to\n",
    "#     # add a new column from a Python list/array.\n",
    "#     df = rt.RDataFrame(t)\n",
    "#     for layer in ['EMB1','EMB2']:\n",
    "#         arr = model_scores[key][layer][:,0]\n",
    "#         arr2 = model_scores[key][layer][:,0]\n",
    "#         df = df.Define(layer + '_NN_0', rdf_array_str.replace('$NAME','arr'))\n",
    "#         df = df.Define(layer + '_NN_1', rdf_array_str.replace('$NAME','arr2'))\n",
    "\n",
    "\n",
    "\n",
    "#     display = df.Display()\n",
    "#     display.Print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a chain with all the original data.\n",
    "chain = rt.TChain(tree_name)\n",
    "for file in data_files.values(): chain.AddFile(file)\n",
    "    \n",
    "# Now let's index things by eventNumber -- this is like sorting.\n",
    "# See https://root-forum.cern.ch/t/usage-of-tchainindex/19074/4 (TTreeIndex vs. TChainIndex)\n",
    "chain_idx = rt.TTreeIndex(chain,'eventNumber','0') # first index is eventNumber, second is empty (runNumber is always the same)\n",
    "\n",
    "# Note that the indices are not unique, i.e. for each event number there are likely multiple entries.\n",
    "# However I don't think this is an issue.\n",
    "n_idx = chain_idx.GetN()\n",
    "assert(n_idx == chain.GetEntries()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    chain.GetEntry(chain_idx.GetIndex()[i])\n",
    "    print(chain.eventNumber,chain.runNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_idx.GetEntryNumberWithIndex(1058,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(-15,15,1):\n",
    "    j = 378 + i\n",
    "    chain.GetEntry(j)\n",
    "    print(j, chain.eventNumber)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code below this cell is old/unused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append(path_prefix)\n",
    "# sys.path\n",
    "# from  util import ml_util as mu\n",
    "# inputpath = path_prefix+'data/'\n",
    "\n",
    "# # first making our DataFrames and taking care of scalars\n",
    "# branches = ['runNumber', 'eventNumber', 'truthE', 'truthPt', 'truthEta', 'truthPhi', 'clusterIndex', 'nCluster', 'clusterE', 'clusterECalib', 'clusterPt', 'clusterEta', 'clusterPhi', 'cluster_nCells', 'cluster_sumCellE', 'cluster_ENG_CALIB_TOT', 'cluster_ENG_CALIB_OUT_T', 'cluster_ENG_CALIB_DEAD_TOT', 'cluster_EM_PROBABILITY', 'cluster_HAD_WEIGHT', 'cluster_OOC_WEIGHT', 'cluster_DM_WEIGHT', 'cluster_CENTER_MAG', 'cluster_FIRST_ENG_DENS', 'cluster_cell_dR_min', 'cluster_cell_dR_max', 'cluster_cell_dEta_min', 'cluster_cell_dEta_max', 'cluster_cell_dPhi_min', 'cluster_cell_dPhi_max', 'cluster_cell_centerCellEta', 'cluster_cell_centerCellPhi', 'cluster_cell_centerCellLayer', 'cluster_cellE_norm']\n",
    "# rootfiles = [\"pi0\", \"piplus\", \"piminus\"]\n",
    "# trees = {\n",
    "#     rfile : ur.open(inputpath+rfile+\".root\")['ClusterTree']\n",
    "#     for rfile in rootfiles\n",
    "# }\n",
    "# pdata = {\n",
    "#     ifile : itree.pandas.df(branches, flatten=False)\n",
    "#     for ifile, itree in trees.items()\n",
    "# }\n",
    "\n",
    "# np0 = len(pdata['pi0'])\n",
    "# npp = len(pdata['piplus'])\n",
    "# npm = len(pdata['piminus'])\n",
    "\n",
    "# # Taking care of multi-dim branches using Max's ml_util. I think that the uproot-pandas interface doesn't handle these nicely.\n",
    "# pcells = {\n",
    "#     ifile : {\n",
    "#         layer : mu.setupCells(itree, layer)\n",
    "#         for layer in layers\n",
    "#     }\n",
    "#     for ifile, itree in trees.items()\n",
    "# }\n",
    "\n",
    "# print(\"Number of pi0 events: {}\".format(np0))\n",
    "# print(\"Number of pi+ events: {}\".format(npp))\n",
    "# print(\"Number of pi- events: {}\".format(npm))\n",
    "# print(\"Total: {}\".format(np0+npp+npm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Merge the pandas DataFrames.\n",
    "# pi0_frame  = pdata['pi0']\n",
    "# pipm_frame = pd.concat([pdata['piminus'],pdata['piplus']])\n",
    "# pdata = {'pi0': pi0_frame, 'pipm':pipm_frame}\n",
    "# assert(len(pdata['pipm']) == npp+npm)\n",
    "\n",
    "# # Let's write a function to \"nicely\" merge the dicts, \n",
    "# # in case we want to do this again in some other way.\n",
    "# def MergeImageDicts(dict1,dict2):\n",
    "#     assert(set(dict1.keys()) == set(dict2.keys()))\n",
    "#     dict3 = {}\n",
    "#     for key in dict1.keys():\n",
    "#         arr1 = dict1[key]\n",
    "#         arr2 = dict2[key]\n",
    "#         arr3 = np.concatenate((arr1,arr2),axis=0)\n",
    "#         dict3[key] = arr3\n",
    "#     return dict3\n",
    "\n",
    "# # Now merge the dictionaries.\n",
    "# merged_dict = MergeImageDicts(pcells['piminus'],pcells['piplus'])\n",
    "# pcells['pipm'] = merged_dict\n",
    "# del pcells['piplus']\n",
    "# del pcells['piminus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/workspace/LCStudies/setup/fastjet/fastjet-install/lib/python3.8/site-packages')\n",
    "import fastjet as fj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fj.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fj.PseudoJet.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AwesomeModel:\n",
    "    def predict(self, x):\n",
    "        return x[0] * x[1]\n",
    "\n",
    "model = AwesomeModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "import ROOT\n",
    "@ROOT.Numba.Declare([\"RVec<float>\", \"int\"], \"float\") \n",
    "def pysumpow(x: np.ndarray, y: int):\n",
    "    return np.sum(x)**y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = range(10)\n",
    "b = np.random.rand(10)\n",
    "df = ROOT.RDataFrame(10)\n",
    "df = df.Define(\"x\", 'auto to_eval = std::string(\"a[\") + std::to_string(rdfentry_) + \"]\"; return float(TPython::Eval(to_eval.c_str()));')\n",
    "df = df.Define(\"y\", 'auto to_eval = std::string(\"b[\") + std::to_string(rdfentry_) + \"]\"; return float(TPython::Eval(to_eval.c_str()));')\n",
    "\n",
    "\n",
    "display = df.Display()\n",
    "display.Print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml4p]",
   "language": "python",
   "name": "conda-env-ml4p-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
