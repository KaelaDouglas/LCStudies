{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jet Clustering using \"Smart Topo-Clusters\"\n",
    "\n",
    "The big idea is to use neural networks for classification and/or energy calibration of topo-clusters, and use these topo-clusters for making jets. In this notebook I'll be playing around with some ideas for this, to see what works.\n",
    "\n",
    "In this notebook we will *not* be training neural networks. That's taken care of by other notebooks in the `/classifier` and `/regression` directories of this repo. We will instead be applying the existing, trained networks to some data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Setup\n",
    "\n",
    "First, let's import a bunch of packages we know we'll need right off-the-bat.\n",
    "\n",
    "Note that as we've set up our environment with `conda`, our `ROOT` installation has all the bells and whistles. This includes the `pythia8` library and its associated `ROOT` wrapper, `TPythia8`. We will use this for jet-clustering, as it comes `fj-core`. (alternatively we could use [pyjet](https://github.com/scikit-hep/pyjet) but this requires linking an external fastjet build for speed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.22/02\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ROOT as rt\n",
    "import uproot as ur\n",
    "import pandas as pd\n",
    "import sys, glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's also some slightly contrived setup for `latex`. We may need this for the `atlas_mpl_style` package, which is employed in some of Max's plotting utilities that we may want to borrow. Since `latex` isn't set up on the [UChicago ML platform](https://ml.maniac.uchicago.edu) by default, our setup script may install it separately but it's still not on `$PATH` since we don't touch our bash profile. This cell uses some `IPython` magic to adjust `$PATH` for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/opt/conda/envs/ml4p/bin:/opt/conda/condabin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/texlive/2020/bin/x86_64-linux\n"
     ]
    }
   ],
   "source": [
    "# Check if latex is set up already.\n",
    "# We use some Jupyter magic -- alternatively one could use python's subprocess here.\n",
    "has_latex = !command -v latex\n",
    "has_latex = (not has_latex == [])\n",
    "\n",
    "# If latex was not a recognized command, our setup script should have installed\n",
    "# at a fixed location, but it is not on the $PATH. Now let's use some Jupyter magic.\n",
    "# See https://ipython.readthedocs.io/en/stable/interactive/shell.html for info.\n",
    "if(not has_latex):\n",
    "    latex_prefix = '/usr/local/texlive/2020/bin/x86_64-linux'\n",
    "    jupyter_env = %env\n",
    "    path = jupyter_env['PATH']\n",
    "    path = path + ':' + latex_prefix\n",
    "    %env PATH = $path\n",
    "    jupyter_env = %env\n",
    "    path = jupyter_env['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some extra setup\n",
    "path_prefix = '/workspace/LCStudies/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Fetching the data\n",
    "\n",
    "\n",
    "Now we get our data. For now, our classifiers are being trained to distinguish between $\\pi^+$ and $\\pi^0$. Assuming that all charged pions behave the same way, we can really treat this as a $\\pi^\\pm$ vs. $\\pi^0$ classifier.\n",
    "\n",
    "**For our toy workflow, we'll say that we only want to cluster $\\pi^\\pm$ topo-clusters into jets.** We will treat $\\pi^0$ as a background.\n",
    "\n",
    "We will load the data into a `pandas` DataFrame. As a matter of taste I'm not a *huge* `pandas` fan -- it's sometimes rather slow with big datasets, compared to using `ROOT` or `h5`-based methods -- but for now it will work just fine! This code snippet is borrowed from Max's notebooks. \n",
    "\n",
    "Note that we want to concat together our $\\pi^-$ and $\\pi^+$ data, so we end up with $2$ DataFrames intead of $3$. I haven't figured out how to handle `ROOT::TChain` in `uproot` (i.e. creating chains instead of trees, for exporting to `pandas`) but we can concatenate the DataFrames after-the-fact. Maybe this is slow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A ROOT-based way of loading data.\n",
    "# data_dir = '/workspace/LCStudies/data'\n",
    "# data_files = glob.glob(data_dir + '/*.root')\n",
    "# data_files.sort() # pi0 will be the 1st entry now\n",
    "# data_files = {'background':[data_files[0]], 'signal':[data_files[1],data_files[2]]}\n",
    "\n",
    "# # Now, we want to get the data in the files. For signal, we will be combining 2 TTree's, so we will use the TChain functionality.\n",
    "# tree_name = 'ClusterTree' # this name is the same across all our files\n",
    "# data_chains = {'background':rt.TChain(),'signal':rt.TChain()}\n",
    "# for key, chain in data_chains.items():\n",
    "#     chain.SetTitle(key)\n",
    "#     for file in data_files[key]: chain.AddFile(file,rt.TTree.kMaxEntries,tree_name)\n",
    "#     report = key\n",
    "#     if(key == 'signal'): report += '    '\n",
    "#     report += ': ' + str(chain.GetEntries())\n",
    "#     print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta-data for our dataset\n",
    "layers = [\"EMB1\", \"EMB2\", \"EMB3\", \"TileBar0\", \"TileBar1\", \"TileBar2\"]\n",
    "nlayers = len(layers)\n",
    "cell_size_phi = [0.098, 0.0245, 0.0245, 0.1, 0.1, 0.1]\n",
    "cell_size_eta = [0.0031, 0.025, 0.05, 0.1, 0.1, 0.2]\n",
    "len_phi = [4, 16, 16, 4, 4, 4]\n",
    "len_eta = [128, 16, 8, 4, 4, 2]\n",
    "\n",
    "assert(len(len_phi) == nlayers)\n",
    "assert(len(len_eta) == nlayers)\n",
    "\n",
    "cell_shapes = {layers[i]:(len_eta[i],len_phi[i]) for i in range(nlayers)}\n",
    "#for i in range(nlayers): cell_shapes[layers[i]] = (len_eta[i],len_phi[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pi0 events: 263891\n",
      "Number of pi+ events: 435967\n",
      "Number of pi- events: 434627\n",
      "Total: 1134485\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(path_prefix)\n",
    "sys.path\n",
    "from  util import ml_util as mu\n",
    "inputpath = path_prefix+'data/'\n",
    "\n",
    "# first making our DataFrames and taking care of scalars\n",
    "branches = ['runNumber', 'eventNumber', 'truthE', 'truthPt', 'truthEta', 'truthPhi', 'clusterIndex', 'nCluster', 'clusterE', 'clusterECalib', 'clusterPt', 'clusterEta', 'clusterPhi', 'cluster_nCells', 'cluster_sumCellE', 'cluster_ENG_CALIB_TOT', 'cluster_ENG_CALIB_OUT_T', 'cluster_ENG_CALIB_DEAD_TOT', 'cluster_EM_PROBABILITY', 'cluster_HAD_WEIGHT', 'cluster_OOC_WEIGHT', 'cluster_DM_WEIGHT', 'cluster_CENTER_MAG', 'cluster_FIRST_ENG_DENS', 'cluster_cell_dR_min', 'cluster_cell_dR_max', 'cluster_cell_dEta_min', 'cluster_cell_dEta_max', 'cluster_cell_dPhi_min', 'cluster_cell_dPhi_max', 'cluster_cell_centerCellEta', 'cluster_cell_centerCellPhi', 'cluster_cell_centerCellLayer', 'cluster_cellE_norm']\n",
    "rootfiles = [\"pi0\", \"piplus\", \"piminus\"]\n",
    "trees = {\n",
    "    rfile : ur.open(inputpath+rfile+\".root\")['ClusterTree']\n",
    "    for rfile in rootfiles\n",
    "}\n",
    "pdata = {\n",
    "    ifile : itree.pandas.df(branches, flatten=False)\n",
    "    for ifile, itree in trees.items()\n",
    "}\n",
    "\n",
    "np0 = len(pdata['pi0'])\n",
    "npp = len(pdata['piplus'])\n",
    "npm = len(pdata['piminus'])\n",
    "\n",
    "# Taking care of multi-dim branches using Max's ml_util. I think that the uproot-pandas interface doesn't handle these nicely.\n",
    "pcells = {\n",
    "    ifile : {\n",
    "        layer : mu.setupCells(itree, layer)\n",
    "        for layer in layers\n",
    "    }\n",
    "    for ifile, itree in trees.items()\n",
    "}\n",
    "\n",
    "print(\"Number of pi0 events: {}\".format(np0))\n",
    "print(\"Number of pi+ events: {}\".format(npp))\n",
    "print(\"Number of pi- events: {}\".format(npm))\n",
    "print(\"Total: {}\".format(np0+npp+npm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we merge the $\\pi^+$ and $\\pi^-$ DataFrames and dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the pandas DataFrames.\n",
    "pi0_frame  = pdata['pi0']\n",
    "pipm_frame = pd.concat([pdata['piminus'],pdata['piplus']])\n",
    "pdata = {'pi0': pi0_frame, 'pipm':pipm_frame}\n",
    "assert(len(pdata['pipm']) == npp+npm)\n",
    "\n",
    "# Let's write a function to \"nicely\" merge the dicts, \n",
    "# in case we want to do this again in some other way.\n",
    "def MergeImageDicts(dict1,dict2):\n",
    "    assert(set(dict1.keys()) == set(dict2.keys()))\n",
    "    dict3 = {}\n",
    "    for key in dict1.keys():\n",
    "        arr1 = dict1[key]\n",
    "        arr2 = dict2[key]\n",
    "        arr3 = np.concatenate((arr1,arr2),axis=0)\n",
    "        dict3[key] = arr3\n",
    "    return dict3\n",
    "\n",
    "# Now merge the dictionaries.\n",
    "merged_dict = MergeImageDicts(pcells['piplus'],pcells['piminus'])\n",
    "pcells['pipm'] = merged_dict\n",
    "del pcells['piplus']\n",
    "del pcells['piminus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for the time being, we will not have a 1:1 signal-to-background ratio. This is because we have many more $\\pi^\\pm$ entries than $\\pi^0$ entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Applying the classifier network\n",
    "\n",
    "\n",
    "Now, let's import some `tensorflow` and `keras` stuff that we'll need for applying our trained networks to the data. We'll also repackage the data for the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of devices: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-26 21:33:27.167619: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2020-10-26 21:33:27.202115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:3e:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2020-10-26 21:33:27.202383: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-10-26 21:33:27.204223: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-10-26 21:33:27.206027: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-10-26 21:33:27.206327: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-10-26 21:33:27.208262: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-10-26 21:33:27.209147: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-10-26 21:33:27.213103: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-10-26 21:33:27.215298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2020-10-26 21:33:27.215989: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2020-10-26 21:33:27.226129: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3200000000 Hz\n",
      "2020-10-26 21:33:27.231071: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562e86c7d600 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-10-26 21:33:27.231087: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-10-26 21:33:27.232332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:3e:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2020-10-26 21:33:27.232375: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-10-26 21:33:27.232386: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-10-26 21:33:27.232396: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-10-26 21:33:27.232406: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-10-26 21:33:27.232415: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-10-26 21:33:27.232425: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-10-26 21:33:27.232435: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-10-26 21:33:27.234525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2020-10-26 21:33:27.234564: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-10-26 21:33:27.375856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-10-26 21:33:27.375879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
      "2020-10-26 21:33:27.375887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
      "2020-10-26 21:33:27.379267: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10211 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:3e:00.0, compute capability: 7.5)\n",
      "2020-10-26 21:33:27.381467: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562e83d658d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2020-10-26 21:33:27.381483: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\n"
     ]
    }
   ],
   "source": [
    "ngpu = 1\n",
    "gpu_list = [\"/gpu:\"+str(i) for i in range(ngpu)]\n",
    "import tensorflow as tf\n",
    "strategy = tf.distribute.MirroredStrategy(devices=gpu_list)\n",
    "ngpu = strategy.num_replicas_in_sync\n",
    "print ('Number of devices: {}'.format(ngpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "training_dataset = ['pi0','pipm']\n",
    "\n",
    "# create train/validation/test subsets containing 70%/10%/20%\n",
    "# of events from each type of pion event\n",
    "for p_index, plabel in enumerate(training_dataset):\n",
    "    mu.splitFrameTVT(pdata[plabel],trainfrac=0.7)\n",
    "    pdata[plabel]['label'] = p_index\n",
    "\n",
    "# merge pi0 and pi+- events\n",
    "pdata_merged = pd.concat([pdata[ptype] for ptype in training_dataset])\n",
    "\n",
    "pcells_merged = {\n",
    "    layer : np.concatenate([pcells[ptype][layer]\n",
    "                            for ptype in training_dataset])\n",
    "    for layer in layers\n",
    "}\n",
    "\n",
    "plabels = np_utils.to_categorical(pdata_merged['label'],len(training_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load an NN model that we've previously trained & saved.\n",
    "\n",
    "For now, we'll deal with our simple, per-layer networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EMB1\n",
      "Loading EMB2\n",
      "Loading EMB3\n",
      "Loading TileBar0\n",
      "Loading TileBar1\n",
      "Loading TileBar2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-26 21:52:21.427251: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "modelpath = path_prefix + 'classifier/Models'\n",
    "\n",
    "models = {}\n",
    "model_history = {}\n",
    "model_scores = {}\n",
    "for layer in layers:\n",
    "    print('Loading ' + layer)\n",
    "    models[layer] = tf.keras.models.load_model(modelpath+'/model_' + layer + '_flat_do20.h5')\n",
    "    \n",
    "    # load history object\n",
    "    with open(modelpath + '/model_' + layer + '_flat_do20.history','rb') as model_history_file:\n",
    "        model_history[layer] = pickle.load(model_history_file)\n",
    "    \n",
    "    # recalculate network scores for the dataset\n",
    "    model_scores[layer] = models[layer].predict(\n",
    "        pcells_merged[layer]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each event, the corresponding `model_score` entry is a tuple.\n",
    "The first entry is the \"background score\" -- how likely the cluster is to be a $\\pi^0$. The second is the \"signal score\" -- how likely the cluster is to be a $\\pi^\\pm$. **TODO:** Check this. Seems to *empirically* be the correct interpretation, at least.\n",
    "\n",
    "We \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml4p]",
   "language": "python",
   "name": "conda-env-ml4p-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
