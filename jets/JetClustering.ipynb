{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jet Clustering\n",
    "\n",
    "This workflow is for use with the jet samples, that contain both `ClusterTree` and `EventTree` (provided by the `MLTree` utility). This **cannot** handle data where the `EventTree` does not exist, because that contains info on piecing the clusters together into events*, and the baseline jet clustering.\n",
    "\n",
    "\\* This pieceing together can be accomplished in workflows like `EventReconstructionPion.ipynb` but it's rather complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO:\n",
    "\n",
    "- finish up calculation of scores\n",
    "- jet clustering (save to new file?)\n",
    "- comparison of jets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Setup\n",
    "\n",
    "First, let's import a bunch of packages we know we'll need right off-the-bat.\n",
    "\n",
    "Note that as we've set up our environment with `conda`, our `ROOT` installation has all the bells and whistles. This includes the `pythia8` library and its associated `ROOT` wrapper, `TPythia8`. We can optionally use this for jet-clustering, as it comes `fj-core`.\n",
    "Alternatively we could use the Pythonic interface for `fastjet` or [pyjet](https://github.com/scikit-hep/pyjet), but the latter requires linking an external fastjet build for speed and this doesn't seem to work when following their documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.22/02\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ROOT as rt\n",
    "import uproot as ur\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some extra setup\n",
    "path_prefix = '/workspace/LCStudies/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Fetching the data\n",
    "\n",
    "Now we get our data. For now, our classifiers are being trained to distinguish between $\\pi^+$ and $\\pi^0$. Assuming that all charged pions behave the same way, we can really treat this as a $\\pi^\\pm$ vs. $\\pi^0$ classifier. **For our toy workflow, we'll say that we only want to cluster $\\pi^\\pm$ topo-clusters into jets.** We will treat $\\pi^0$ as a background.\n",
    "\n",
    "For our input data, we have `ROOT` files containing a tree called `ClusterTree`. In each tree, each entry corresponds with one topo-cluster, and the different files correspond with different topo-cluster parent particles (e.g. $3$ files for $\\pi^+$,$\\pi^-$ and $\\pi^0$). Each topo-cluster entry contains information on the event from which it came (\"runNumber\" and \"eventNumber\"), and many topo-clusters (within and across files) share the same event. Our ultimate goal is to regroup this data into one file where each entry corresponds with one *event*. This is a sensible way to arrange the data before performing any jet clustering (which is performed by event), and writing to a file will allow us to skip this whole process after doing it once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Some of this meta-data is unused.\n",
    "# ----- Meta-data for our dataset -----\n",
    "layers = [\"EMB1\", \"EMB2\", \"EMB3\", \"TileBar0\", \"TileBar1\", \"TileBar2\"]\n",
    "nlayers = len(layers)\n",
    "cell_size_phi = [0.098, 0.0245, 0.0245, 0.1, 0.1, 0.1]\n",
    "cell_size_eta = [0.0031, 0.025, 0.05, 0.1, 0.1, 0.2]\n",
    "len_phi = [4, 16, 16, 4, 4, 4]\n",
    "len_eta = [128, 16, 8, 4, 4, 2]\n",
    "assert(len(len_phi) == nlayers)\n",
    "assert(len(len_eta) == nlayers)\n",
    "meta_data = {\n",
    "    layers[i]:{\n",
    "        'cell_size':(cell_size_eta[i],cell_size_phi[i]),\n",
    "        'dimensions':(len_eta[i],len_phi[i])\n",
    "    }\n",
    "    for i in range(nlayers)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring file: /workspace/LCStudies/data/jet/user.angerami.21717971.OutputStream._000077.root (no EventTree/ClusterTree found).\n",
      "Ignoring file: /workspace/LCStudies/data/jet/user.angerami.21717971.OutputStream._000308.root (no EventTree/ClusterTree found).\n",
      "Ignoring file: /workspace/LCStudies/data/jet/user.angerami.21717971.OutputStream._000225.root (no EventTree/ClusterTree found).\n",
      "Ignoring file: /workspace/LCStudies/data/jet/user.angerami.21717971.OutputStream._000259.root (no EventTree/ClusterTree found).\n",
      "Ignoring file: /workspace/LCStudies/data/jet/user.angerami.21717971.OutputStream._000457.root (no EventTree/ClusterTree found).\n",
      "Ignoring file: /workspace/LCStudies/data/jet/user.angerami.21717971.OutputStream._000270.root (no EventTree/ClusterTree found).\n",
      "Ignoring file: /workspace/LCStudies/data/jet/user.angerami.21717971.OutputStream._000026.root (no EventTree/ClusterTree found).\n",
      "Ignoring file: /workspace/LCStudies/data/jet/user.angerami.21717971.OutputStream._000481.root (no EventTree/ClusterTree found).\n",
      "Ignoring file: /workspace/LCStudies/data/jet/user.angerami.21717971.OutputStream._000729.root (no EventTree/ClusterTree found).\n",
      "Ignoring file: /workspace/LCStudies/data/jet/user.angerami.21717971.OutputStream._000272.root (no EventTree/ClusterTree found).\n",
      "Ignoring file: /workspace/LCStudies/data/jet/user.angerami.21717971.OutputStream._000502.root (no EventTree/ClusterTree found).\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "import subprocess as sub\n",
    "# where the original data sits\n",
    "data_dir = '/workspace/LCStudies/data/jet'\n",
    "data_filenames = glob.glob(data_dir + '/' + '*.root')\n",
    "\n",
    "# our \"local\" data dir, where we create modified data files\n",
    "from pathlib import Path\n",
    "jet_data_dir = path_prefix + 'jets/data'\n",
    "Path(jet_data_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get the original data.\n",
    "files = {name:rt.TFile(name,'READ') for name in data_filenames}\n",
    "\n",
    "# Some data files might be missing an EventTree.\n",
    "# For now, we will skip these because our methods count on an existing EventTree.\n",
    "delete_keys = []\n",
    "for key, val in files.items():\n",
    "    file_keys = [x.GetName() for x in val.GetListOfKeys()]\n",
    "    if('ClusterTree' not in file_keys or 'EventTree' not in file_keys):\n",
    "        delete_keys.append(key)\n",
    "\n",
    "for key in delete_keys: \n",
    "    print('Ignoring file:',key,'(no EventTree/ClusterTree found).')\n",
    "    del files[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying data files: |\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m| 100.0% Complete\n"
     ]
    }
   ],
   "source": [
    "if(path_prefix not in sys.path): sys.path.append(path_prefix)\n",
    "from  util import qol_util as qu # for progress bar\n",
    "\n",
    "# now we make a local copy of the files in the jet_data_dir, keeping only certain branches\n",
    "active_branches = {}\n",
    "active_branches['cluster'] = [\n",
    "    'runNumber',\n",
    "    'eventNumber',\n",
    "    'truthE',\n",
    "    'truthPt',\n",
    "    'truthEta',\n",
    "    'truthPhi',\n",
    "    'clusterIndex',\n",
    "    'nCluster',\n",
    "    'clusterE',\n",
    "    'clusterECalib',\n",
    "    'clusterPt',\n",
    "    'clusterEta',\n",
    "    'clusterPhi',\n",
    "    'cluster_nCells',\n",
    "    'cluster_ENG_CALIB_TOT',\n",
    "    'EMB1',\n",
    "    'EMB2',\n",
    "    'EMB3',\n",
    "    'TileBar0',\n",
    "    'TileBar1',\n",
    "    'TileBar2'\n",
    "]\n",
    "active_branches['event'] = [\n",
    "    'runNumber',\n",
    "    'eventNumber',\n",
    "    'lumiBlock',\n",
    "    'NPV',\n",
    "    'nTruthPart',\n",
    "    'clusterCount',\n",
    "    'nCluster',\n",
    "    'clusterE',\n",
    "    'clusterPt',\n",
    "    'clusterEta',\n",
    "    'clusterPhi',\n",
    "    'AntiKt4EMTopoJetsPt',\n",
    "    'AntiKt4EMTopoJetsEta',\n",
    "    'AntiKt4EMTopoJetsPhi',\n",
    "    'AntiKt4EMTopoJetsE',\n",
    "    'AntiKt4LCTopoJetsPt',\n",
    "    'AntiKt4LCTopoJetsEta',\n",
    "    'AntiKt4LCTopoJetsPhi',\n",
    "    'AntiKt4LCTopoJetsE',\n",
    "    'AntiKt4TruthJetsPt',\n",
    "    'AntiKt4TruthJetsEta',\n",
    "    'AntiKt4TruthJetsPhi',\n",
    "    'AntiKt4TruthJetsE'\n",
    "]\n",
    "\n",
    "tree_names = {'cluster':'ClusterTree','event':'EventTree'}\n",
    "data_filenames = []\n",
    "\n",
    "l = len(files.keys())\n",
    "i = 0\n",
    "qu.printProgressBarColor(i, l, prefix='Copying data files:', suffix='Complete', length=50)\n",
    "\n",
    "for path, tfile in files.items():\n",
    "    filename_new = jet_data_dir + '/' + path.split('/')[-1]\n",
    "    old_trees = {x:tfile.Get(tree_names[x]) for x in tree_names.keys()}\n",
    "    \n",
    "    for key, tree in old_trees.items():\n",
    "        tree.SetBranchStatus('*',0)\n",
    "        for bname in active_branches[key]: tree.SetBranchStatus(bname,1)\n",
    "    \n",
    "    tfile_new = rt.TFile(filename_new,'RECREATE')\n",
    "    new_trees = {x:old_trees[x].CloneTree() for x in old_trees.keys()}\n",
    "    tfile_new.Write()\n",
    "    data_filenames.append(filename_new)\n",
    "    i += 1\n",
    "    qu.printProgressBarColor(i, l, prefix='Copying data files:', suffix='Complete', length=50)\n",
    "    del old_trees\n",
    "    del new_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the files & trees with uproot\n",
    "files = {name:rt.TFile(name,'READ') for name in data_filenames}\n",
    "tree_names = {'cluster':'ClusterTree','event':'EventTree'}\n",
    "#trees = {key:{tree_key:file.Get(tree_name) for tree_key,tree_name in tree_names.items()} for key,file in files.items()}\n",
    "ur_trees = {file:{tree_key:ur.open(file)[tree_name] for tree_key,tree_name in tree_names.items()} for file in data_filenames}\n",
    "\n",
    "# reminder: how to get an awkward array for a particular branch, here a is a key corresponding to a filename\n",
    "#ur_trees[a]['cluster'].array('EMB1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading calo images: |\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m| 100.0% Complete\n"
     ]
    }
   ],
   "source": [
    "# Prep the calo images. These will be network inputs.\n",
    "if(path_prefix not in sys.path): sys.path.append(path_prefix)\n",
    "from util import ml_util as mu\n",
    "from util import qol_util as qu # for progress bar\n",
    "\n",
    "l = len(layers) * len(ur_trees.keys())\n",
    "i = 0\n",
    "pfx = 'Loading calo images:'\n",
    "sfx = 'Complete'\n",
    "qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=50)\n",
    "\n",
    "calo_images = {}\n",
    "for dfile in data_filenames:\n",
    "    calo_images[dfile] = {}\n",
    "    for layer in layers:\n",
    "        calo_images[dfile][layer] = mu.setupCells(ur_trees[dfile]['cluster'],layer)\n",
    "        i += 1\n",
    "        qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to \"out of memory\" issues I'm experiencing with loading the images above **and** the regression inputs below (which contain another copy of the images, with standardization), I will try to delete some things as they are no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing regression inputs: |\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m| 100.0% Complete\n"
     ]
    }
   ],
   "source": [
    "# Load some additional information, needed for energy regression.\n",
    "if(path_prefix not in sys.path): sys.path.append(path_prefix)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from util import ml_util as mu # for passing calo images to regression networks\n",
    "\n",
    "scaler_e = StandardScaler()\n",
    "scaler_cal = StandardScaler()\n",
    "scaler_eta = StandardScaler()\n",
    "# epsilon = 1.0e-12 # we will clean values of calibrated energy\n",
    "\n",
    "regression_cols = {}\n",
    "for dfile in data_filenames:\n",
    "    regression_cols[dfile] = {}\n",
    "    \n",
    "#     e_calib = ur_trees[dfile]['cluster'].array('cluster_ENG_CALIB_TOT')\n",
    "#     e_calib = np.where(e_calib < epsilon, epsilon, e_calib) # cleaning: remove zeros to prevent log from blowing up\n",
    "#     regression_cols[dfile]['s_logECalib'] = scaler_cal.fit_transform(np.log(e_calib).reshape(-1,1))\n",
    "        \n",
    "    e = ur_trees[dfile]['cluster'].array('clusterE')\n",
    "    regression_cols[dfile]['s_logE'] = scaler_cal.fit_transform(np.log(e).reshape(-1,1))\n",
    "    \n",
    "    eta = ur_trees[dfile]['cluster'].array('clusterEta')\n",
    "    regression_cols[dfile]['s_eta'] = scaler_cal.fit_transform(eta.reshape(-1,1))\n",
    "\n",
    "l = len(ur_trees.keys())\n",
    "i = 0\n",
    "pfx = 'Preparing regression inputs:'\n",
    "sfx = 'Complete'\n",
    "qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=50)\n",
    "    \n",
    "regression_input = {}\n",
    "for dfile in data_filenames:\n",
    "    combined_images = np.concatenate(tuple([calo_images[dfile][layer] for layer in layers]), axis=1)\n",
    "    s_combined,scaler_combined = mu.standardCells(combined_images, layers)\n",
    "    regression_input[dfile] = np.column_stack((regression_cols[dfile]['s_logE'], regression_cols[dfile]['s_eta'],s_combined))\n",
    "    del regression_cols[dfile]\n",
    "    del ur_trees[dfile]\n",
    "    i = i + 1\n",
    "    qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3)  Adding network scores\n",
    "\n",
    "Now that we have our data, we want to apply our classification and regression networks to all of the topo-clusters, and save the resulting scores for each cluster.\n",
    "\n",
    "We will save the scores in a new tree in each file, which will have as many entries as the existing `ClusterTree`. (It will be a friend tree).\n",
    "\n",
    "**Note** that these networks have been trained elsewhere -- and to start, we're using networks trained on a totally different dataset where we didn't have pile-up. And with our jet dataset, we don't actually know which topo-clusters are caused by $\\pi^\\pm$ and which are caused by $\\pi^0$, so we can only gauge results by how the jet energy resolution is affected, or topo-cluster energy (i.e. we cannot directly determine *classification* accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "# Setup for TensorFlow and Keras.\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # disable some of the tensorflow info printouts, only display errors\n",
    "ngpu = 1\n",
    "gpu_list = [\"/gpu:\"+str(i) for i in range(ngpu)]\n",
    "import tensorflow as tf\n",
    "strategy = tf.distribute.MirroredStrategy(devices=gpu_list)\n",
    "ngpu = strategy.num_replicas_in_sync\n",
    "print ('Number of devices: {}'.format(ngpu))\n",
    "\n",
    "# Dictionary for storing all our neural network models that will be evaluated\n",
    "network_models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our classification model. As we use the simple combo network, we must *also* load all of our so-called \"flat networks\", one per calo layer.\n",
    "# We must evaluate all these networks on the data, and then pass their outputs to the simple combo network.\n",
    "classification_dir = path_prefix + 'classifier/Models'\n",
    "regression_dir = path_prefix + 'regression/Models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading flat classification models... \n",
      "\tLoading EMB1... Done.\n",
      "\tLoading EMB2... Done.\n",
      "\tLoading EMB3... Done.\n",
      "\tLoading TileBar0... Done.\n",
      "\tLoading TileBar1... Done.\n",
      "\tLoading TileBar2... Done.\n"
     ]
    }
   ],
   "source": [
    "# flat classifiers\n",
    "print('Loading flat classification models... ')\n",
    "flat_model_files = glob.glob(classification_dir + '/flat/' + '*.h5')\n",
    "flat_model_files.sort()\n",
    "flat_model_names = []\n",
    "for model in flat_model_files:\n",
    "    model_name = model.split('model_')[-1].split('_flat')[0]\n",
    "    print('\\tLoading ' + model_name + '... ',end='')\n",
    "    flat_model_names.append(model_name)\n",
    "    network_models[model_name] = tf.keras.models.load_model(model)\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading simple combo classification model... Done.\n"
     ]
    }
   ],
   "source": [
    "# combo classifier\n",
    "print('Loading simple combo classification model... ',end='')\n",
    "combo_model_file = classification_dir + '/simple/' + 'model_simple_do20.h5'\n",
    "network_models['combo'] = tf.keras.models.load_model(combo_model_file)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading charged-pion energy regression model... Done.\n",
      "Loading neutral-pion energy regression model... Done.\n"
     ]
    }
   ],
   "source": [
    "# energy regression networks\n",
    "print('Loading charged-pion energy regression model... ',end='')\n",
    "charged_energy_model_file = regression_dir + '/' + 'all_charged.h5'\n",
    "network_models['e_charged'] = tf.keras.models.load_model(charged_energy_model_file)\n",
    "print('Done.')\n",
    "\n",
    "print('Loading neutral-pion energy regression model... ',end='')\n",
    "neutral_energy_model_file = regression_dir + '/' + 'all_neutral.h5'\n",
    "network_models['e_neutral'] = tf.keras.models.load_model(neutral_energy_model_file)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get the network scores.\n",
    "if(path_prefix not in sys.path): sys.path.append(path_prefix)\n",
    "from util import ml_util as mu # for passing calo images to classification networks\n",
    "from  util import qol_util as qu # for progress bar\n",
    "\n",
    "model_scores = {}\n",
    "# We will need to evaluate scores in a particular order, since some networks depend on others' output:\n",
    "# 1) Flat networks (classification)\n",
    "# 2) Simple combo network (classification using flat networks) \n",
    "# 3) Energy regression networks (regression, using simple combo network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shortened file list for debugging\n",
    "a = data_filenames[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating flat networks: |\u001b[31m█\u001b[0m\u001b[31m█\u001b[0m\u001b[31m█\u001b[0m\u001b[31m█\u001b[0m\u001b[31m█\u001b[0m\u001b[31m█\u001b[0m\u001b[31m█\u001b[0m\u001b[31m█\u001b[0m\u001b[31m█\u001b[0m\u001b[31m█\u001b[0m\u001b[31m█\u001b[0m\u001b[31m█\u001b[0m\u001b[31m█\u001b[0m\u001b[31m█\u001b[0m------------------------------------| 28.6% Complete\r"
     ]
    }
   ],
   "source": [
    "# Flat networks.\n",
    "# Note: The network names correspond to the branch names they act on.\n",
    "l = len(flat_model_names) * len(data_filenames)\n",
    "i = 0\n",
    "pfx = 'Evaluating flat networks:'\n",
    "sfx = 'Complete'\n",
    "qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=50)\n",
    "for layer in flat_model_names:\n",
    "    model = network_models[layer]\n",
    "    model_scores[layer] = {}\n",
    "    \n",
    "    for dfile in a: # TODO: change a -> data_filenames\n",
    "        model_input = calo_images[dfile][layer]\n",
    "        model_scores[layer][dfile] = model.predict(model_input)\n",
    "        i += 1\n",
    "        qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating combo network: |\u001b[31m█\u001b[0m\u001b[31m█\u001b[0m\u001b[31m█\u001b[0m\u001b[31m█\u001b[0m\u001b[31m█\u001b[0m\u001b[31m█\u001b[0m\u001b[31m█\u001b[0m\u001b[31m█\u001b[0m\u001b[31m█\u001b[0m\u001b[31m█\u001b[0m\u001b[31m█\u001b[0m\u001b[31m█\u001b[0m\u001b[31m█\u001b[0m\u001b[31m█\u001b[0m------------------------------------| 28.6% Complete\r"
     ]
    }
   ],
   "source": [
    "# Combo network.\n",
    "name = 'combo'\n",
    "model = network_models[name]\n",
    "model_scores[name] = {}\n",
    "\n",
    "l = len(data_filenames)\n",
    "i = 0\n",
    "pfx = 'Evaluating combo network:'\n",
    "sfx = 'Complete'\n",
    "qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=50)\n",
    "for dfile in a: # TODO: change a -> ur_trees.keys()\n",
    "    input_scores = np.column_stack([model_scores[layer][dfile][:,1] for layer in layers]) # TODO: Why the [:,1]? Based on Max's code.\n",
    "    model_scores[name][dfile] = model.predict(input_scores)\n",
    "    i += 1\n",
    "    qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=50)\n",
    "    \n",
    "# We can safely delete the flat network scores now (maybe this saves some memory, though I should probably do better memory management in general)\n",
    "for layer in layers:\n",
    "    del model_scores[layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy regression - charged pions.\n",
    "name = 'e_charged'\n",
    "model = network_models[name]\n",
    "model_scores[name] = {}\n",
    "\n",
    "l = len(ur_trees.keys())\n",
    "i = 0\n",
    "pfx = 'Evaluating energy regression networks (charged pions):'\n",
    "sfx = 'Complete'\n",
    "qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=50)\n",
    "\n",
    "    \n",
    "#     combined_images = np.concatenate(tuple([calo_images[key][layer] for layer in layers]), axis=1)\n",
    "#     del calo_images[key] # delete this element of calo_images, it has been copied\n",
    "\n",
    "#     s_combined,scaler_combined = mu.standardCells(combined_images, layers)\n",
    "\n",
    "#     All_input[key] = np.column_stack((data_frames[key]['s_logE'], data_frames[key]['s_eta'],s_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "#\n",
    "# regression network(s) (need to finish)\n",
    "#\n",
    "# save scores to files (in new tree?)\n",
    "#\n",
    "# perform clustering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml4p]",
   "language": "python",
   "name": "conda-env-ml4p-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
