{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jet Clustering\n",
    "\n",
    "This workflow is for use with the jet samples, that contain both `ClusterTree` and `EventTree` (provided by the `MLTree` utility). This **cannot** handle data where the `EventTree` does not exist, because that contains info on piecing the clusters together into events*, and the baseline jet clustering.\n",
    "\n",
    "\\* This pieceing together can be accomplished in workflows like `EventReconstructionPion.ipynb` but it's rather complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First, let's import a bunch of packages we know we'll need right off-the-bat.\n",
    "\n",
    "- `numpy` for computational stuff, and `numba` to be able to speed it up in some places if needed.\n",
    "- `ROOT` for file I/O, 4-vector maths and plotting, and `uproot` for quick Pythonic file reading (but not writing).\n",
    "- A few other \"standard\" libraries like `sys`, `os`, `glob`, `uuid`, `pathlib` and `subprocess`.\n",
    "- Our local `util` library, where we have various functions for data preparation, graphics stuff and some jet clustering functionality.\n",
    "\n",
    "Our jet clustering will be using the Python interface for `fastjet`. This is installed by our setup script in `/LCStudies/setup/fastjet`, and will be accessed within `util.jet_util`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag for skipping file preparation. Can turn this on if you have already made the local copy of data files *and* computed\n",
    "# the network score branches -- which is a time-intensive step. Skipping will give a useful speedup when working on just clustering\n",
    "# or making plots, but this will crash if you haven't run the data copy / network score steps once.\n",
    "skip_scores = True\n",
    "\n",
    "# Flag for skipping calculation of topo-cluster minimum dR. This is a bit time-intensive.\n",
    "skip_dR = False\n",
    "\n",
    "# Debug: Uses only one input file, which will speed things up.\n",
    "debug = False\n",
    "\n",
    "classification_threshold = 0.6\n",
    "\n",
    "# select our source for training -- options are 'pion' (default), 'pion_reweighted' and 'jet'\n",
    "source = 'pion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.22/02\n"
     ]
    }
   ],
   "source": [
    "# Imports - generic stuff\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ROOT as rt\n",
    "import uproot as ur # uproot for accessing ROOT files quickly (and in a Pythonic way)\n",
    "import sys, os, glob, uuid # glob for searching for files, uuid for random strings to name ROOT objects and avoid collisions\n",
    "import subprocess as sub\n",
    "from numba import jit\n",
    "from pathlib import Path\n",
    "from IPython.utils import io # For suppressing some print statements from functions.\n",
    "\n",
    "path_prefix = os.getcwd() + '/../'\n",
    "if(path_prefix not in sys.path): sys.path.append(path_prefix)\n",
    "from util import ml_util as mu # for passing calo images to regression networks\n",
    "from util import qol_util as qu # for progress bar\n",
    "from util import jet_util as ju"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display our plots, let's get a dark style that will look nice in presentations (and JupyterLab in dark mode).\n",
    "dark_style = qu.PlotStyle('dark')\n",
    "light_style = qu.PlotStyle('light')\n",
    "plot_style = dark_style\n",
    "plot_style.SetStyle() # sets style for plots - still need to adjust legends, paves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import `tensorflow` (and some of its `keras` stuff), as well as some stuff from `sklearn` for neural network I/O scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "# Imports and setup for TensorFlow and Keras.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # disable some of the tensorflow info printouts, only display errors\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ngpu = 1\n",
    "gpu_list = [\"/gpu:\"+str(i) for i in range(ngpu)]\n",
    "strategy = tf.distribute.MirroredStrategy(devices=gpu_list)\n",
    "ngpu = strategy.num_replicas_in_sync\n",
    "print ('Number of devices: {}'.format(ngpu))\n",
    "\n",
    "# Dictionary for storing all our neural network models that will be evaluated\n",
    "network_models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up a whole bunch of paths. We use `source = pion` by default, whereby we use networks trained on the single-pion data. We can alternatively use `source = jet` to use our \"facsimile jet training data\" -- a subset of jet data where we have tried to match topo-clusters to pions -- but this is not fully implemented yet. For example, this workflow explicitly re-derives the network scalers using the `pion` data, so to use the `jet` data we will have to modify that code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(source == 'pion' or source == 'jet'):\n",
    "    classification_dir = path_prefix + 'classifier/Models/' + source\n",
    "    regression_dir = path_prefix + 'regression/Models/' + source\n",
    "    \n",
    "elif(source == 'pion_reweighted'):\n",
    "    classification_dir = path_prefix + 'classifier/Models/' + 'pion'\n",
    "    regression_dir = path_prefix + 'regression/Models/' + source\n",
    "\n",
    "data_dir = path_prefix + 'data/jet'\n",
    "training_data_dir = path_prefix + 'data/pion' # TODO: deal with situation where source is jet\n",
    "fj_dir = path_prefix + '/setup/fastjet/fastjet-install/lib/python3.8/site-packages'\n",
    "plot_dir = path_prefix + 'jets/Plots/' + source\n",
    "\n",
    "try: os.makedirs(plot_dir)\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the Plot directory if it does not exist yet\n",
    "if(not os.path.exists(plot_dir)): os.makedirs(plot_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Calorimeter meta-data -----\n",
    "layers = [\"EMB1\", \"EMB2\", \"EMB3\", \"TileBar0\", \"TileBar1\", \"TileBar2\"]\n",
    "nlayers = len(layers)\n",
    "cell_size_phi = [0.098, 0.0245, 0.0245, 0.1, 0.1, 0.1]\n",
    "cell_size_eta = [0.0031, 0.025, 0.05, 0.1, 0.1, 0.2]\n",
    "len_phi = [4, 16, 16, 4, 4, 4]\n",
    "len_eta = [128, 16, 8, 4, 4, 2]\n",
    "assert(len(len_phi) == nlayers)\n",
    "assert(len(len_eta) == nlayers)\n",
    "meta_data = {\n",
    "    layers[i]:{\n",
    "        'cell_size':(cell_size_eta[i],cell_size_phi[i]),\n",
    "        'dimensions':(len_eta[i],len_phi[i])\n",
    "    }\n",
    "    for i in range(nlayers)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading flat classification models... \n",
      "\tLoading EMB1... Done.\n",
      "\tLoading EMB2... Done.\n",
      "\tLoading EMB3... Done.\n",
      "\tLoading TileBar0... Done.\n",
      "\tLoading TileBar1... Done.\n",
      "\tLoading TileBar2... Done.\n",
      "Loading simple combo classification model... Done.\n",
      "Loading charged-pion energy regression model... Done.\n",
      "Loading neutral-pion energy regression model... Done.\n"
     ]
    }
   ],
   "source": [
    "# flat classifiers\n",
    "print('Loading flat classification models... ')\n",
    "flat_model_files = glob.glob(classification_dir + '/flat/' + '*.h5')\n",
    "flat_model_files.sort()\n",
    "flat_model_names = []\n",
    "for model in flat_model_files:\n",
    "    model_name = model.split('model_')[-1].split('_flat')[0]\n",
    "    print('\\tLoading ' + model_name + '... ',end='')\n",
    "    flat_model_names.append(model_name)\n",
    "    network_models[model_name] = tf.keras.models.load_model(model)\n",
    "    print('Done.')\n",
    "\n",
    "# combo classifier\n",
    "print('Loading simple combo classification model... ',end='')\n",
    "combo_model_file = classification_dir + '/simple/' + 'model_simple_do20.h5'\n",
    "network_models['combo'] = tf.keras.models.load_model(combo_model_file)\n",
    "print('Done.')\n",
    "\n",
    "# energy regression networks\n",
    "print('Loading charged-pion energy regression model... ',end='')\n",
    "charged_energy_model_file = regression_dir + '/' + 'all_charged.h5'\n",
    "network_models['e_charged'] = tf.keras.models.load_model(charged_energy_model_file)\n",
    "print('Done.')\n",
    "\n",
    "print('Loading neutral-pion energy regression model... ',end='')\n",
    "neutral_energy_model_file = regression_dir + '/' + 'all_neutral.h5'\n",
    "network_models['e_neutral'] = tf.keras.models.load_model(neutral_energy_model_file)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make a \"local\" copy of the jet data. We will only copy over certain branches, and we will skip any files that don't contain an `eventTree` in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our \"local\" data dir, where we create modified data files\n",
    "jet_data_dir = path_prefix + 'jets/data'\n",
    "Path(jet_data_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if(skip_scores):\n",
    "    data_filenames = glob.glob(jet_data_dir + '/*.root')\n",
    "    \n",
    "    # debugging - take only one file, for speed\n",
    "    if(debug): data_filenames = [data_filenames[0]]\n",
    "    \n",
    "else:\n",
    "    data_filenames = glob.glob(data_dir + '/' + '*.root')\n",
    "\n",
    "    # debugging - lets us use a single file to speed stuff up a lot.\n",
    "    if(debug): data_filenames = [data_dir + '/' + 'user.angerami.21685345.OutputStream._000062.root']\n",
    "\n",
    "    # Get the original data.\n",
    "    files = {name:rt.TFile(name,'READ') for name in data_filenames}\n",
    "\n",
    "    # Some data files might be missing an EventTree.\n",
    "    # For now, we will skip these because our methods count on an existing EventTree.\n",
    "    delete_keys = []\n",
    "    for key, val in files.items():\n",
    "        file_keys = [x.GetName() for x in val.GetListOfKeys()]\n",
    "        if('ClusterTree' not in file_keys or 'EventTree' not in file_keys):\n",
    "            delete_keys.append(key)\n",
    "\n",
    "    for key in delete_keys: \n",
    "        print('Ignoring file:',key,'(no EventTree/ClusterTree found).')\n",
    "        del files[key]\n",
    "\n",
    "    # now we make a local copy of the files in the jet_data_dir, keeping only certain branches\n",
    "    active_branches = {}\n",
    "    active_branches['cluster'] = [\n",
    "        'runNumber',\n",
    "        'eventNumber',\n",
    "        'truthE',\n",
    "        'truthPt',\n",
    "        'truthEta',\n",
    "        'truthPhi',\n",
    "        'clusterIndex',\n",
    "        'nCluster',\n",
    "        'clusterE',\n",
    "        'clusterECalib',\n",
    "        'clusterPt',\n",
    "        'clusterEta',\n",
    "        'clusterPhi',\n",
    "        'cluster_nCells',\n",
    "        'cluster_ENG_CALIB_TOT',\n",
    "        'EMB1',\n",
    "        'EMB2',\n",
    "        'EMB3',\n",
    "        'TileBar0',\n",
    "        'TileBar1',\n",
    "        'TileBar2'\n",
    "    ]\n",
    "    active_branches['event'] = [\n",
    "        'runNumber',\n",
    "        'eventNumber',\n",
    "        'lumiBlock',\n",
    "        'NPV',\n",
    "        'nTruthPart',\n",
    "        'clusterCount',\n",
    "        'nCluster',\n",
    "        'clusterE',\n",
    "        'clusterPt',\n",
    "        'clusterEta',\n",
    "        'clusterPhi',\n",
    "        'AntiKt4EMTopoJetsPt',\n",
    "        'AntiKt4EMTopoJetsEta',\n",
    "        'AntiKt4EMTopoJetsPhi',\n",
    "        'AntiKt4EMTopoJetsE',\n",
    "        'AntiKt4LCTopoJetsPt',\n",
    "        'AntiKt4LCTopoJetsEta',\n",
    "        'AntiKt4LCTopoJetsPhi',\n",
    "        'AntiKt4LCTopoJetsE',\n",
    "        'AntiKt4TruthJetsPt',\n",
    "        'AntiKt4TruthJetsEta',\n",
    "        'AntiKt4TruthJetsPhi',\n",
    "        'AntiKt4TruthJetsE'\n",
    "    ]\n",
    "\n",
    "    tree_names = {'cluster':'ClusterTree','event':'EventTree'}\n",
    "    data_filenames = []\n",
    "\n",
    "    l = len(files.keys())\n",
    "    i = 0\n",
    "    qu.printProgressBarColor(i, l, prefix='Copying data files:', suffix='Complete', length=50)\n",
    "\n",
    "    for path, tfile in files.items():\n",
    "        filename_new = jet_data_dir + '/' + path.split('/')[-1]\n",
    "        old_trees = {x:tfile.Get(tree_names[x]) for x in tree_names.keys()}\n",
    "    \n",
    "        for key, tree in old_trees.items():\n",
    "            tree.SetBranchStatus('*',0)\n",
    "            for bname in active_branches[key]: tree.SetBranchStatus(bname,1)\n",
    "    \n",
    "        tfile_new = rt.TFile(filename_new,'RECREATE')\n",
    "        new_trees = {x:old_trees[x].CloneTree() for x in old_trees.keys()}\n",
    "        tfile_new.Write()\n",
    "        data_filenames.append(filename_new)\n",
    "        i += 1\n",
    "        qu.printProgressBarColor(i, l, prefix='Copying data files:', suffix='Complete', length=50)\n",
    "        del old_trees\n",
    "        del new_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the files & trees with uproot\n",
    "tree_names = {'cluster':'ClusterTree','event':'EventTree'}\n",
    "ur_trees = {file:{tree_key:ur.open(file)[tree_name] for tree_key,tree_name in tree_names.items()} for file in data_filenames}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data.\n",
    "training_data_filenames = {'pp':training_data_dir+'/piplus.root','pm':training_data_dir+'/piminus.root','p0':training_data_dir+'/pi0.root'}\n",
    "training_trees = {key:ur.open(val)['ClusterTree'] for key, val in training_data_filenames.items()}\n",
    "training_branches = ['clusterE', 'clusterEta', 'cluster_ENG_CALIB_TOT']\n",
    "training_frames = {key:val.pandas.df(training_branches,flatten=False) for key,val in training_trees.items()}\n",
    "\n",
    "# We applied a lower cut on cluster_ENG_CALIB_TOT, as very low-energy clusters can throw off training.\n",
    "# We apply the same cut here for consistency, when re-deriving the scalers.\n",
    "energy_cut = 5.0e-1 # GeV\n",
    "training_indices = {}\n",
    "for key in training_frames.keys():\n",
    "    training_indices[key] = (training_frames[key]['cluster_ENG_CALIB_TOT'] > energy_cut).to_numpy()\n",
    "    training_frames[key] = training_frames[key][training_indices[key]]\n",
    "\n",
    "# Combining dataframes. Charged pions will be under 'pp', neutral will be under 'p0'.\n",
    "training_frames['pp'] = training_frames['pp'].append(training_frames['pm'])\n",
    "del training_frames['pm']\n",
    "\n",
    "for key,frame in training_frames.items():\n",
    "    frame['logE']      = np.log(frame['clusterE']             )\n",
    "    frame['logECalib'] = np.log(frame['cluster_ENG_CALIB_TOT'])\n",
    "\n",
    "# Create scalers.\n",
    "scaler_e   = {key:StandardScaler() for key in training_frames.keys()}\n",
    "scaler_cal = {key:StandardScaler() for key in training_frames.keys()}\n",
    "scaler_eta = {key:StandardScaler() for key in training_frames.keys()}\n",
    "\n",
    "# Fit the scalers.\n",
    "for key, frame in training_frames.items():\n",
    "    scaler_e[key].fit(  frame['logE'      ].to_numpy().reshape(-1,1))\n",
    "    scaler_cal[key].fit(frame['logECalib' ].to_numpy().reshape(-1,1))\n",
    "    scaler_eta[key].fit(frame['clusterEta'].to_numpy().reshape(-1,1))\n",
    "\n",
    "# Relabel the keys of our scalers, so they make more sense below.\n",
    "# 'pp' -> 'charged'\n",
    "# 'p0' -> 'neutral'\n",
    "for scaler in [scaler_e, scaler_cal, scaler_eta]:\n",
    "    scaler['charged'] = scaler['pp']\n",
    "    del scaler['pp']\n",
    "    scaler['neutral'] = scaler['p0']\n",
    "    del scaler['p0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the training data objects, we do not need them any longer.\n",
    "del training_trees\n",
    "del training_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting network outputs for all clusters\n",
    "\n",
    "Now we will loop over our data files, and get network scores (classification and predicted energies) for all clusters. Note that the latter involves *scaling* of the data, which we will achieve using the scalers that we extracted from the training data above.\n",
    "\n",
    "This isn't the most notebook-esque code, as we're preparing a bunch of inputs *within* the big for loop below (and not saving them or printing them) but it should avoid \"out of memory\" issues: As we are dealing with a large amount of data, preparing all the data in memory before operating on it will result in very high memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# branch buffer for filling our score trees\n",
    "    # make our branch buffer\n",
    "branch_buffer = {\n",
    "    'charged_likelihood_combo': np.zeros(1,dtype=np.dtype('f8')),\n",
    "    'clusterE_charged': np.zeros(1,dtype=np.dtype('f8')),\n",
    "    'clusterE_neutral': np.zeros(1,dtype=np.dtype('f8'))\n",
    "}\n",
    "\n",
    "# Name for the tree that will contain network scores.\n",
    "tree_name = 'ScoreTree'\n",
    "\n",
    "for dfile, trees in ur_trees.items():\n",
    "    \n",
    "    if(skip_scores): \n",
    "        # Explicitly check if ScoreTree is present, otherwise we recompute.\n",
    "        # Useful if score computation was previously interrupted.\n",
    "        file_keys = [str(x,'utf-8') for x in list(ur.open(dfile).keys())]\n",
    "        skip = [tree_name in fkey for fkey in file_keys]\n",
    "        if(True in skip): continue    \n",
    "        \n",
    "    print ('File:',dfile)\n",
    "    # Prepare the calo images.\n",
    "    print('\\tPrepping calo images...')\n",
    "    calo_images = {}\n",
    "    for layer in layers:\n",
    "        calo_images[layer] = mu.setupCells(trees['cluster'],layer)\n",
    "    combined_images = np.concatenate(tuple([calo_images[layer] for layer in layers]), axis=1)\n",
    "\n",
    "    # Prepare some extra combined input for the energy regressions.\n",
    "    print('\\tPrepping extra inputs...')\n",
    "    \n",
    "    e = trees['cluster'].array('clusterE')\n",
    "    e_calib = trees['cluster'].array('cluster_ENG_CALIB_TOT')\n",
    "    eta = trees['cluster'].array('clusterEta')\n",
    "    \n",
    "    # cleaning for e_calib (empirically needed for e_calib to remove values that are too small)\n",
    "    epsilon = energy_cut #1.0e-12 # TODO: Should I set this to energy_cut? Would that make sense?\n",
    "    e_calib = np.where(e_calib < epsilon, epsilon, e_calib)\n",
    "    \n",
    "    s_combined,scaler_combined = mu.standardCells(combined_images, layers) # Note: scaler_combined is unused\n",
    "    \n",
    "    regression_input = {}\n",
    "    for key in scaler_e.keys():\n",
    "        regression_cols = {}\n",
    "        regression_cols['s_logE'] = scaler_e[key].transform(np.log(e).reshape(-1,1))\n",
    "        regression_cols['s_eta'] = scaler_eta[key].transform(eta.reshape(-1,1))\n",
    "        regression_input[key] = np.column_stack((regression_cols['s_logE'], regression_cols['s_eta'],s_combined))\n",
    "\n",
    "    # now find network scores\n",
    "    print('\\tCalculating network outputs...')\n",
    "    model_scores = {}\n",
    "    \n",
    "    print('\\t\\tClassification... ', end='')\n",
    "    # 1) flat networks\n",
    "    for layer in flat_model_names:\n",
    "        model = network_models[layer]\n",
    "        model_scores[layer] = model.predict(calo_images[layer])[:,1] # [:,1] based on Max's code, this is input to combo network. Likelihood of being charged (vs. neutral)\n",
    "    \n",
    "    # 2) combo network\n",
    "    name = 'combo'\n",
    "    model = network_models[name]\n",
    "    input_scores = np.column_stack([model_scores[layer] for layer in layers])\n",
    "    model_scores[name] = model.predict(input_scores)[:,1] # likelihood of being charged pion (versus neutral pion)\n",
    "    print('Done.')\n",
    "    \n",
    "    print('\\t\\tRegression... ', end='')\n",
    "    # 3) energy regression networks\n",
    "    name = 'e_charged'\n",
    "    model = network_models[name]\n",
    "    model_scores[name] = np.exp(scaler_cal['charged'].inverse_transform(model.predict(regression_input['charged'])))\n",
    "    \n",
    "    name = 'e_neutral'\n",
    "    model = network_models[name]\n",
    "    model_scores[name] = np.exp(scaler_cal['neutral'].inverse_transform(model.predict(regression_input['neutral'])))\n",
    "    print('Done.')\n",
    "    \n",
    "    # Now we should save these scores to a new tree.\n",
    "    f = rt.TFile(dfile, 'UPDATE')\n",
    "    t = rt.TTree(tree_name, tree_name)\n",
    "    \n",
    "    print('Saving network scores to tree ' + tree_name + '... ',end='')    \n",
    "    # --- Setup the branches using our buffer. This is a rather general/flexible code block. ---\n",
    "    branches = {}\n",
    "    for bname, val in branch_buffer.items():\n",
    "        descriptor = bname\n",
    "        bshape = val.shape\n",
    "        if(bshape != (1,)):\n",
    "            for i in range(len(bshape)):\n",
    "                descriptor += '[' + str(bshape[i]) + ']'\n",
    "        descriptor += '/'\n",
    "        if(val.dtype == np.dtype('i2')): descriptor += 'S'\n",
    "        elif(val.dtype == np.dtype('i4')): descriptor += 'I'\n",
    "        elif(val.dtype == np.dtype('i8')): descriptor += 'L'\n",
    "        elif(val.dtype == np.dtype('f4')): descriptor += 'F'\n",
    "        elif(val.dtype == np.dtype('f8')): descriptor += 'D'\n",
    "        else:\n",
    "            print('Warning, setup issue for branch: ', key, '. Skipping.')\n",
    "            continue\n",
    "        branches[bname] = t.Branch(bname,val,descriptor)\n",
    "    \n",
    "    # Fill the model score tree, and save it to the local data file.\n",
    "    nentries = model_scores['combo'].shape[0]\n",
    "    for i in range(nentries):\n",
    "        branch_buffer['charged_likelihood_combo'][0] = model_scores['combo'][i]\n",
    "        branch_buffer['clusterE_charged'][0] = model_scores['e_charged'][i]\n",
    "        branch_buffer['clusterE_neutral'][0] = model_scores['e_neutral'][i]\n",
    "        t.Fill()\n",
    "    \n",
    "    t.Write(tree_name, rt.TObject.kOverwrite)\n",
    "    f.Close()\n",
    "    print('Done.')\n",
    "    \n",
    "tree_names['score'] = tree_name\n",
    "ur_trees = {file:{tree_key:ur.open(file)[tree_name] for tree_key,tree_name in tree_names.items()} for file in data_filenames}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on to jet clustering, we can already check to see if our energy regressions seem sensible. Let's make distributions of:\n",
    "- The classification score\n",
    "- Each regressed energy, for **all** clusters (i.e. charged and neutral energy regressions for all clusters regardless of their classifications)\n",
    "- Regressed energy / reco energy, where we choose the regressed energy for each cluster based on its classification score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = rt.TCanvas(str(uuid.uuid4()),'network checks',800,1800)\n",
    "c.Divide(1,4)\n",
    "\n",
    "# classification scores\n",
    "class_hist = rt.TH1F(str(uuid.uuid4()), 'Classification score (charged likelihood);Score;Count',100,0.,1.)\n",
    "for dfile, trees in ur_trees.items():\n",
    "    for score in ur_trees[dfile]['score'].array('charged_likelihood_combo'): class_hist.Fill(score)\n",
    "class_hist.SetFillColorAlpha(rt.kGreen,0.7)\n",
    "class_hist.SetLineColor(rt.kGreen)\n",
    "c.cd(1)\n",
    "class_hist.Draw('HIST')\n",
    "rt.gPad.SetLogy()\n",
    "\n",
    "# energy regressions - for all clusters (score-agnostic)\n",
    "\n",
    "true_e_hist = rt.TH1F(str(uuid.uuid4()), 'E_{CALIB}^{TOT} (all clusters);E_{CALIB}^{TOT} [GeV];Count',60,0.,60.)\n",
    "charged_hist = rt.TH1F(str(uuid.uuid4()), 'Predicted Charged Energy (all clusters);Energy [GeV];Count',60,0.,60.)\n",
    "neutral_hist = rt.TH1F(str(uuid.uuid4()), 'Predicted Neutral Energy (all clusters);Energy [GeV];Count',60,0.,60.)\n",
    "\n",
    "for dfile, trees in ur_trees.items():\n",
    "    for energy in ur_trees[dfile]['cluster'].array('cluster_ENG_CALIB_TOT'): true_e_hist.Fill(energy)\n",
    "    for energy in ur_trees[dfile]['score'].array('clusterE_charged'): charged_hist.Fill(energy)\n",
    "    for energy in ur_trees[dfile]['score'].array('clusterE_neutral'): neutral_hist.Fill(energy)\n",
    "\n",
    "true_e_hist.SetFillColorAlpha(rt.kGreen,0.5)\n",
    "true_e_hist.SetLineColor(rt.kGreen)  \n",
    "\n",
    "charged_hist.SetFillColorAlpha(rt.kBlue,0.7)\n",
    "charged_hist.SetLineColor(rt.kBlue)  \n",
    "\n",
    "neutral_hist.SetFillColorAlpha(rt.kRed,0.7)\n",
    "neutral_hist.SetLineColor(rt.kRed) \n",
    "\n",
    "c.cd(2)\n",
    "leg1 = rt.TLegend(0.7,0.6,0.9,0.9)\n",
    "leg1.SetTextSize(0.06)\n",
    "leg1.SetTextColor(plot_style.text)\n",
    "leg1.AddEntry(charged_hist,'E_{pred}^{#pm}','f')\n",
    "leg1.AddEntry(true_e_hist,'E_{CALIB}^{TOT}','f')\n",
    "\n",
    "charged_hist.Draw('HIST')\n",
    "true_e_hist.Draw('HIST SAME')\n",
    "leg1.Draw()\n",
    "rt.gPad.SetLogy()\n",
    "\n",
    "c.cd(3)\n",
    "leg2 = rt.TLegend(0.7,0.6,0.9,0.9)\n",
    "leg2.SetTextSize(0.06)\n",
    "leg2.SetTextColor(plot_style.text)\n",
    "leg2.AddEntry(neutral_hist,'E_{pred}^{0}','f')\n",
    "leg2.AddEntry(true_e_hist,'E_{CALIB}^{TOT}','f')\n",
    "\n",
    "neutral_hist.Draw('HIST')\n",
    "true_e_hist.Draw('HIST SAME')\n",
    "leg2.Draw()\n",
    "rt.gPad.SetLogy()\n",
    "\n",
    "# regressed energy (most likely) / calibration hits\n",
    "zero_energies = 0\n",
    "n_tot = 0\n",
    "classification_threshold = 0.5\n",
    "energy_ratio_hist = rt.TH1F(str(uuid.uuid4()), 'Predicted Energy / E_{CALIB}^{TOT};E_{pred} / E_{CALIB}^{TOT};Count',10000,1.0e-4,1.0e2)\n",
    "for dfile, trees in ur_trees.items():\n",
    "    scores = trees['score'].array('charged_likelihood_combo')\n",
    "    charged_e = trees['score'].array('clusterE_charged')\n",
    "    neutral_e = trees['score'].array('clusterE_neutral')\n",
    "    true_e = trees['cluster'].array('cluster_ENG_CALIB_TOT')\n",
    "    n_tot += len(true_e)\n",
    "    \n",
    "    for i in range(len(scores)):\n",
    "        if(true_e[i] == 0.):\n",
    "            zero_energies += 1\n",
    "            continue\n",
    "        if(scores[i] > 0.5): energy_ratio_hist.Fill(charged_e[i] / true_e[i])\n",
    "        else: energy_ratio_hist.Fill(neutral_e[i] / true_e[i])\n",
    "\n",
    "energy_ratio_hist.SetFillColorAlpha(rt.kViolet,0.7)\n",
    "energy_ratio_hist.SetLineColor(rt.kViolet)\n",
    "c.cd(4)\n",
    "energy_ratio_hist.Draw('HIST')\n",
    "rt.gPad.SetLogx()\n",
    "rt.gPad.SetLogy()\n",
    "energy_ratio_hist.GetXaxis().SetRangeUser(1.0e-4, 1.0e2)\n",
    "\n",
    "rt.gStyle.SetOptStat(0)\n",
    "\n",
    "print('Number of clusters with ENG_CALIB_TOT == 0: {val1:.2e} ({val2:.2f}% of clusters)'.format(val1 = zero_energies, val2 = 100. * zero_energies / n_tot))\n",
    "\n",
    "c.SaveAs(plot_dir + '/' + 'cluster_plots.png')\n",
    "c.Draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots above, we see that things are *OK* in that our predicted energies fall within the same range as the targeted ones. However, as the differences between energy distributions (and the last plot, in purple) show, there are an appreciable number of events where our predicted energy is off by and order of magnitude or so. Note that, for the last plot, we're using a classification threshold of $0.5$ (the point in the classifier output where we make the split between topo-clusters classified as charged or neutral). There may be a more optimal choice of cut, but I think it's unlikely that small tweaks to this will totally eliminate the issue we see here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jet clustering\n",
    "\n",
    "Now, we want to perform jet-clustering, where we'll use the regressed energies (and the classification score will tell us which regressed energy to use for each cluster).\n",
    "\n",
    "First, let's set up some information on cuts we want to make. \n",
    "- We will have a *global* jet $eta$ cut, so that we only consider jets within a certain $eta$ window. We do this as we only have topo-cluster images for topo-clusters with $|\\eta| < 0.7$, so we will see some edge effects if performing clustering near that boundary.\n",
    "- We will apply a *minimum energy cut* on our truth jets. When performing jet matching, we will only be matching reco jets to truth jets that pass this cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_eta_cut = 0.3 # eta cut to be applied to all jets -- those we make and those we're given\n",
    "global_truth_e_cut = 25. # GeV -- recall that jet energies are stored in MeV!\n",
    "\n",
    "# pavetext with info on cuts\n",
    "cut_info = [\n",
    "    '|#eta_{j}| <' + ' {val:.1f},'.format(val=global_eta_cut),\n",
    "    'E_{j}^{true}' + ' > {val:.0f} [GeV],'.format(val=global_truth_e_cut),\n",
    "    'All reco jets matched',\n",
    "    'to truth w/ #Delta R < 0.3 .'\n",
    "]\n",
    "\n",
    "cut_pave = rt.TPaveText(0.675, 0.5, 0.875, 0.7, 'NDC')\n",
    "cut_pave.SetFillColorAlpha(plot_style.canv,0.1)\n",
    "cut_pave.SetBorderSize(0)\n",
    "cut_pave.SetTextColor(plot_style.text)\n",
    "cut_pave.SetTextFont(42)\n",
    "cut_pave.SetTextSize(0.03)\n",
    "cut_pave.SetTextAlign(12)\n",
    "for line in cut_info: \n",
    "    cut_pave.AddText(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform our jet clustering employing our regressed topo-cluster energies, and save these new jets as `AntiKt4MLTopoJets` in a new tree in each of our files, `JetTree`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 0.4\n",
    "pt_min = 0.\n",
    "eta_max = global_eta_cut\n",
    "tree_name = 'JetTree'\n",
    "\n",
    "ju.ClusterJets(ur_trees, \n",
    "               'AntiKt4MLTopoJets',\n",
    "               R=R, \n",
    "               pt_min = pt_min, \n",
    "               eta_max = global_eta_cut, \n",
    "               fj_dir = fj_dir, \n",
    "               classification_threshold = classification_threshold,\n",
    "               tree_name = 'JetTree'\n",
    ")\n",
    "\n",
    "# update our uproot tree access dictionary, adding our new tree!\n",
    "tree_names['jet'] = tree_name\n",
    "ur_trees = {file:{tree_key:ur.open(file)[tree_name] for tree_key,tree_name in tree_names.items()} for file in data_filenames}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jet matching\n",
    "\n",
    "Now, we want to match the jets we just clustered with the truth jets, to see how well we've reconstructed things.\n",
    "\n",
    "Here's how we will perform jet-matching:\n",
    "\n",
    "- Get the list of all reco jets and truth jets for an event\n",
    "- Loop through the truth jets\n",
    "    - Find the closest reco jet within $\\Delta R=0.3$, if it exists, and call it a match\n",
    "        - If we fail to find a match, make a note of this\n",
    "    - Take the matched reco jet off the list, so we don't match it a 2nd time\n",
    "\n",
    "We will perform this process for all the different reco jet definitions in the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the reco jet matching information to a new tree. A few notes on this information:\n",
    "\n",
    "-  For each reconstruction-level jet type (EM, LC, ML), we will save a vector of ints for each event. The vector is of the same length as the number of reco jets in that event, and each position will give the index of the truth jet to which that reco jet is matched. Unmatched reco jets will be represented in this vector with a $-1$.\n",
    "- Any cuts applied to reco jets should be built-in here. If a reco jet fails to pass a reco jet cut, we won't even bother trying to match it (and will label it as unmatched).\n",
    "- Any cuts on the truth jets should also be built-in here. For example, if we're only considering truth jets with energy > 25 GeV, we are only interested in reco jets that are matched to truth jets that pass this cut. So, we will use that cut when applying the matching, versus just matching any and all jets and worrying about this cut later on. This should make plotting kinematics easier later on, with the caveat that we cannot look at reco jets matched to truth jets that *do not* pass our truth jet cuts, without redoing this step with those cuts removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info on the different jet definitions, and the keys of the trees with which they're associated\n",
    "jet_defs = {\n",
    "    'EM':('event', 'AntiKt4EMTopoJets'),\n",
    "    'LC':('event', 'AntiKt4LCTopoJets'),\n",
    "    'ML':('jet',   'AntiKt4MLTopoJets'),\n",
    "    'Truth':('event', 'AntiKt4TruthJets')\n",
    "}\n",
    "\n",
    "# explicitly listing the reco jets from above\n",
    "reco_jet_defs = {\n",
    "    'EM':('event', 'AntiKt4EMTopoJets'),\n",
    "    'LC':('event', 'AntiKt4LCTopoJets'),\n",
    "    'ML':('jet',   'AntiKt4MLTopoJets')\n",
    "}\n",
    "\n",
    "# colors, for plotting purposes\n",
    "colors = {\n",
    "    'Truth': rt.kViolet,\n",
    "    'EM': rt.kGreen,\n",
    "    'LC': rt.kRed,\n",
    "    'ML': rt.kBlue\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform jet matching\n",
    "\n",
    "R = 0.3 # matching radius\n",
    "tree_name = 'JetMatchTree'\n",
    "\n",
    "ju.MatchRecoJets(ur_trees,\n",
    "                 jet_defs = jet_defs, \n",
    "                 R = R,\n",
    "                 eta_max = global_eta_cut, # redundant, this was applied during jet clustering itself\n",
    "                 truth_e_min = global_truth_e_cut,\n",
    "                 tree_name = tree_name\n",
    "                )\n",
    "   \n",
    "# update our uproot tree access dictionary, adding our new tree!\n",
    "tree_names['jet_match'] = tree_name\n",
    "ur_trees = {file:{tree_key:ur.open(file)[tree_name] for tree_key,tree_name in tree_names.items()} for file in data_filenames}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot energy ratios\n",
    "results = ju.PlotEnergyRatio(ur_trees,\n",
    "                             reco_jet_defs = reco_jet_defs,\n",
    "                             colors = colors,\n",
    "                             truth_jet_def='AntiKt4TruthJets',\n",
    "                             match_key='jet_match',\n",
    "                             min_ratio = 0.,\n",
    "                             max_ratio = 10.,\n",
    "                             nbins = 100,\n",
    "                             paves = [cut_pave],\n",
    "                             plot_dir = plot_dir\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the stuff we're seeing above looks weird. Many of the cluster energies are too low, but there's also a very large tail to the distribution.\n",
    "\n",
    "But even with this rescaling having been done, we see issues such as in the plot above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jet Kinematic distributions\n",
    "\n",
    "To get a better sense of what our data looks like, let's produce some kinematic plots for all flavors of jets. We'll see how the different jet definitions' kinematics compare, and if something is off with our ML jets.\n",
    "\n",
    "For our reco jets, we will only be considering those that have been matched with truth jets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ju.PlotJetKinematics(ur_trees,\n",
    "                               jet_defs = jet_defs,\n",
    "                               colors = colors,\n",
    "                               plot_dir = plot_dir,\n",
    "                               eta_max = global_eta_cut,         # used for cut on truth jets, reco jets will already have cuts applied via matching\n",
    "                               truth_e_min = global_truth_e_cut, # used for cut on truth jets, reco jets will already have cuts applied via matching\n",
    "                               paves = [cut_pave],\n",
    "                               logx = [] # names of variables to be plotted with log scale on x axis, see jet_util.py for name conventions\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The energies for the ML jets don't look great -- as we might expect, the issues with the cluster energies (mostly being way too small, and a few being way too large) is carrying over to jet clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying jet clustering\n",
    "\n",
    "For completeness, we should make sure that the jet clustering above is working properly at all. To do this, we can try to reproduce the EM jets, which just use the `clusterE` branch for their topo-cluster energies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reproducing EM jets\n",
    "\n",
    "This code will look a lot like our jet clustering above, but we will be using the default reco energy. We'll save our new EM jets to a tree called `JetTree_EM`.\n",
    "\n",
    "Note that we *will* apply a $5$ GeV minimum $p_T$ cut to the jets that we produce, as we can see that this was applied to the original EM jets and we're trying to reproduce those results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 0.4\n",
    "pt_min = 5. # GeV\n",
    "eta_max = global_eta_cut\n",
    "tree_name = 'JetTree_EM'\n",
    "\n",
    "ju.ClusterJets(ur_trees, \n",
    "               'AntiKt4EMTopoJets',\n",
    "               R=R, \n",
    "               pt_min = pt_min, \n",
    "               eta_max = global_eta_cut, \n",
    "               fj_dir = fj_dir, \n",
    "               tree_name = tree_name,\n",
    "               energy_tree_key = 'cluster',\n",
    "               energy_branch = 'clusterE'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update our uproot tree access dictionary, adding our new tree!\n",
    "tree_names['jet_em'] = tree_name\n",
    "ur_trees = {file:{tree_key:ur.open(file)[tree_name] for tree_key,tree_name in tree_names.items()} for file in data_filenames}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We *also* need to match our new `EM2` jets to truth jets, for a fair comparison with the `EM` jets. We will save the matching info to `JetMatchTree_EM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info on the different jet definitions, and the keys of the trees with which they're associated\n",
    "jet_defs = {\n",
    "    'EM':('jet_em', 'AntiKt4EMTopoJets'),\n",
    "    'Truth':('event', 'AntiKt4TruthJets')\n",
    "}\n",
    "R = 0.3 # matching radius\n",
    "tree_name = 'JetMatchTree_EM'\n",
    "\n",
    "ju.MatchRecoJets(ur_trees, \n",
    "              jet_defs = jet_defs, \n",
    "              R = R,\n",
    "              eta_max = global_eta_cut, # redundant, this was applied during jet clustering itself\n",
    "              truth_e_min = global_truth_e_cut,\n",
    "              tree_name = tree_name\n",
    "             )\n",
    "   \n",
    "# update our uproot tree access dictionary, adding our new tree!\n",
    "tree_names['jet_match_em'] = tree_name\n",
    "ur_trees = {file:{tree_key:ur.open(file)[tree_name] for tree_key,tree_name in tree_names.items()} for file in data_filenames}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the kinematic distributions of our new EM jets and the old EM jets. Our hope is that they match. Note that we might expect to find some lower $p_T$ jets too, as it looks like a $p_T$ cut was applied to the original EM jets and we aren't necessarily applying one here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_defs = {\n",
    "    'EM':('event', 'AntiKt4EMTopoJets'),\n",
    "    'EM2':('jet_em', 'AntiKt4EMTopoJets'),\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    'EM': rt.kGreen,\n",
    "    'EM2': rt.kBlue\n",
    "}\n",
    "\n",
    "scale_factors = 0.001 # jet info is in MeV, we want to plot it all in GeV\n",
    "energy_hists = {key:rt.TH1F(str(uuid.uuid4()), key + ' Jets;Energy [GeV];Count', 30, 0., 150.) for key in jet_defs.keys()}\n",
    "pt_hists     = {key:rt.TH1F(str(uuid.uuid4()), key + ' Jets;p_{T} [GeV];Count', 30, 0., 150.) for key in jet_defs.keys()}\n",
    "eta_hists    = {key:rt.TH1F(str(uuid.uuid4()), key + ' Jets;#eta;Count', 50, -1., 1.) for key in jet_defs.keys()}\n",
    "m_hists      = {key:rt.TH1F(str(uuid.uuid4()), key + ' Jets;m [GeV];Count', 55, -5., 50.) for key in jet_defs.keys()}\n",
    "ep_hists     = {key:rt.TH1F(str(uuid.uuid4()), key + ' Jets;Energy / p_{T};Count', 90, 0.9, 1.2) for key in jet_defs.keys()}\n",
    "n_hists      = {key:rt.TH1I(str(uuid.uuid4()), key + ' Jets;N_{jets};Count', 5, 0, 5) for key in jet_defs.keys()}\n",
    "\n",
    "vec = rt.Math.PtEtaPhiEVector()\n",
    "for dfile, tree in ur_trees.items():\n",
    "    for key, jet_def in jet_defs.items():\n",
    "        tkey  = jet_def[0]\n",
    "        jname = jet_def[1]\n",
    "        \n",
    "        # Truth jets -- apply global jet cuts, and truth-specific jet cuts.\n",
    "        if(key == 'Truth'):\n",
    "            eta = tree[tkey].array(jname + 'Eta')\n",
    "            energy = tree[tkey].array(jname + 'E')\n",
    "            # Truth jet energy cut & jet eta cut.\n",
    "            jet_indices = (np.abs(eta) <= global_eta_cut) * (energy  >= 1.0e3 * global_truth_e_cut)\n",
    "        \n",
    "        # Reco jets -- apply jet-matching cut (global jet cuts are built-in). No further reco-specific jet cuts for now.\n",
    "        else:\n",
    "            jmatch_key = 'jet_match'\n",
    "            if(key == 'EM2'): jmatch_key = 'jet_match_em'\n",
    "            matching = tree[jmatch_key].array(jname + 'Match')\n",
    "            jet_indices = (matching > -1)\n",
    "            \n",
    "        # Now get all jets that passed the cuts.\n",
    "        # First do a bit of manipulation with eta, to also get the number of jets per event.\n",
    "        eta = tree[tkey].array(jname + 'Eta')[jet_indices]\n",
    "        n   = [len(x) for x in eta]\n",
    "        eta = eta.flatten()\n",
    "        energy = scale_factors * tree[tkey].array(jname + 'E')[jet_indices].flatten()\n",
    "        pt     = scale_factors * tree[tkey].array(jname + 'Pt')[jet_indices].flatten()\n",
    "        ep     = energy / pt\n",
    "        \n",
    "        for i in range(len(n)): n_hists[key].Fill(n[i])\n",
    "            \n",
    "        for i in range(len(ep)):\n",
    "            energy_hists[key].Fill(energy[i])\n",
    "            pt_hists[key].Fill(pt[i])\n",
    "            eta_hists[key].Fill(eta[i])\n",
    "            ep_hists[key].Fill(ep[i])\n",
    "            \n",
    "            # Compute the jet mass and plot it too.\n",
    "            vec.SetCoordinates(pt[i],eta[i],0.,energy[i])\n",
    "            m_hists[key].Fill(vec.M())\n",
    "\n",
    "hist_lists = [energy_hists, pt_hists, eta_hists, ep_hists, m_hists, n_hists]\n",
    "names      = ['energy',     'pt',     'eta',     'ep',     'm',     'n']\n",
    "styles = { 'EM': 3345,  'EM2': 3354}# fill styles\n",
    "for key in jet_defs.keys():\n",
    "    for hist_list in hist_lists:\n",
    "        qu.SetColor(hist_list[key],colors[key],alpha = 0.9)\n",
    "        hist_list[key].SetFillStyle(styles[key])\n",
    "        hist_list[key].SetLineWidth(2)\n",
    "            \n",
    "legend = rt.TLegend(0.7,0.7,0.9,0.9)\n",
    "legend.SetTextColor(plot_style.text)\n",
    "legend.AddEntry(energy_hists['EM'],'EM','f')\n",
    "legend.AddEntry(energy_hists['EM2'],'EM (new)','f')\n",
    "\n",
    "rt.gStyle.SetOptStat(0)\n",
    "nx = 2\n",
    "l = len(hist_lists)\n",
    "ny = int(np.ceil(l / nx))\n",
    "c = rt.TCanvas(str(uuid.uuid4()), str(uuid.uuid4()), nx * 600, ny * 400)\n",
    "c.Divide(nx,ny)\n",
    "\n",
    "stacks = []\n",
    "for i in range(len(hist_lists)):\n",
    "    c.cd(i+1)\n",
    "    hist_list = hist_lists[i]\n",
    "    rt.gPad.SetLogy()\n",
    "    stack = rt.THStack(str(uuid.uuid4()),'')\n",
    "    #stack.SetTitle(list(hist_list.values())[0].GetTitle())\n",
    "    for key in hist_list.keys():\n",
    "        stack.Add(hist_list[key])\n",
    "    stack.Draw('NOSTACK HIST')\n",
    "    legend.Draw()\n",
    "    cut_pave.Draw()\n",
    "    stack.GetXaxis().SetTitle(list(hist_list.values())[0].GetXaxis().GetTitle())\n",
    "    stack.GetYaxis().SetTitle(list(hist_list.values())[0].GetYaxis().GetTitle())\n",
    "    stack.SetMinimum(5.0e-1)\n",
    "    stacks.append(stack)\n",
    "c.Draw()\n",
    "c.SaveAs(plot_dir + '/' + 'verification_em.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do see some small differences in distributions. For example, the smallest non-zero bin in the energy and $p_T$ distributions. \n",
    "\n",
    "I'm not entirely sure how to account for this difference: We are applying our jet-matching criteria and cuts to *both* sets of jets being plotted here -- the EM jets that we were given and the ones that we re-clustered ourselves. Looking through the clustering code we have, the logic seems pretty sound to me and it looks like it almost perfectly reproduces the distributions. \n",
    "- Could edge effects be playing some small role? We don't have topo-cluster info saved for topo-clusters with $|\\eta| > 0.7$. We tried to account for this by applying a jet cut of $|\\eta_j| < 0.3$, but is it possible that the missing topo-clusters could still be having a small effect on our reclustering of EM jets? The only alternative I could think of is some cut being applied to the EM jets, that I haven't incorporated when re-clustering them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding effects of overlapping topo-clusters: Looking at isolated topo-clusters\n",
    "\n",
    "From the above, we've seen that\n",
    "- our ML topo-clusters have an energy distribution that is too spread out.\n",
    "- our ML jets also have an energy distribution is very spread out.\n",
    "- our jet clustering itself appears to be functioning properly.\n",
    "\n",
    "It's possible that our ML jets aren't performing well due to some difference in conditions between the networks' training samples -- our \"pion dataset\" -- and the jet dataset that we're looking at. For example, we may be seeing a higher incidence of topo-clusters that overlap, as we are dealing with jets versus single pions. To see if this is the case, we might try to look for \"isolated topo-clusters\" -- those that are well-separated from others in $(\\eta, \\phi)$ -- and see if our energy regression is reconstructing their energies better than it's doing for the dataset as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see a distribution of the minimum $\\Delta R$ for each topo-cluster (i.e. looking for the distances to the nearest topo-cluster).\n",
    "\n",
    "**Note**: Rather than invoke `ROOT.Math.PtEtaPhiEVector` and `ROOT.Math.VectorUtil.DeltaR()`, we're going to re-implement these methods in `numpy` + `numba`, as this appears to give a big speedup when compared to just using `PyROOT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def dPhi(phi1, phi2):\n",
    "    dphi = phi2 - phi1\n",
    "    if(dphi > np.pi): dphi -= 2. * np.pi\n",
    "    elif(dphi < -np.pi): dphi += 2. * np.pi\n",
    "    return dphi\n",
    "\n",
    "@jit\n",
    "def dR(eta1,eta2,phi1,phi2):\n",
    "    return np.sqrt(np.power(eta2 - eta1, 2) + np.power(dPhi(phi1,phi2),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name for the tree where we will store minimum dR\n",
    "tree_name = 'ClusterDistanceTree'\n",
    "\n",
    "# Prepare our progress bar.\n",
    "nevents = 0\n",
    "for dfile, tree in ur_trees.items():\n",
    "    cluster_min = tree['event'].array('clusterCount')\n",
    "    nevents += len(cluster_min)\n",
    "m = 0\n",
    "l = nevents\n",
    "prefix = 'Computing minimum dR:'\n",
    "suffix = 'Complete'\n",
    "if(not skip_dR): qu.printProgressBarColor(m, l, prefix=prefix, suffix=suffix, length=50)\n",
    "for dfile, tree in ur_trees.items():\n",
    "    if(skip_dR): continue\n",
    "    # eta and phi for all clusters in file (across events)\n",
    "    eta = tree['cluster'].array('clusterEta')\n",
    "    phi = tree['cluster'].array('clusterPhi')\n",
    "\n",
    "    # event info\n",
    "    cluster_min = tree['event'].array('clusterCount')\n",
    "    cluster_max = cluster_min + tree['event'].array('nCluster') - 1\n",
    "    \n",
    "    # Prepare the new tree\n",
    "    branch_buffer = {\n",
    "        'MinDeltaR':rt.std.vector('float')()\n",
    "    }\n",
    "    f = rt.TFile(dfile,'UPDATE')\n",
    "    t = rt.TTree(tree_name, tree_name)\n",
    "    branches = {}\n",
    "    for key,val in branch_buffer.items(): branches[key] = t.Branch(key, val)\n",
    "        \n",
    "    # loop over events\n",
    "    n = len(cluster_min)\n",
    "    for i in range(n):\n",
    "        branch_buffer['MinDeltaR'].clear()\n",
    "        nCluster = cluster_max[i] - cluster_min[i] + 1\n",
    "\n",
    "        distances = np.zeros((nCluster,nCluster))\n",
    "        for j in range(nCluster):\n",
    "            for k in range(j):\n",
    "                distances[j,k] = dR(eta[j],phi[j],eta[k],phi[k])\n",
    "                distances[k,j] = distances[j,k]\n",
    "            distances[j,j] = 999. # easy way to eliminate the diagonal, which we do not want to consider.\n",
    "        \n",
    "        # Now loop through rows (or columns), pick the minimum from each.\n",
    "        min_distances = np.array([np.min(distances[j]) for j in range(nCluster)])\n",
    "        for entry in min_distances: \n",
    "            branch_buffer['MinDeltaR'].push_back(entry)\n",
    "            \n",
    "        t.Fill()\n",
    "        m += 1\n",
    "        qu.printProgressBarColor(m, l, prefix=prefix, suffix=suffix, length=50)\n",
    "    t.Write(tree_name,rt.TObject.kOverwrite)\n",
    "    f.Close()\n",
    "# update our uproot tree access dictionary, adding our new tree!\n",
    "tree_names['cluster_dist'] = tree_name\n",
    "ur_trees = {file:{tree_key:ur.open(file)[tree_name] for tree_key,tree_name in tree_names.items()} for file in data_filenames}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the distribution of the minimum $\\Delta R$ between topo-clusters, as well as the ratio of $E_\\text{pred}$ to $E_\\text{calib}^\\text{tot}$, as a function of this minimum $\\Delta R$.\n",
    "\n",
    "Note that we're looking at *all* topo-clusters in our testing data. This will potentially include clusters that were used in jets that we discarded -- because they didn't pass our cuts -- but we do not currently save the $\\text{cluster} \\leftrightarrow \\text{jet}$ mapping that would let us easily discard such clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_ranges = {\n",
    "    'dR':(64,0.,3.2),\n",
    "    'E_ratio':(50,0.01,10.)\n",
    "}\n",
    "\n",
    "# Distribution of minimum dR for topo-clusters.\n",
    "c = rt.TCanvas(str(uuid.uuid4()), 'cluster_distances', 800, 600)\n",
    "c.SetLeftMargin(0.15)\n",
    "h = rt.TH1F(str(uuid.uuid4()), 'min(#DeltaR) for topo-clusters;min(#DeltaR);Count', hist_ranges['dR'][0],hist_ranges['dR'][1],hist_ranges['dR'][2])\n",
    "h.SetLineColor(rt.kBlue)\n",
    "h.SetFillColorAlpha(rt.kBlue,0.7)\n",
    "for dfile, tree in ur_trees.items():\n",
    "    min_distances = tree['cluster_dist'].array('MinDeltaR').flatten()\n",
    "    for entry in min_distances: h.Fill(entry)\n",
    "h.Draw()\n",
    "c.Draw()\n",
    "c.SaveAs(plot_dir + '/' + 'cluster_min_dR.png')\n",
    "\n",
    "# Energy ratio as a function of minimum dR.\n",
    "# We'll also make a column-normalized version of this plot.\n",
    "h2 = rt.TH2F(str(uuid.uuid4()), 'Energy ratio vs. min(#DeltaR);min(#DeltaR);E_{pred} / E_{calib}^{tot};Count',hist_ranges['dR'][0],hist_ranges['dR'][1],hist_ranges['dR'][2],hist_ranges['E_ratio'][0],hist_ranges['E_ratio'][1],hist_ranges['E_ratio'][2])\n",
    "h3 = rt.TH2F(str(uuid.uuid4()), 'Energy ratio vs. min(#DeltaR) (column-normalized);min(#DeltaR);E_{pred} / E_{calib}^{tot};% Count',hist_ranges['dR'][0],hist_ranges['dR'][1],hist_ranges['dR'][2],hist_ranges['E_ratio'][0],hist_ranges['E_ratio'][1],hist_ranges['E_ratio'][2])\n",
    "\n",
    "for dfile, tree in ur_trees.items():\n",
    "    min_distances = tree['cluster_dist'].array('MinDeltaR'                       ).flatten()\n",
    "    scores        = tree['score'       ].array('charged_likelihood_combo'        ).flatten()\n",
    "    charged_e     = tree['score'       ].array('clusterE_charged'                ).flatten()\n",
    "    neutral_e     = tree['score'       ].array('clusterE_neutral'                ).flatten()\n",
    "    eng_calib_tot = tree['cluster'     ].array('cluster_ENG_CALIB_TOT'           ).flatten()\n",
    "    \n",
    "    # Get predicted energies, using classification.\n",
    "    e_ratio = np.copy(charged_e)\n",
    "    e_ratio[scores < classification_threshold] = neutral_e[scores < classification_threshold]\n",
    "    e_ratio[eng_calib_tot == 0.] = 0.\n",
    "    eng_calib_tot[eng_calib_tot == 0.] = 1.\n",
    "    e_ratio = e_ratio / eng_calib_tot\n",
    "    \n",
    "    n = len(scores)\n",
    "    for i in range(n): \n",
    "        h2.Fill(min_distances[i],e_ratio[i])\n",
    "        h3.Fill(min_distances[i],e_ratio[i])\n",
    "        \n",
    "# We will normalize one of these plots, so that each column of bins sums to 1.\n",
    "# To do this, we get the 2D histogram's profile along X. This may not necessarily\n",
    "# match our 1D histogram from above, as our given E_ratio range may cause some things\n",
    "# to land in underflow/overflow.\n",
    "projx = h3.ProjectionX()\n",
    "    \n",
    "for i in range(projx.GetNbinsX()):\n",
    "    integral = projx.GetBinContent(i+1)\n",
    "    for j in range(h3.GetNbinsY()):\n",
    "        h3.SetBinContent(i+1,j+1, 100. * h3.GetBinContent(i+1,j+1) / integral)\n",
    "\n",
    "# We also want to mark where the mean is for each column.\n",
    "profx = h2.ProfileX()\n",
    "profx.SetLineColor(rt.kRed)\n",
    "\n",
    "markers = []\n",
    "for i in range(profx.GetNbinsX()):\n",
    "    x = profx.GetBinCenter(i+1)\n",
    "    y = profx.GetBinContent(i+1)\n",
    "    r = 0.25 * profx.GetBinWidth(i+1)\n",
    "    marker = rt.TEllipse(x,y,r)\n",
    "    marker.SetLineColor(rt.kRed)\n",
    "    marker.SetFillColor(rt.kRed)\n",
    "    markers.append(marker)\n",
    "            \n",
    "c2 = rt.TCanvas(str(uuid.uuid4()), 'energy_dR', 800, 600)\n",
    "c2.SetRightMargin(0.15)\n",
    "\n",
    "c3 = rt.TCanvas(str(uuid.uuid4()), 'energy_dR', 800, 600)\n",
    "c3.SetRightMargin(0.15)\n",
    "\n",
    "legend = rt.TLegend(0.7,0.8,0.85,0.9)\n",
    "legend.AddEntry(profx,'mean','l')\n",
    "legend.SetTextColor(plot_style.text)\n",
    "\n",
    "c2.cd()\n",
    "h2.Draw('COLZ')\n",
    "profx.Draw('HIST L SAME')\n",
    "legend.Draw()\n",
    "c2.Draw()\n",
    "c2.SaveAs(plot_dir + '/' + 'cluster_min_dR_vs_energy.png')\n",
    "\n",
    "c3.cd()\n",
    "h3.Draw('COLZ')\n",
    "profx.Draw('HIST L SAME')\n",
    "legend.Draw()\n",
    "c3.Draw()\n",
    "c3.SaveAs(plot_dir + '/' + 'cluster_min_dR_vs_energy_norm.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be a strong correlation between the ratio of a topo-cluster's predicted energy to its calibration hits, and the minimum distance to the nearest topo-cluster. We could consider looking at the median but I don't think this will be enlightening.\n",
    "\n",
    "Note that I've excluded the zero bin on the y-axis, as this contains a very large number of entries corresponding to the case of $E_\\text{calib}^\\text{tot} = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at topo-cluster kinematics for training (pion gun) and testing (jet) samples.\n",
    "\n",
    "In our default scenario, we use our single-pion data for training (we also have the option of using jet data itself, though its labeling is questionable so it's really just \"facsimile\" training data). It's worthwhile to look at differences in topo-cluster kinematics between this training sample, and the jet data on which we evaluate the network. We can potentially use any differences in kinematics to re-weight our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_kinematics = {}\n",
    "\n",
    "ranges = {\n",
    "    'pt':(120,0.,12.),\n",
    "    'e':(120,0.,12.),\n",
    "    'eta':(40,-1.,1.),\n",
    "    'phi':(40,0., np.pi),\n",
    "    'eng_calib_tot':(125,-0.5,12.)\n",
    "}\n",
    "\n",
    "titles = {\n",
    "    'pt':';p_{T} [GeV];% Count',\n",
    "    'e':';E [GeV];% Count',\n",
    "    'eta':';#eta;% Count',\n",
    "    'phi':';#phi;% Count',\n",
    "    'eng_calib_tot':';E_{calib}^{tot};% Count'\n",
    "}\n",
    "\n",
    "branches = {\n",
    "    'pt':'clusterPt',\n",
    "    'e':'clusterE',\n",
    "    'eta':'clusterEta',\n",
    "    'phi':'clusterPhi',\n",
    "    'eng_calib_tot':'cluster_ENG_CALIB_TOT'\n",
    "}\n",
    "\n",
    "color = {\n",
    "    'train': rt.kBlue,\n",
    "    'test': rt.kRed\n",
    "}\n",
    "\n",
    "log = {\n",
    "    'pt':True,\n",
    "    'e':True,\n",
    "    'eta':False,\n",
    "    'phi':False,\n",
    "    'eng_calib_tot':True\n",
    "}\n",
    "\n",
    "assert(set(ranges.keys()) == set(titles.keys()))\n",
    "assert(set(ranges.keys()) == set(branches.keys()))\n",
    "keys = list(ranges.keys())\n",
    "keys2 = list(color.keys())\n",
    "\n",
    "for key in keys:\n",
    "    cluster_kinematics[key] = {}\n",
    "    for key2 in keys2:\n",
    "        cluster_kinematics[key][key2] = rt.TH1F(str(uuid.uuid4()), key, ranges[key][0], ranges[key][1], ranges[key][2])\n",
    "\n",
    "        if(key2=='test'):\n",
    "            for dfile, tree in ur_trees.items():\n",
    "                for entry in tree['cluster'].array(branches[key]).flatten(): cluster_kinematics[key][key2].Fill(entry)\n",
    "            \n",
    "        elif(key2=='train'):\n",
    "            for dfile, tree in training_trees.items():\n",
    "                vals = tree.array(branches[key]).flatten()[training_indices[dfile]]\n",
    "                for entry in vals: cluster_kinematics[key][key2].Fill(entry)\n",
    "        \n",
    "        # now normalize each histogram to have an area of 100\n",
    "        integral = cluster_kinematics[key][key2].Integral()\n",
    "        #print(key,key2, 1. / integral)\n",
    "        cluster_kinematics[key][key2].Scale(100. / integral)\n",
    "        \n",
    "        # some formatting\n",
    "        cluster_kinematics[key][key2].SetFillColorAlpha(color[key2],0.3)\n",
    "        cluster_kinematics[key][key2].SetLineColor(color[key2])\n",
    "                    \n",
    "nx = 2\n",
    "ny = int(np.ceil(len(ranges) / nx))\n",
    "n = len(keys)\n",
    "\n",
    "plot_size = 450\n",
    "c = rt.TCanvas(str(uuid.uuid4()),'c_cluster_kinematics',plot_size * nx, plot_size * ny)\n",
    "c.Divide(nx,ny)\n",
    "\n",
    "legend = rt.TLegend(0.7,0.8,0.9,0.9)\n",
    "legend.AddEntry(cluster_kinematics[keys[0]]['train'],'train','f')\n",
    "legend.AddEntry(cluster_kinematics[keys[0]]['test'],'test','f')\n",
    "legend.SetTextColor(plot_style.text)\n",
    "\n",
    "stacks = []\n",
    "for i in range(n):\n",
    "    \n",
    "    mini_canvas = rt.TCanvas(str(uuid.uuid4()),'mini_canv',500,400)\n",
    "    canvases = [(c,i+1),(mini_canvas,0)]\n",
    "    \n",
    "    stack = rt.THStack(str(uuid.uuid4()),'stack')\n",
    "    stack.Add(cluster_kinematics[keys[i]]['train'])\n",
    "    stack.Add(cluster_kinematics[keys[i]]['test'])\n",
    "    \n",
    "    for canvas, num in canvases:\n",
    "        canvas.cd(num)\n",
    "        \n",
    "        rt.gPad.SetLeftMargin(0.2)\n",
    "        stack.SetTitle(titles[keys[i]])\n",
    "        stack.Draw('NOSTACK HIST')\n",
    "        legend.Draw()\n",
    "\n",
    "        if(log[keys[i]]): rt.gPad.SetLogy()\n",
    "        else: stack.GetHistogram().SetMinimum(0.)\n",
    "        \n",
    "        if(num == 0): canvas.SaveAs(plot_dir + '/' + 'cluster_' + keys[i] + '.png')\n",
    "    stacks.append(stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that there are *significant* differences between these distributions, in both shape and domain (especially in the case of $E_\\text{calib}^\\text{tot}$). As noted earlier when looking at minimum $\\Delta R$, we're considering *all* topo-clusters for the testing data. This may include some that were used to cluster jets that didn't pass cuts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
