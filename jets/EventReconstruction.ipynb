{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Reconstruction\n",
    "\n",
    "In this notebook, we will take some output files from `MLTree` as input, and use its cluster trees -- trees containing lists of topo-clusters-- to produce a `ROOT` file containing an \"event tree\".\n",
    "\n",
    "Note that the `MLTree` utility can already produce an event tree, which provides indices to reference the cluster tree. However, this may not be so helpful if combining multiple outputs from `MLTree`. This is because the events in each cluster tree are grouped by event (sequential *within* an event number), but the order of events is random and may be unique between outputs (with multiple output files generated for one run, corresponding with different species of particles or whatnot).\n",
    "\n",
    "Our event tree will be a literal regrouping of the cluster tree(s) data, so that each entry in our event tree will provide a set of all clusters from that event. This will allow for quick reading and event analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Setup\n",
    "\n",
    "First, let's import a bunch of packages we know we'll need right off-the-bat.\n",
    "\n",
    "Note that as we've set up our environment with `conda`, our `ROOT` installation has all the bells and whistles. This includes the `pythia8` library and its associated `ROOT` wrapper, `TPythia8`. We can optionally use this for jet-clustering, as it comes `fj-core`.\n",
    "Alternatively we could use the Pythonic interface for `fastjet` or [pyjet](https://github.com/scikit-hep/pyjet), but the latter requires linking an external fastjet build for speed and this doesn't seem to work when following their documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.22/02\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ROOT as rt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some extra setup\n",
    "path_prefix = '/workspace/LCStudies/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Fetching the data\n",
    "\n",
    "Now we get our data. For now, our classifiers are being trained to distinguish between $\\pi^+$ and $\\pi^0$. Assuming that all charged pions behave the same way, we can really treat this as a $\\pi^\\pm$ vs. $\\pi^0$ classifier. **For our toy workflow, we'll say that we only want to cluster $\\pi^\\pm$ topo-clusters into jets.** We will treat $\\pi^0$ as a background.\n",
    "\n",
    "For our input data, we have `ROOT` files containing a tree called `ClusterTree`. In each tree, each entry corresponds with one topo-cluster, and the different files correspond with different topo-cluster parent particles (e.g. $3$ files for $\\pi^+$,$\\pi^-$ and $\\pi^0$). Each topo-cluster entry contains information on the event from which it came (\"runNumber\" and \"eventNumber\"), and many topo-clusters (within and across files) share the same event. Our ultimate goal is to regroup this data into one file where each entry corresponds with one *event*. This is a sensible way to arrange the data before performing any jet clustering (which is performed by event), and writing to a file will allow us to skip this whole process after doing it once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Some of this meta-data is unused.\n",
    "# ----- Meta-data for our dataset -----\n",
    "layers = [\"EMB1\", \"EMB2\", \"EMB3\", \"TileBar0\", \"TileBar1\", \"TileBar2\"]\n",
    "nlayers = len(layers)\n",
    "cell_size_phi = [0.098, 0.0245, 0.0245, 0.1, 0.1, 0.1]\n",
    "cell_size_eta = [0.0031, 0.025, 0.05, 0.1, 0.1, 0.2]\n",
    "len_phi = [4, 16, 16, 4, 4, 4]\n",
    "len_eta = [128, 16, 8, 4, 4, 2]\n",
    "assert(len(len_phi) == nlayers)\n",
    "assert(len(len_eta) == nlayers)\n",
    "meta_data = {\n",
    "    layers[i]:{\n",
    "        'cell_size':(cell_size_eta[i],cell_size_phi[i]),\n",
    "        'dimensions':(len_eta[i],len_phi[i])\n",
    "    }\n",
    "    for i in range(nlayers)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Saving signal flags to clusters\n",
    "\n",
    "We will add a signal flag for each cluster -- what we ultimately consider \"signal\" and \"background\" depends on the task at hand, and we might want some more complicated labeling (e.g. particle ID)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "data_dir = '/workspace/LCStudies/data'\n",
    "data_files = glob.glob(data_dir + '/*.root')\n",
    "data_files = {x.split('/')[-1].replace('.root',''):x for x in data_files}\n",
    "tree_name = 'ClusterTree'\n",
    "\n",
    "sig_definition = {'signal':['piminus','piplus'],'background':['pi0']} # defining signal and background\n",
    "\n",
    "if(path_prefix not in sys.path): sys.path.append(path_prefix)\n",
    "from  util import qol_util as qu\n",
    "from pathlib import Path\n",
    "jet_data_dir = path_prefix + 'jets/data'\n",
    "Path(jet_data_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get our original data files.\n",
    "files = {key:rt.TFile(file,'READ') for key, file in data_files.items()}\n",
    "trees = {key:file.Get(tree_name) for key, file in files.items()}\n",
    "\n",
    "# Now we want to effectively add some new columns. We accomplish this with \"friend\" trees.\n",
    "# We're not actually making these trees friends yet. Instead we will form TChains and friend those.\n",
    "\n",
    "# Creating our branch buffer.\n",
    "data = {\n",
    "    'signal':np.zeros(1,dtype=np.dtype('i2')),\n",
    "    'file_index'   :np.zeros(1,dtype=np.dtype('i2')) # TODO: Is this needed any longer?\n",
    "}\n",
    "\n",
    "friend_tree_name = tree_name + '_friend'\n",
    "friend_data_files = {}\n",
    "\n",
    "file_index = 0\n",
    "stride = 2000\n",
    "\n",
    "for key in sorted(trees.keys()):\n",
    "    \n",
    "    friend_filename = data_files[key].split('/')[-1]\n",
    "    friend_filename = jet_data_dir + '/' + friend_filename\n",
    "    friend_file = rt.TFile(friend_filename,'RECREATE')\n",
    "    friend_data_files[key] = friend_filename\n",
    "    \n",
    "    friend_tree = rt.TTree(friend_tree_name,friend_tree_name)\n",
    "    branches = {}\n",
    "\n",
    "    # --- Setup the branches using our buffer. This is a rather general/flexible code block. ---\n",
    "    for bname, val in data.items():\n",
    "        descriptor = bname\n",
    "        bshape = val.shape\n",
    "        if(bshape != (1,)):\n",
    "            for i in range(len(bshape)):\n",
    "                descriptor += '[' + str(bshape[i]) + ']'\n",
    "        descriptor += '/'\n",
    "        if(val.dtype == np.dtype('i2')): descriptor += 'S'\n",
    "        elif(val.dtype == np.dtype('i4')): descriptor += 'I'\n",
    "        elif(val.dtype == np.dtype('i8')): descriptor += 'L'\n",
    "        elif(val.dtype == np.dtype('f4')): descriptor += 'F'\n",
    "        elif(val.dtype == np.dtype('f8')): descriptor += 'D'\n",
    "        else:\n",
    "            print('Warning, setup issue for branch: ', key, '. Skipping.')\n",
    "            continue\n",
    "        branches[key] = friend_tree.Branch(bname,val,descriptor)\n",
    "    # --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\n",
    "    \n",
    "    # Now we fill the friend tree.\n",
    "    nentries = trees[key].GetEntries()\n",
    "\n",
    "    # Signal flag and file index will be constant per file since input trees are divided by particle identity.\n",
    "    sig = 0\n",
    "    if(key in sig_definition['signal']): sig = 1\n",
    "    \n",
    "    prefix = 'Filling friend tree for ' + key + ':'\n",
    "    if(len(prefix) < 32): prefix = prefix + ' ' * (32 - len(prefix))\n",
    "    \n",
    "    for i in range(nentries):\n",
    "        data['signal'][0] = sig\n",
    "        data['file_index'][0] = file_index\n",
    "        friend_tree.Fill()\n",
    "    \n",
    "    friend_tree.Write()\n",
    "    friend_file.Close()\n",
    "    file_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Merging topo-cluster data, writing to a file and creating an eventNumber index\n",
    "\n",
    "Our trees containing the signal flag and network scores are now saved to disk. Now let's make `TChain`s and make them friends, so that we've effectively tacked on new columns to our original data. We will then save these `TChain`s as a `TTree` to an uncompressed file, and use the `TTreeIndex` functionality to sort our events. We do this conversion & saving because it appears to greatly speed up our reading when using the `TTreeIndex`. I assume this has to do something with the entries -- from both the main chain and its friend -- all being saved in the same file as opposed to being scattered across multiple ones. ([See here](https://root-forum.cern.ch/t/usage-of-tchainindex/19074/4) for a discussion of `TChainIndex` versus `TTreeIndex`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chains are now friends.\n"
     ]
    }
   ],
   "source": [
    "chain = rt.TChain(tree_name)\n",
    "friend_chain = rt.TChain(friend_tree_name)\n",
    "for key in data_files.keys(): \n",
    "    chain.AddFile(data_files[key],-1)\n",
    "    friend_chain.AddFile(friend_data_files[key],-1)\n",
    "    \n",
    "chain_filename = jet_data_dir + '/' + 'clusters.root'\n",
    "chain_file = rt.TFile(chain_filename,'RECREATE','',0) # uncompressed file\n",
    "clone = chain.CloneTree(-1,'FAST')\n",
    "friend_clone = friend_chain.CloneTree(-1,'FAST')\n",
    "clone.Write()\n",
    "friend_clone.Write()\n",
    "chain_file.Close()\n",
    "\n",
    "chain_file = rt.TFile(chain_filename,'READ')\n",
    "chain = chain_file.Get(tree_name)\n",
    "friend_chain = chain_file.Get(friend_tree_name)\n",
    "assert(chain.GetEntries() == friend_chain.GetEntries()) # number of entries must match, otherwise something has gone very wrong\n",
    "nentries = chain.GetEntries()\n",
    "chain.AddFriend(friend_chain)\n",
    "print('The chains are now friends.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Preparing to create an event TTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a function to quickly get the maximum length of a vector branch in our tree. We only have one vector branch for now but there may be others with a future dataset.\n",
    "\n",
    "This will let us turn the branch from one of type vector to one of type array. It's useful if we're adding an extra dimension, as I'm currently having issues with making branches like vectors of arrays on the Python side of things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetMaxVectorLength(chain, branchname):\n",
    "    draw_string = branchname + '@.size()'\n",
    "    chain.Draw(draw_string)\n",
    "    h = rt.gPad.GetPrimitive('htemp') # some slightly idiomatic ROOT stuff, one of the few examples of weird default behavior\n",
    "    max_length = int(h.GetXaxis().GetBinCenter(h.FindLastBinAbove(0)))\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create our `TTreeIndex`. We will use the branch `eventNumber` as our `majornumber`, so that the index effectively sorts our tree by `eventNumber` and we have our events grouped together. Since there are many duplicate `eventNumber` entries, and we do not use any `minornumber`, this is *not* a unique index. But this is fine, because we do not care about the sorting within any single `eventNumber` value. It just means that we *cannot* access every single entry with a call to `TTreeIndex::GetEntryNumberWithIndex()`, but rather we'll have to loop through the elements of `TTreeIndex:GetIndex()` sequentially.\n",
    "\n",
    "**TODO:** As a consequence of our indexing, we do not protect against the possibility of two clusters having the same `eventNumber` but different `runNumber`s. We should add this at some point to avoid the possibility of mixing clusters from what are really separate events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting minornumber to 0 effectively gets rid of it\n",
    "chain.SetBranchStatus('*',0)\n",
    "chain.SetBranchStatus('eventNumber',1)\n",
    "chain_idx = rt.TTreeIndex(chain,'eventNumber','0') # TODO: consider changing to majornumber=runNumber, minorNumber=eventNumber. In this particular case runNumber is always the same.\n",
    "n_idx = chain_idx.GetN()\n",
    "assert(n_idx == chain.GetEntriesFast()) # ensure our TTreeIndex is of the right length, otherwise something is wrong\n",
    "chain_indices = chain_idx.GetIndex() # a C++-style array of (ROOT) type Long64_t...\n",
    "chain_indices = np.array([chain_indices[i] for i in range(n_idx)],dtype=np.dtype('i8')) #... now a numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine the maximum number of topo-clusters per event. We can set a naïve upper bound with nfiles * max(nCluster), but the actual upper bound is probably lower than this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding max nClusters |████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% Complete\n",
      "max(nClusters) per event = 34\n"
     ]
    }
   ],
   "source": [
    "chain.SetBranchStatus('*',0)\n",
    "chain.SetBranchStatus('eventNumber',1)\n",
    "chain.SetBranchStatus('nCluster',1)\n",
    "\n",
    "n_clusters_max = 0\n",
    "max_tmp = 0\n",
    "\n",
    "chain.GetEntry(0)\n",
    "eN_prev = chain.eventNumber\n",
    "\n",
    "nentries = chain.GetEntries()\n",
    "stride = int(nentries/100)\n",
    "l = int(nentries/stride)\n",
    "bar_length = 120\n",
    "prefix = 'Finding max nClusters'\n",
    "qu.printProgressBar(0, l, prefix=prefix, suffix='Complete', length=bar_length)\n",
    "for i in range(nentries):\n",
    "    idx = chain_indices[i]\n",
    "    chain.GetEntry(idx)\n",
    "    if(chain.eventNumber != eN_prev):\n",
    "        if(max_tmp > n_clusters_max): n_clusters_max = max_tmp\n",
    "        max_tmp = 0\n",
    "    max_tmp += 1\n",
    "    eN_prev = chain.eventNumber\n",
    "    if(i%stride == 0): qu.printProgressBar(int(i/stride), l, prefix=prefix, suffix='Complete', length=bar_length)\n",
    "    \n",
    "print('max(nClusters) per event =',n_clusters_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to copy our data to a new `ROOT` file, where each entry corresponds with an **event**. Our `chain_indices` lets us loop through the existing data in a sensible way.\n",
    "\n",
    "Certain variables are *per-event* variables, such as `runNumber`. These will remain as scalars. Other variables are *per-cluster* variables, such as `clusterE` (scalar) or `EMB1` (2D vector). These will become *arrays* of whatever their previous type was. The branch `nCluster` will keep track of their length for each event. Note that this means we be rewriting the contents of the `nCluster` branch, rather than copying it over -- it currently only keeps track of the number of clusters per event per file, not the total number of clusters per event.\n",
    "\n",
    "As one last note, we will have to be a little careful about looping through our `TChain` of input trees for the sake of speed. We're using a `TTreeIndex` that we built, but this will amount to hopping around a lot (reading entries in a very non-sequential order w.r.t. the chain/trees). [This can slow things down with a lot of file I/0](https://root-forum.cern.ch/t/ttree-getentry-with-a-ttreeindex-is-too-slow/17370/5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting from ROOT type names to leaflist decorators.\n",
    "# Vector decorator will not work, but gives a sensible string\n",
    "# telling us the depth (how many vectors).\n",
    "def RTypeConversion(type_string):\n",
    "    if(type_string == 'Short_t' or type_string == 'short'):    return 'S'\n",
    "    elif(type_string == 'Int_t' or type_string == 'int'):    return 'I'\n",
    "    elif(type_string == 'Float_t' or type_string == 'float'):  return 'F'\n",
    "    elif(type_string == 'Double_t' or type_string == 'double'): return 'D'\n",
    "    elif('vector' in type_string): # special case\n",
    "#         type_substring = '<'.join(type_string.split('<')[1:])\n",
    "#         type_substring = '>'.join(type_substring.split('>')[:-1])\n",
    "#         type_substring = RTypeConversion(type_substring)\n",
    "#         return 'v_' + type_substring\n",
    "        return type_string\n",
    "    else: return '?'\n",
    "\n",
    "def GetShape(shape_string):\n",
    "    dims = shape_string.replace('[',' ').replace(']', ' ').split()\n",
    "    return tuple([int(x) for x in dims])\n",
    "\n",
    "def RType2NType(type_string):\n",
    "    if(type_string == 'S'):   return np.dtype('i2')\n",
    "    elif(type_string == 'I'): return np.dtype('i4')\n",
    "    elif(type_string == 'L'): return np.dtype('i8')\n",
    "    elif(type_string == 'F'): return np.dtype('f4')\n",
    "    elif(type_string == 'D'): return np.dtype('f8')\n",
    "    else: raise ValueError('Input not understood.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly turn on all branches.\n",
    "chain.SetBranchStatus('*',1)\n",
    "friend_chain.SetBranchStatus('*',1)\n",
    "\n",
    "# n_clusters_max = int(len(data_files.keys()) * chain.GetMaximum('nCluster')) # safe upper limit, but might be unnecessarily high\n",
    "\n",
    "# Building our branch buffer for our new trees. This time we'll add leaflists to the buffer as well.\n",
    "# Slightly hacky but this should be pretty flexible for any basic-type branches.\n",
    "\n",
    "branch_info = [x.GetListOfLeaves()[0] for x in chain.GetListOfBranches()]\n",
    "branch_info = [(x.GetTitle(),x.GetTypeName()) for x in branch_info]\n",
    "branch_names = [x[0].split('[')[0] for x in branch_info]\n",
    "\n",
    "friend_branch_info = [x.GetListOfLeaves()[0] for x in friend_chain.GetListOfBranches()]\n",
    "friend_branch_info = [(x.GetTitle(),x.GetTypeName()) for x in friend_branch_info]\n",
    "friend_branch_names = [x[0].split('[')[0] for x in friend_branch_info]\n",
    "\n",
    "branch_names = branch_names + friend_branch_names\n",
    "branch_info = branch_info + friend_branch_info\n",
    "\n",
    "# Now let's consider removing some branches that we don't think we'll need for our event dataset.\n",
    "# This can potentially speed things up a lot. Especially true for branches of type std::vector at the moment,\n",
    "# as I am probably not handling them in the smartest way.\n",
    "branch_names_remove = ['cluster_cellE_norm']\n",
    "\n",
    "# In some cases we might want to change a branch's type when copying it over. One should be careful with this,\n",
    "# but an obvious case is nCluster -- which we are explicitly overwriting anyway, so we know its new type.\n",
    "rtypes_forced = {'nCluster':'S'}\n",
    "\n",
    "perEvent = ['runNumber','eventNumber','nCluster','file_index'] # keep track of which branches only need one entry per event\n",
    "vector_branches = [] # keep track of any branches that are of (C++) type std::vector\n",
    "\n",
    "# We must also keep track of the original shapes of any array branches that we read, as they will be read out as 1D cppy arrays\n",
    "# and will need to be reshaped before being placed in our buffer.\n",
    "input_shapes = {}\n",
    "\n",
    "branch_buffer = {}\n",
    "for entry in branch_info: \n",
    "    name = entry[0]\n",
    "    shape = (1,)\n",
    "    shape_string = ''\n",
    "    if('[' in name): \n",
    "        shape_string = '[' + '['.join(name.split('[')[1:])\n",
    "        shape = GetShape(shape_string)\n",
    "    name = name.split('[')[0]\n",
    "    \n",
    "    rtype = RTypeConversion(entry[1])    \n",
    "    if(name in rtypes_forced.keys()): rtype = rtypes_forced[name]\n",
    "    \n",
    "    if(name in branch_names_remove): continue\n",
    "    \n",
    "    # save the original shapes of non-scalar branches\n",
    "    if(shape != (1,)): input_shapes[name] = shape # save the original shape\n",
    "\n",
    "    if(name not in perEvent):\n",
    "        if(shape == (1,)): \n",
    "            shape = (n_clusters_max,)\n",
    "            shape_string = '[' + 'nCluster' + ']'\n",
    "        else:\n",
    "            shape = tuple([n_clusters_max] + list(shape))\n",
    "            shape_string = '[' + 'nCluster' + ']' + shape_string\n",
    "      \n",
    "    if('vector' in rtype):\n",
    "        continue\n",
    "        # We will make the vector into an array. Assuming vector is of some basic type! (not vector of vectors, etc.)\n",
    "        rsubtype = '<'.join(rtype.split('<')[1:])\n",
    "        rsubtype = '>'.join(rsubtype.split('>')[:-1])\n",
    "        rtype = RTypeConversion(rsubtype)\n",
    "        n_max = GetMaxVectorLength(chain,name)\n",
    "        input_shapes[name]=(n_max,)\n",
    "        shape = tuple(list(shape) + [n_max])\n",
    "        shape_string += '[' + str(n_max) + ']'\n",
    "        vector_branches.append(name)\n",
    "        #branch_buffer[name] = [rt.vector(rsubtype)(),0]\n",
    "        #TODO: Add a branch for the vector length\n",
    "        \n",
    "    branch_buffer[name] = [np.zeros(shape,dtype=RType2NType(rtype)),name + shape_string + '/' + rtype]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) Creating event TTree\n",
    "\n",
    "Now we're really ready to make our event TTree -- each entry will correspond with a full event, and hold all of its topo-clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to write 5000 events, corresponding to 21929 input topo-clusters.\n"
     ]
    }
   ],
   "source": [
    "# Determining how many events to write. By default we want them all, but for debugging we might only want some subset.\n",
    "nevents = -1\n",
    "nentries = chain_indices.shape[0]\n",
    "if(nevents > 0):\n",
    "    # determine how many entries we need to get this many events\n",
    "    chain.SetBranchStatus('*',0)\n",
    "    chain.SetBranchStatus('eventNumber',1)\n",
    "    nevents_tally = 0\n",
    "    chain.GetEntry(chain_indices[0])\n",
    "    eN_prev = chain.eventNumber\n",
    "    for i in range(nentries):\n",
    "        chain.GetEntry(chain_indices[i])\n",
    "        if(chain.eventNumber != eN_prev): nevents_tally += 1\n",
    "        eN_prev = chain.eventNumber\n",
    "        if(nevents_tally == nevents):\n",
    "            nentries = i-1 # don't include the event we've just started\n",
    "            break\n",
    "            \n",
    "if(nevents <= 0): nevents = 'all'\n",
    "report_string = 'Preparing to write {nev} events, corresponding to {nen} input topo-clusters.'.format(nev=nevents,nen=nentries)\n",
    "print(report_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employing strategy 1 (load full input tree into memory).\n",
      "Employing strategy 2 (increasing basket size for input tree).\n"
     ]
    }
   ],
   "source": [
    "# Activate the branches we need, keep any ignored ones deactivated\n",
    "chain.SetBranchStatus('*',1)\n",
    "for name in branch_names_remove: chain.SetBranchStatus(name,0)\n",
    "    \n",
    "# Strategies for speeding things up more. \n",
    "strategy = 3\n",
    "\n",
    "if(strategy == 1 or strategy == 3):\n",
    "    filesize = chain_file.GetSize() # in bytes\n",
    "    chain.SetMaxVirtualSize(int(1.5 * filesize))\n",
    "    for name in branch_names:\n",
    "        if(name not in branch_names_remove): chain.GetBranch(name).LoadBaskets()\n",
    "    print('Employing strategy 1 (load full input tree into memory).')\n",
    "\n",
    "if(strategy == 2 or strategy == 3):\n",
    "    # Increasing basket_size will help a little. But because entry access is somewhat random,\n",
    "    # with each event's clusters generally spread out into multiple small groups of adjacent entries,\n",
    "    # performance will quickly plateau and big increases may not give any real gains.\n",
    "    basket_size_multiplier = 3\n",
    "    basket_size = 16000 * basket_size_multiplier\n",
    "    for name in branch_names:\n",
    "        if(name in branch_names_remove): continue\n",
    "        if(name in friend_branch_names): friend_chain.SetBasketSize(name,basket_size)\n",
    "        else: chain.SetBasketSize(name,basket_size)\n",
    "    print('Employing strategy 2 (increasing basket size for input tree).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing event tree: |\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m| 100.0% Complete\n",
      "6.2 seconds. (input data rate = 3546.2 Hz)\n"
     ]
    }
   ],
   "source": [
    "#TODO: Deal with read/write bugging out at 99% when trying to write the full dataset. Works fine for subsets.\n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "Path(jet_data_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Make the new TFile and TTree.\n",
    "event_filename = path_prefix + 'jets/data' + '/' + 'events.root'\n",
    "event_treename = 'eventTree'\n",
    "event_file = rt.TFile(event_filename,'RECREATE','',0) # TODO: Making this uncompressed for debugging purposes\n",
    "event_tree = rt.TTree(event_treename,event_treename)\n",
    "#event_tree.SetDirectory(0) # tree will start in memory!\n",
    "\n",
    "# Set up the branches. Note that we must add branches specifying lengths *before*\n",
    "# any branches whose lengths they specify. For now, that means nCluster must go first.\n",
    "# TODO: Make this less hacky.\n",
    "name = 'nCluster'\n",
    "assert(name in branch_buffer.keys())\n",
    "buffer   = branch_buffer[name][0]\n",
    "leaflist = branch_buffer[name][1]\n",
    "event_tree.Branch(name,buffer,leaflist)\n",
    "\n",
    "for key, value in branch_buffer.items():\n",
    "    if(key == 'nCluster'): continue\n",
    "    name = key\n",
    "    buffer = value[0]\n",
    "    leaflist = value[-1]\n",
    "    if(leaflist == 0): event_tree.Branch(name,value[0])\n",
    "    else: event_tree.Branch(name,buffer,leaflist)\n",
    "\n",
    "chain.GetEntry(chain_indices[0])\n",
    "eN_prev = chain.eventNumber\n",
    "cluster_idx = 0\n",
    "\n",
    "stride = int(nentries/100)\n",
    "l = int(nentries/stride)\n",
    "bar_length = 120\n",
    "prefix = 'Writing event tree:'\n",
    "qu.printProgressBar(0, l, prefix=prefix, suffix='Complete', length=bar_length)\n",
    "\n",
    "start = time.time()\n",
    "# dt = np.zeros(5)\n",
    "# Now loop through the input chain and write to our new tree.\n",
    "for i in range(nentries):\n",
    "\n",
    "#     t0 = time.time()\n",
    "    chain.GetEntry(chain_indices[i])\n",
    "    eN = chain.eventNumber\n",
    "#     dt[0] += time.time() - t0\n",
    "    \n",
    "    if(eN != eN_prev):\n",
    "        # We've finished an event and have just entered a new one.\n",
    "        # Write everything that's in the buffer. Corresponds to the previous event.\n",
    "#         t0 = time.time()\n",
    "        event_tree.Fill()\n",
    "        cluster_idx = 0 # reset cluster_idx\n",
    "#         dt[1] += time.time() - t0\n",
    "    \n",
    "    # Fill info from the current event.\n",
    "    for name in branch_buffer.keys():\n",
    "        shape = branch_buffer[name][0].shape\n",
    "        \n",
    "        # Our per-event branches\n",
    "        if(shape == (1,)):\n",
    "#             t0 = time.time()\n",
    "            if(name == 'nCluster'): branch_buffer[name][0][0] = cluster_idx + 1 # will reach max value before write\n",
    "            else: branch_buffer[name][0][0] = getattr(chain,name) # a more C-like way of getting branches\n",
    "#             dt[2] += time.time() - t0\n",
    "        # Our per-cluster branches\n",
    "        else:\n",
    "            ndim = branch_buffer[name][0].ndim\n",
    "            if(ndim == 1): \n",
    "#                 t0 = time.time()\n",
    "                branch_buffer[name][0][cluster_idx] = getattr(chain,name) # per-cluster scalar\n",
    "#                 dt[3] += time.time() - t0\n",
    "            else:\n",
    "                # Multi-dim branches.\n",
    "#                 t0 = time.time()\n",
    "#                 if(name in vector_branches): # Our one vector branch seems to be a culprit for slowdowns.\n",
    "#                     n = len(getattr(chain,name))\n",
    "#                     branch_buffer[name][0][cluster_idx,:n] = np.array(getattr(chain,name))[:]\n",
    "#                     continue\n",
    "                branch_buffer[name][0][cluster_idx,:] = np.array(getattr(chain,name)).reshape(input_shapes[name])[:] # per-cluster array\n",
    "#                 dt[4] += time.time() - t0\n",
    "    \n",
    "    cluster_idx += 1\n",
    "    eN_prev = eN\n",
    "    if(i%stride == 0): qu.printProgressBarColor(int(i/stride), l, prefix=prefix, suffix='Complete', length=bar_length)\n",
    "    if(i == nentries-1): # Make sure to call a Fill() if we're at the end of the chain\n",
    "        event_tree.Fill()\n",
    "        #qu.printProgressBar(l, l, prefix=prefix, suffix='Complete', length=bar_length)\n",
    "    \n",
    "event_file.cd()\n",
    "event_tree.Write('',rt.TObject.kOverwrite)\n",
    "event_file.Close()\n",
    "end = time.time()\n",
    "dt_tot = end - start\n",
    "rate = (nentries / dt_tot)\n",
    "print('{val:.1f} seconds. (input data rate = {rate:.1f} Hz)'.format(val=dt_tot,rate=rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above method is still a little slow, with most of the slowdown having to do with reading entries from our combined `clusterTree` in a non-sequential fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7) Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we clean up some of the intermediate files we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting /workspace/LCStudies/jets/data/piplus.root .\n",
      "Deleting /workspace/LCStudies/jets/data/pi0.root .\n",
      "Deleting /workspace/LCStudies/jets/data/piminus.root .\n",
      "Deleting /workspace/LCStudies/jets/data/clusters.root .\n"
     ]
    }
   ],
   "source": [
    "cleanup = True\n",
    "import glob, os\n",
    "rfiles = glob.glob(jet_data_dir + '/*.root')\n",
    "if(cleanup):\n",
    "    for rfile in rfiles:\n",
    "        if(rfile == event_filename): continue\n",
    "        print('Deleting',rfile,'.')\n",
    "        os.remove(rfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml4p]",
   "language": "python",
   "name": "conda-env-ml4p-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
