{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jet Clustering\n",
    "\n",
    "This workflow is for use with the jet samples, that contain both `ClusterTree` and `EventTree` (provided by the `MLTree` utility). This **cannot** handle data where the `EventTree` does not exist, because that contains info on piecing the clusters together into events*, and the baseline jet clustering.\n",
    "\n",
    "\\* This pieceing together can be accomplished in workflows like `EventReconstructionPion.ipynb` but it's rather complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO:\n",
    "\n",
    "- finish up calculation of scores\n",
    "- jet clustering (save to new file?)\n",
    "- comparison of jets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Setup\n",
    "\n",
    "First, let's import a bunch of packages we know we'll need right off-the-bat.\n",
    "\n",
    "Note that as we've set up our environment with `conda`, our `ROOT` installation has all the bells and whistles. This includes the `pythia8` library and its associated `ROOT` wrapper, `TPythia8`. We can optionally use this for jet-clustering, as it comes `fj-core`.\n",
    "Alternatively we could use the Pythonic interface for `fastjet` or [pyjet](https://github.com/scikit-hep/pyjet), but the latter requires linking an external fastjet build for speed and this doesn't seem to work when following their documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.22/02\n"
     ]
    }
   ],
   "source": [
    "# Imports - generic stuff\n",
    "\n",
    "import numpy as np\n",
    "import ROOT as rt\n",
    "import uproot as ur\n",
    "import sys, os, glob\n",
    "import subprocess as sub\n",
    "from pathlib import Path\n",
    "\n",
    "path_prefix = '/workspace/LCStudies/'\n",
    "if(path_prefix not in sys.path): sys.path.append(path_prefix)\n",
    "from util import ml_util as mu # for passing calo images to regression networks\n",
    "from util import qol_util as qu # for progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "# Imports and setup for TensorFlow and Keras.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # disable some of the tensorflow info printouts, only display errors\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ngpu = 1\n",
    "gpu_list = [\"/gpu:\"+str(i) for i in range(ngpu)]\n",
    "strategy = tf.distribute.MirroredStrategy(devices=gpu_list)\n",
    "ngpu = strategy.num_replicas_in_sync\n",
    "print ('Number of devices: {}'.format(ngpu))\n",
    "\n",
    "# Dictionary for storing all our neural network models that will be evaluated\n",
    "network_models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup paths\n",
    "data_dir = path_prefix + 'data/jet'\n",
    "classification_dir = path_prefix + 'classifier/Models'\n",
    "regression_dir = path_prefix + 'regression/Models'\n",
    "fj_dir = path_prefix + '/setup/fastjet/fastjet-install/lib/python3.8/site-packages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Calorimeter meta-data -----\n",
    "layers = [\"EMB1\", \"EMB2\", \"EMB3\", \"TileBar0\", \"TileBar1\", \"TileBar2\"]\n",
    "nlayers = len(layers)\n",
    "cell_size_phi = [0.098, 0.0245, 0.0245, 0.1, 0.1, 0.1]\n",
    "cell_size_eta = [0.0031, 0.025, 0.05, 0.1, 0.1, 0.2]\n",
    "len_phi = [4, 16, 16, 4, 4, 4]\n",
    "len_eta = [128, 16, 8, 4, 4, 2]\n",
    "assert(len(len_phi) == nlayers)\n",
    "assert(len(len_eta) == nlayers)\n",
    "meta_data = {\n",
    "    layers[i]:{\n",
    "        'cell_size':(cell_size_eta[i],cell_size_phi[i]),\n",
    "        'dimensions':(len_eta[i],len_phi[i])\n",
    "    }\n",
    "    for i in range(nlayers)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading flat classification models... \n",
      "\tLoading EMB1... Done.\n",
      "\tLoading EMB2... Done.\n",
      "\tLoading EMB3... Done.\n",
      "\tLoading TileBar0... Done.\n",
      "\tLoading TileBar1... Done.\n",
      "\tLoading TileBar2... Done.\n",
      "Loading simple combo classification model... Done.\n",
      "Loading charged-pion energy regression model... Done.\n",
      "Loading neutral-pion energy regression model... Done.\n"
     ]
    }
   ],
   "source": [
    "# flat classifiers\n",
    "print('Loading flat classification models... ')\n",
    "flat_model_files = glob.glob(classification_dir + '/flat/' + '*.h5')\n",
    "flat_model_files.sort()\n",
    "flat_model_names = []\n",
    "for model in flat_model_files:\n",
    "    model_name = model.split('model_')[-1].split('_flat')[0]\n",
    "    print('\\tLoading ' + model_name + '... ',end='')\n",
    "    flat_model_names.append(model_name)\n",
    "    network_models[model_name] = tf.keras.models.load_model(model)\n",
    "    print('Done.')\n",
    "\n",
    "# combo classifier\n",
    "print('Loading simple combo classification model... ',end='')\n",
    "combo_model_file = classification_dir + '/simple/' + 'model_simple_do20.h5'\n",
    "network_models['combo'] = tf.keras.models.load_model(combo_model_file)\n",
    "print('Done.')\n",
    "\n",
    "# energy regression networks\n",
    "print('Loading charged-pion energy regression model... ',end='')\n",
    "charged_energy_model_file = regression_dir + '/' + 'all_charged.h5'\n",
    "network_models['e_charged'] = tf.keras.models.load_model(charged_energy_model_file)\n",
    "print('Done.')\n",
    "\n",
    "print('Loading neutral-pion energy regression model... ',end='')\n",
    "neutral_energy_model_file = regression_dir + '/' + 'all_neutral.h5'\n",
    "network_models['e_neutral'] = tf.keras.models.load_model(neutral_energy_model_file)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make a \"local\" copy of the jet data. We will only copy over certain branches, and we will skip any files that don't contain an `eventTree` in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying data files: |\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m| 100.0% Complete\n"
     ]
    }
   ],
   "source": [
    "data_filenames = glob.glob(data_dir + '/' + '*.root')\n",
    "\n",
    "# debugging\n",
    "data_filenames = [data_dir + '/' + 'user.angerami.21685345.OutputStream._000062.root', data_dir + '/' + 'user.angerami.21685345.OutputStream._000113.root']\n",
    "\n",
    "# our \"local\" data dir, where we create modified data files\n",
    "jet_data_dir = path_prefix + 'jets/data'\n",
    "Path(jet_data_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get the original data.\n",
    "files = {name:rt.TFile(name,'READ') for name in data_filenames}\n",
    "\n",
    "# Some data files might be missing an EventTree.\n",
    "# For now, we will skip these because our methods count on an existing EventTree.\n",
    "delete_keys = []\n",
    "for key, val in files.items():\n",
    "    file_keys = [x.GetName() for x in val.GetListOfKeys()]\n",
    "    if('ClusterTree' not in file_keys or 'EventTree' not in file_keys):\n",
    "        delete_keys.append(key)\n",
    "\n",
    "for key in delete_keys: \n",
    "    print('Ignoring file:',key,'(no EventTree/ClusterTree found).')\n",
    "    del files[key]\n",
    "\n",
    "if(path_prefix not in sys.path): sys.path.append(path_prefix)\n",
    "from  util import qol_util as qu # for progress bar\n",
    "\n",
    "# now we make a local copy of the files in the jet_data_dir, keeping only certain branches\n",
    "active_branches = {}\n",
    "active_branches['cluster'] = [\n",
    "    'runNumber',\n",
    "    'eventNumber',\n",
    "    'truthE',\n",
    "    'truthPt',\n",
    "    'truthEta',\n",
    "    'truthPhi',\n",
    "    'clusterIndex',\n",
    "    'nCluster',\n",
    "    'clusterE',\n",
    "    'clusterECalib',\n",
    "    'clusterPt',\n",
    "    'clusterEta',\n",
    "    'clusterPhi',\n",
    "    'cluster_nCells',\n",
    "    'cluster_ENG_CALIB_TOT',\n",
    "    'EMB1',\n",
    "    'EMB2',\n",
    "    'EMB3',\n",
    "    'TileBar0',\n",
    "    'TileBar1',\n",
    "    'TileBar2'\n",
    "]\n",
    "active_branches['event'] = [\n",
    "    'runNumber',\n",
    "    'eventNumber',\n",
    "    'lumiBlock',\n",
    "    'NPV',\n",
    "    'nTruthPart',\n",
    "    'clusterCount',\n",
    "    'nCluster',\n",
    "    'clusterE',\n",
    "    'clusterPt',\n",
    "    'clusterEta',\n",
    "    'clusterPhi',\n",
    "    'AntiKt4EMTopoJetsPt',\n",
    "    'AntiKt4EMTopoJetsEta',\n",
    "    'AntiKt4EMTopoJetsPhi',\n",
    "    'AntiKt4EMTopoJetsE',\n",
    "    'AntiKt4LCTopoJetsPt',\n",
    "    'AntiKt4LCTopoJetsEta',\n",
    "    'AntiKt4LCTopoJetsPhi',\n",
    "    'AntiKt4LCTopoJetsE',\n",
    "    'AntiKt4TruthJetsPt',\n",
    "    'AntiKt4TruthJetsEta',\n",
    "    'AntiKt4TruthJetsPhi',\n",
    "    'AntiKt4TruthJetsE'\n",
    "]\n",
    "\n",
    "tree_names = {'cluster':'ClusterTree','event':'EventTree'}\n",
    "data_filenames = []\n",
    "\n",
    "l = len(files.keys())\n",
    "i = 0\n",
    "qu.printProgressBarColor(i, l, prefix='Copying data files:', suffix='Complete', length=50)\n",
    "\n",
    "for path, tfile in files.items():\n",
    "    filename_new = jet_data_dir + '/' + path.split('/')[-1]\n",
    "    old_trees = {x:tfile.Get(tree_names[x]) for x in tree_names.keys()}\n",
    "    \n",
    "    for key, tree in old_trees.items():\n",
    "        tree.SetBranchStatus('*',0)\n",
    "        for bname in active_branches[key]: tree.SetBranchStatus(bname,1)\n",
    "    \n",
    "    tfile_new = rt.TFile(filename_new,'RECREATE')\n",
    "    new_trees = {x:old_trees[x].CloneTree() for x in old_trees.keys()}\n",
    "    tfile_new.Write()\n",
    "    data_filenames.append(filename_new)\n",
    "    i += 1\n",
    "    qu.printProgressBarColor(i, l, prefix='Copying data files:', suffix='Complete', length=50)\n",
    "    del old_trees\n",
    "    del new_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the files & trees with uproot\n",
    "files = {name:rt.TFile(name,'READ') for name in data_filenames}\n",
    "tree_names = {'cluster':'ClusterTree','event':'EventTree'}\n",
    "ur_trees = {file:{tree_key:ur.open(file)[tree_name] for tree_key,tree_name in tree_names.items()} for file in data_filenames}\n",
    "\n",
    "# reminder: how to get an awkward array for a particular branch, here a is a key corresponding to a filename\n",
    "#ur_trees[a]['cluster'].array('EMB1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will loop over our data files. This isn't the most notebook-esque code, but it should avoid \"out of memory\" issues: As we are dealing with a large amount of data, preparing all the data in memory before operating on it will result in very high memory usage. Thus we will sacrifice a multi-cell approach of preparing all the data step-by-step, in order to make sure we don't load more stuff into memory at a time than we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/LCStudies/jets/data/user.angerami.21685345.OutputStream._000062.root\n",
      "\tPrepping calo images...\n",
      "\tPrepping extra inputs...\n",
      "\tCalculating network outputs...\n",
      "\t\tClassification... Done.\n",
      "\t\tRegression... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-c55733e11ad5>:67: RuntimeWarning: overflow encountered in exp\n",
      "  model_scores[name] = np.exp(scaler_cal.inverse_transform(model.predict(regression_input)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "/workspace/LCStudies/jets/data/user.angerami.21685345.OutputStream._000113.root\n",
      "\tPrepping calo images...\n",
      "\tPrepping extra inputs...\n",
      "\tCalculating network outputs...\n",
      "\t\tClassification... Done.\n",
      "\t\tRegression... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-c55733e11ad5>:67: RuntimeWarning: overflow encountered in exp\n",
      "  model_scores[name] = np.exp(scaler_cal.inverse_transform(model.predict(regression_input)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# branch buffer for filling our score trees\n",
    "    # make our branch buffer\n",
    "branch_buffer = {\n",
    "    'charged_likelihood_combo': np.zeros(1,dtype=np.dtype('f8')),\n",
    "    'clusterE_charged': np.zeros(1,dtype=np.dtype('f8')),\n",
    "    'clusterE_neutral': np.zeros(1,dtype=np.dtype('f8'))\n",
    "}\n",
    "\n",
    "for dfile, trees in ur_trees.items():\n",
    "    \n",
    "    print (dfile)\n",
    "    # prep the calo images\n",
    "    print('\\tPrepping calo images...')\n",
    "    calo_images = {}\n",
    "    for layer in layers:\n",
    "        calo_images[layer] = mu.setupCells(trees['cluster'],layer)\n",
    "    combined_images = np.concatenate(tuple([calo_images[layer] for layer in layers]), axis=1)\n",
    "\n",
    "    # prep some extra combined input for energy regression\n",
    "    print('\\tPrepping extra inputs...')\n",
    "    scaler_e = StandardScaler()\n",
    "    scaler_cal = StandardScaler()\n",
    "    scaler_eta = StandardScaler()\n",
    "    \n",
    "    e = trees['cluster'].array('clusterE')\n",
    "    e_calib = trees['cluster'].array('cluster_ENG_CALIB_TOT')\n",
    "    eta = trees['cluster'].array('clusterEta')\n",
    "    \n",
    "    # cleaning for e and e_calib (empirically needed for e_calib to remove values that are too large)\n",
    "    epsilon = 1.0e-12\n",
    "    e = np.where(e < epsilon, epsilon, e)\n",
    "    e_calib = np.where(e_calib < epsilon, epsilon, e_calib)\n",
    "    \n",
    "    regression_cols = {}\n",
    "    regression_cols['s_logE'] = scaler_e.fit_transform(np.log(e).reshape(-1,1))\n",
    "    regression_cols['s_logECalib'] = scaler_cal.fit_transform(np.log(e_calib).reshape(-1,1))\n",
    "    regression_cols['s_eta'] = scaler_eta.fit_transform(eta.reshape(-1,1))\n",
    "    \n",
    "    s_combined,scaler_combined = mu.standardCells(combined_images, layers)\n",
    "    regression_input = np.column_stack((regression_cols['s_logE'], regression_cols['s_eta'],s_combined))\n",
    "\n",
    "    # now find network scores\n",
    "    print('\\tCalculating network outputs...')\n",
    "    model_scores = {}\n",
    "    \n",
    "    print('\\t\\tClassification... ', end='')\n",
    "    # 1) flat networks\n",
    "    for layer in flat_model_names:\n",
    "        model = network_models[layer]\n",
    "        model_scores[layer] = model.predict(calo_images[layer])[:,1] # [:,1] based on Max's code, this is input to combo network. Likelihood of being charged (vs. neutral)\n",
    "    \n",
    "    # 2) combo network\n",
    "    name = 'combo'\n",
    "    model = network_models[name]\n",
    "    input_scores = np.column_stack([model_scores[layer] for layer in layers])\n",
    "    model_scores[name] = model.predict(input_scores)[:,1] # likelihood of being charged pion (versus neutral pion)\n",
    "    print('Done.')\n",
    "    \n",
    "    print('\\t\\tRegression... ', end='')\n",
    "    # 3) energy regression networks\n",
    "    name = 'e_charged'\n",
    "    model = network_models[name]\n",
    "    model_scores[name] = np.exp(scaler_cal.inverse_transform(model.predict(regression_input)))\n",
    "    \n",
    "    name = 'e_neutral'\n",
    "    model = network_models[name]\n",
    "    model_scores[name] = np.exp(scaler_cal.inverse_transform(model.predict(regression_input)))\n",
    "    print('Done.')\n",
    "    \n",
    "    # Now we should save these scores to a new tree.\n",
    "    f = rt.TFile(dfile, 'UPDATE')\n",
    "    tree_name = 'ScoreTree'\n",
    "    t = rt.TTree(tree_name, tree_name)\n",
    "    \n",
    "    print('Saving network scores to tree ' + tree_name + '... ',end='')    \n",
    "    # --- Setup the branches using our buffer. This is a rather general/flexible code block. ---\n",
    "    branches = {}\n",
    "    for bname, val in branch_buffer.items():\n",
    "        descriptor = bname\n",
    "        bshape = val.shape\n",
    "        if(bshape != (1,)):\n",
    "            for i in range(len(bshape)):\n",
    "                descriptor += '[' + str(bshape[i]) + ']'\n",
    "        descriptor += '/'\n",
    "        if(val.dtype == np.dtype('i2')): descriptor += 'S'\n",
    "        elif(val.dtype == np.dtype('i4')): descriptor += 'I'\n",
    "        elif(val.dtype == np.dtype('i8')): descriptor += 'L'\n",
    "        elif(val.dtype == np.dtype('f4')): descriptor += 'F'\n",
    "        elif(val.dtype == np.dtype('f8')): descriptor += 'D'\n",
    "        else:\n",
    "            print('Warning, setup issue for branch: ', key, '. Skipping.')\n",
    "            continue\n",
    "        branches[bname] = t.Branch(bname,val,descriptor)\n",
    "    \n",
    "    # Fill the model score tree, and save it to the local data file.\n",
    "    nentries = model_scores['combo'].shape[0]\n",
    "    for i in range(nentries):\n",
    "        branch_buffer['charged_likelihood_combo'][0] = model_scores['combo'][i]\n",
    "        branch_buffer['clusterE_charged'][0] = model_scores['e_charged'][i]\n",
    "        branch_buffer['clusterE_neutral'][0] = model_scores['e_neutral'][i]\n",
    "        t.Fill()\n",
    "    \n",
    "    t.Write('',rt.TObject.kOverwrite)\n",
    "    f.Close()\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have classification and energy regression scores for all of our topo-clusters. Next, we want to perform jet-clustering, where we'll use the regressed energies (and the classification score will tell us which regressed energy to use for each cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(fj_dir)\n",
    "import fastjet as fj\n",
    "\n",
    "# Jet clustering params\n",
    "R = 0.4\n",
    "jet_def = fj.JetDefinition(fj.antikt_algorithm, R)\n",
    "\n",
    "files = {name:rt.TFile(name,'READ') for name in data_filenames}\n",
    "tree_names = {'cluster':'ClusterTree','event':'EventTree','scores':'ScoreTree'}\n",
    "ur_trees = {file:{tree_key:ur.open(file)[tree_name] for tree_key,tree_name in tree_names.items()} for file in data_filenames}\n",
    "\n",
    "for dfile, trees in ur_trees.items():\n",
    "    \n",
    "    # event info\n",
    "    cluster_min = trees['event'].array('clusterCount')\n",
    "    cluster_max = cluster_min + trees['event'].array('nCluster') - 1\n",
    "    \n",
    "    # cluster info (pre-existing)\n",
    "    cluster_vec = np.column_stack(tuple(trees['cluster'].arrays(['clusterPt','clusterEta','clusterPhi']).values()))\n",
    "    \n",
    "    # cluster info (scores)\n",
    "    cluster_classification = trees['scores'].array('charged_likelihood_combo')\n",
    "    cluster_energies = np.column_stack(tuple(trees['scores'].arrays(['clusterE_charged','clusterE_neutral']).values()))\n",
    "\n",
    "    vec_polar = rt.Math.PtEtaPhiEVector()    \n",
    "    # loop over events\n",
    "    nevents = tree['event'].numentries\n",
    "    for i in range(nevents):\n",
    "        cluster_idxs = np.linspace(cluster_min[i], cluster_max[i], cluster_max[i] - cluster_min[i] + 1)        \n",
    "        nCluster = cluster_idxs.shape[0]\n",
    "        pseudojets = nCluster * [fj.PseudoJet(0.,0.,0.,0.)] # TODO: pre-allocating space? does this speed things up?\n",
    "\n",
    "        for j, idx in enumerate(cluster_idxs):\n",
    "            energy = cluster_energies[idx,0]\n",
    "            if cluster_classification[idx] < 0.5: energy = cluster_energies[idx,1]\n",
    "            vec_polar.SetCoordinates(cluster_vec[idx,0],cluster_vec[idx,1],cluster_vec[idx,2],energy)\n",
    "            pseudojets[j] = fj.PseudoJet(vec_polar.Px(), vec_polar.Py(), vec_polar.Pz(), vec_polar.E()) # fastjet uses Cartesian\n",
    "            \n",
    "        jets = jet_def(pseudojets) # perform jet clustering\n",
    "        njets = len(jets)\n",
    "        # TODO: save jet info to a TTree\n",
    "\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml4p]",
   "language": "python",
   "name": "conda-env-ml4p-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
